{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Backward-Compatible Representation Learning with DualPrompt\n",
    "\n",
    "## Continual Learning and Feature Compatibility Analysis\n",
    "\n",
    "This project investigates the backward compatibility of feature representations learned by a continual learning model (DualPrompt), following the evaluation principles introduced in *Towards Backward-Compatible Representation Learning*.\n",
    "\n",
    "The goal is to verify whether features extracted from an updated model can be used as **queries** against a **gallery indexed with features from a previous model**, without re-extracting the gallery features.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Environment Setup\n",
    "\n",
    "This section reports the execution environment by checking:\n",
    "- the PyTorch version\n",
    "- CUDA availability\n",
    "- the number of available GPUs\n",
    "\n",
    "This information is provided to ensure reproducibility of the experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rpchdPejLWre",
    "outputId": "3ba50125-88d9-42e8-ccef-d8965d5d624f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PyTorch version: 2.9.0+cu126\n",
      "CUDA available: True\n",
      "Num GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Num GPUs:\", torch.cuda.device_count())\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## DualPrompt Codebase\n",
    "\n",
    "For the experimental phase, a PyTorch-based implementation of DualPrompt was employed, preferred over the official release for its superior portability and compatibility with Google Colab. This codebase ensures parity of results and strictly adheres to the original design, while providing a more adaptable structure for testing and integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/JH-LEE-KR/dualprompt-pytorch.git\n",
    "%cd dualprompt-pytorch\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DAl7ZOPILlc-",
    "outputId": "f67286e5-1de2-46a4-8b18-889adc775b27"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'dualprompt-pytorch'...\n",
      "remote: Enumerating objects: 79, done.\u001B[K\n",
      "remote: Counting objects: 100% (35/35), done.\u001B[K\n",
      "remote: Compressing objects: 100% (11/11), done.\u001B[K\n",
      "remote: Total 79 (delta 27), reused 24 (delta 24), pack-reused 44 (from 1)\u001B[K\n",
      "Receiving objects: 100% (79/79), 58.17 KiB | 945.00 KiB/s, done.\n",
      "Resolving deltas: 100% (41/41), done.\n",
      "/content/dualprompt-pytorch\n"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dependency Installation\n",
    "\n",
    "The main dependencies required for training and evaluation are installed, including:\n",
    "- PyTorch and torchvision\n",
    "- timm for Vision Transformer models\n",
    "- utility libraries for preprocessing and visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install torch torchvision timm pillow matplotlib\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "uoJw1wl5L67x",
    "outputId": "48fc9e4c-4e25-40b4-aa57-505a3f3c076d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.8/1.8 MB\u001B[0m \u001B[31m36.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.1.2\n",
      "    Uninstalling pip-24.1.2:\n",
      "      Successfully uninstalled pip-24.1.2\n",
      "Successfully installed pip-25.3\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
      "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.24)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.36.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.7.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2026.1.4)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install timm==0.6.7"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "fJfcsvd3M41f",
    "outputId": "34139372-588d-4a20-9553-d18dfe112bf0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting timm==0.6.7\n",
      "  Downloading timm-0.6.7-py3-none-any.whl.metadata (33 kB)\n",
      "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.12/dist-packages (from timm==0.6.7) (2.9.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm==0.6.7) (0.24.0+cu126)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.4->timm==0.6.7) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.4->timm==0.6.7) (3.0.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.6.7) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.6.7) (11.3.0)\n",
      "Downloading timm-0.6.7-py3-none-any.whl (509 kB)\n",
      "Installing collected packages: timm\n",
      "  Attempting uninstall: timm\n",
      "    Found existing installation: timm 1.0.24\n",
      "    Uninstalling timm-1.0.24:\n",
      "      Successfully uninstalled timm-1.0.24\n",
      "Successfully installed timm-0.6.7\n"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dataset: CIFAR-100\n",
    "\n",
    "The DualPrompt model is trained on **CIFAR-100**, a standard benchmark for continual learning.\n",
    "\n",
    "The dataset contains:\n",
    "- 100 object classes\n",
    "- RGB images of size 32×32\n",
    "- standard train/test splits\n",
    "\n",
    "The dataset is downloaded locally and used for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from torchvision import datasets\n",
    "\n",
    "data_path = \"./data\"\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "datasets.CIFAR100(root=data_path, download=True, train=True)\n",
    "datasets.CIFAR100(root=data_path, download=True, train=False)\n",
    "print(\" CIFAR-100 saved in:\", data_path)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fis_OF8UL8nI",
    "outputId": "0516b6f8-9729-410c-f4be-b34219464fe0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 169M/169M [00:03<00:00, 44.0MB/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " CIFAR-100 saved in: ./data\n"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pre-trained Model Cache\n",
    "\n",
    "A local directory is specified for caching pre-trained models used by `timm`.\n",
    "This avoids repeated downloads and improves execution efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ['TORCH_HOME'] = './torch_cache'\n",
    "os.makedirs(os.environ['TORCH_HOME'], exist_ok=True)\n",
    "print(\"Pre-trained model saved in:\", os.environ['TORCH_HOME'])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jbTNZ0Q4MW_3",
    "outputId": "ed2e495d-f766-4290-9cf7-cfae8687daf2"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pre-trained model saved in: ./torch_cache\n"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Continual Learning Training with DualPrompt\n",
    "\n",
    "In this step, the model is trained using **DualPrompt** on CIFAR-100.\n",
    "\n",
    "Key settings:\n",
    "- Backbone: Vision Transformer (ViT-B/16)\n",
    "- Method: DualPrompt\n",
    "- Scenario: class-incremental continual learning\n",
    "- Output: a trained model with dynamically selected prompts\n",
    "\n",
    "The training follows the standard configuration provided in the official repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!python -m torch.distributed.run \\\n",
    "    --nproc-per-node=1 \\\n",
    "    main.py cifar100_dualprompt \\\n",
    "    --model vit_base_patch16_224 \\\n",
    "    --batch-size 64 \\\n",
    "    --data-path ./data \\\n",
    "    --output_dir ./output\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D6F28bS-MbIL",
    "outputId": "07079293-304a-4e24-8abf-f58555789928",
    "collapsed": true
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "| distributed init (rank 0): env://\n",
      "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[rank0]:[W122 16:08:39.217019991 ProcessGroupNCCL.cpp:5068] Guessing device ID based on global rank. This can cause a hang if rank to GPU mapping is heterogeneous. You can specify device_id in init_process_group()\n",
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "Creating original model: vit_base_patch16_224\n",
      "Creating model: vit_base_patch16_224\n",
      "Namespace(subparser_name='cifar100_dualprompt', batch_size=64, epochs=5, model='vit_base_patch16_224', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='constant', lr=0.03, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='./data', dataset='Split-CIFAR100', shuffle=False, output_dir='./output', device='cuda', seed=42, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, use_g_prompt=True, g_prompt_length=5, g_prompt_layer_idx=[0, 1], use_prefix_tune_for_g_prompt=True, use_e_prompt=True, e_prompt_layer_idx=[2, 3, 4], use_prefix_tune_for_e_prompt=True, prompt_pool=True, size=10, length=5, top_k=1, initializer='uniform', prompt_key=True, prompt_key_init='uniform', use_prompt_mask=True, mask_first_epoch=False, shared_prompt_pool=True, shared_prompt_key=False, batchwise_prompt=True, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=1.0, same_key_value=False, global_pool='token', head_type='token', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], print_freq=10, rank=0, gpu=0, distributed=True, dist_backend='nccl', nb_classes=100)\n",
      "number of params: 330340\n",
      "Start training for 5 epochs\n",
      "Train: Epoch[1/5]  [ 0/79]  eta: 0:07:31  Lr: 0.007500  Loss: 2.2933  Acc@1: 17.1875 (17.1875)  Acc@5: 57.8125 (57.8125)  time: 5.7190  data: 1.0760  max mem: 6436\n",
      "Train: Epoch[1/5]  [10/79]  eta: 0:02:45  Lr: 0.007500  Loss: 1.0929  Acc@1: 62.5000 (59.3750)  Acc@5: 92.1875 (88.2102)  time: 2.3961  data: 0.0981  max mem: 6438\n",
      "Train: Epoch[1/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: 0.6380  Acc@1: 73.4375 (68.8244)  Acc@5: 95.3125 (92.4851)  time: 2.1055  data: 0.0004  max mem: 6438\n",
      "Train: Epoch[1/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: 0.5436  Acc@1: 79.6875 (73.0847)  Acc@5: 96.8750 (94.4052)  time: 2.1990  data: 0.0003  max mem: 6438\n",
      "Train: Epoch[1/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: 0.3320  Acc@1: 82.8125 (75.9146)  Acc@5: 98.4375 (95.3506)  time: 2.2834  data: 0.0005  max mem: 6438\n",
      "Train: Epoch[1/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: 0.4775  Acc@1: 82.8125 (77.4203)  Acc@5: 98.4375 (95.7108)  time: 2.2727  data: 0.0005  max mem: 6438\n",
      "Train: Epoch[1/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: 0.2347  Acc@1: 82.8125 (78.3811)  Acc@5: 96.8750 (95.8504)  time: 2.2194  data: 0.0004  max mem: 6438\n",
      "Train: Epoch[1/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: 0.2119  Acc@1: 85.9375 (79.5335)  Acc@5: 96.8750 (96.2368)  time: 2.2307  data: 0.0006  max mem: 6438\n",
      "Train: Epoch[1/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.0032  Acc@1: 85.9375 (80.1000)  Acc@5: 98.4375 (96.5000)  time: 2.1603  data: 0.0005  max mem: 6438\n",
      "Train: Epoch[1/5] Total time: 0:02:56 (2.2372 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.0032  Acc@1: 85.9375 (80.1000)  Acc@5: 98.4375 (96.5000)\n",
      "Train: Epoch[2/5]  [ 0/79]  eta: 0:03:41  Lr: 0.007500  Loss: 0.1939  Acc@1: 85.9375 (85.9375)  Acc@5: 96.8750 (96.8750)  time: 2.8006  data: 0.5712  max mem: 6438\n",
      "Train: Epoch[2/5]  [10/79]  eta: 0:02:37  Lr: 0.007500  Loss: 0.0320  Acc@1: 85.9375 (85.0852)  Acc@5: 98.4375 (97.8693)  time: 2.2886  data: 0.0522  max mem: 6438\n",
      "Train: Epoch[2/5]  [20/79]  eta: 0:02:13  Lr: 0.007500  Loss: 0.1232  Acc@1: 85.9375 (86.3839)  Acc@5: 98.4375 (98.1399)  time: 2.2359  data: 0.0003  max mem: 6438\n",
      "Train: Epoch[2/5]  [30/79]  eta: 0:01:50  Lr: 0.007500  Loss: -0.1012  Acc@1: 89.0625 (86.9960)  Acc@5: 98.4375 (98.3367)  time: 2.2365  data: 0.0003  max mem: 6438\n",
      "Train: Epoch[2/5]  [40/79]  eta: 0:01:27  Lr: 0.007500  Loss: -0.0056  Acc@1: 90.6250 (87.7287)  Acc@5: 100.0000 (98.5137)  time: 2.2397  data: 0.0004  max mem: 6438\n",
      "Train: Epoch[2/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: 0.0012  Acc@1: 89.0625 (87.9902)  Acc@5: 100.0000 (98.5907)  time: 2.2421  data: 0.0005  max mem: 6438\n",
      "Train: Epoch[2/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: 0.0215  Acc@1: 89.0625 (88.3709)  Acc@5: 98.4375 (98.6168)  time: 2.2437  data: 0.0006  max mem: 6438\n",
      "Train: Epoch[2/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.0583  Acc@1: 89.0625 (88.3143)  Acc@5: 98.4375 (98.7236)  time: 2.2441  data: 0.0005  max mem: 6438\n",
      "Train: Epoch[2/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.0656  Acc@1: 89.0625 (88.3200)  Acc@5: 100.0000 (98.7400)  time: 2.1462  data: 0.0004  max mem: 6438\n",
      "Train: Epoch[2/5] Total time: 0:02:55 (2.2249 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.0656  Acc@1: 89.0625 (88.3200)  Acc@5: 100.0000 (98.7400)\n",
      "Train: Epoch[3/5]  [ 0/79]  eta: 0:04:10  Lr: 0.007500  Loss: -0.0182  Acc@1: 89.0625 (89.0625)  Acc@5: 96.8750 (96.8750)  time: 3.1679  data: 0.9524  max mem: 6438\n",
      "Train: Epoch[3/5]  [10/79]  eta: 0:02:40  Lr: 0.007500  Loss: -0.0689  Acc@1: 89.0625 (89.3466)  Acc@5: 98.4375 (98.2955)  time: 2.3301  data: 0.0871  max mem: 6438\n",
      "Train: Epoch[3/5]  [20/79]  eta: 0:02:15  Lr: 0.007500  Loss: -0.0707  Acc@1: 90.6250 (90.0298)  Acc@5: 98.4375 (98.8095)  time: 2.2532  data: 0.0005  max mem: 6438\n",
      "Train: Epoch[3/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: -0.0571  Acc@1: 89.0625 (89.7177)  Acc@5: 100.0000 (98.9415)  time: 2.2605  data: 0.0007  max mem: 6438\n",
      "Train: Epoch[3/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.0847  Acc@1: 89.0625 (89.3674)  Acc@5: 100.0000 (99.0091)  time: 2.2559  data: 0.0007  max mem: 6438\n",
      "Train: Epoch[3/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.1871  Acc@1: 87.5000 (89.1544)  Acc@5: 100.0000 (99.0502)  time: 2.2486  data: 0.0007  max mem: 6438\n",
      "Train: Epoch[3/5]  [60/79]  eta: 0:00:43  Lr: 0.007500  Loss: -0.0355  Acc@1: 89.0625 (89.0881)  Acc@5: 100.0000 (99.1035)  time: 2.2455  data: 0.0006  max mem: 6438\n",
      "Train: Epoch[3/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.1513  Acc@1: 89.0625 (89.2606)  Acc@5: 98.4375 (99.0317)  time: 2.2440  data: 0.0005  max mem: 6438\n",
      "Train: Epoch[3/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.0415  Acc@1: 90.6250 (89.1800)  Acc@5: 100.0000 (99.1000)  time: 2.1455  data: 0.0004  max mem: 6438\n",
      "Train: Epoch[3/5] Total time: 0:02:56 (2.2375 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.0415  Acc@1: 90.6250 (89.1800)  Acc@5: 100.0000 (99.1000)\n",
      "Train: Epoch[4/5]  [ 0/79]  eta: 0:03:38  Lr: 0.007500  Loss: 0.2099  Acc@1: 76.5625 (76.5625)  Acc@5: 98.4375 (98.4375)  time: 2.7605  data: 0.5436  max mem: 6438\n",
      "Train: Epoch[4/5]  [10/79]  eta: 0:02:37  Lr: 0.007500  Loss: -0.3091  Acc@1: 89.0625 (88.7784)  Acc@5: 98.4375 (98.7216)  time: 2.2878  data: 0.0501  max mem: 6438\n",
      "Train: Epoch[4/5]  [20/79]  eta: 0:02:13  Lr: 0.007500  Loss: -0.1829  Acc@1: 89.0625 (89.7321)  Acc@5: 98.4375 (98.5119)  time: 2.2427  data: 0.0005  max mem: 6438\n",
      "Train: Epoch[4/5]  [30/79]  eta: 0:01:50  Lr: 0.007500  Loss: -0.2017  Acc@1: 90.6250 (89.3649)  Acc@5: 100.0000 (98.8407)  time: 2.2449  data: 0.0005  max mem: 6438\n",
      "Train: Epoch[4/5]  [40/79]  eta: 0:01:27  Lr: 0.007500  Loss: -0.2612  Acc@1: 90.6250 (89.7485)  Acc@5: 98.4375 (98.8567)  time: 2.2447  data: 0.0007  max mem: 6438\n",
      "Train: Epoch[4/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.2468  Acc@1: 90.6250 (89.7059)  Acc@5: 98.4375 (98.8971)  time: 2.2452  data: 0.0006  max mem: 6438\n",
      "Train: Epoch[4/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: 0.0107  Acc@1: 90.6250 (89.6004)  Acc@5: 98.4375 (98.7961)  time: 2.2463  data: 0.0005  max mem: 6438\n",
      "Train: Epoch[4/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.1479  Acc@1: 90.6250 (89.6787)  Acc@5: 98.4375 (98.7896)  time: 2.2474  data: 0.0005  max mem: 6438\n",
      "Train: Epoch[4/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.1620  Acc@1: 90.6250 (89.8800)  Acc@5: 100.0000 (98.8600)  time: 2.1509  data: 0.0004  max mem: 6438\n",
      "Train: Epoch[4/5] Total time: 0:02:56 (2.2290 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.1620  Acc@1: 90.6250 (89.8800)  Acc@5: 100.0000 (98.8600)\n",
      "Train: Epoch[5/5]  [ 0/79]  eta: 0:03:58  Lr: 0.007500  Loss: -0.1251  Acc@1: 90.6250 (90.6250)  Acc@5: 98.4375 (98.4375)  time: 3.0205  data: 0.7920  max mem: 6438\n",
      "Train: Epoch[5/5]  [10/79]  eta: 0:02:40  Lr: 0.007500  Loss: -0.1398  Acc@1: 90.6250 (90.1989)  Acc@5: 98.4375 (99.0057)  time: 2.3196  data: 0.0723  max mem: 6438\n",
      "Train: Epoch[5/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: -0.0939  Acc@1: 90.6250 (90.3274)  Acc@5: 100.0000 (99.3304)  time: 2.2513  data: 0.0003  max mem: 6438\n",
      "Train: Epoch[5/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: -0.0540  Acc@1: 90.6250 (90.4234)  Acc@5: 100.0000 (99.1935)  time: 2.2538  data: 0.0005  max mem: 6438\n",
      "Train: Epoch[5/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.2476  Acc@1: 90.6250 (90.5107)  Acc@5: 100.0000 (99.0091)  time: 2.2562  data: 0.0010  max mem: 6438\n",
      "Train: Epoch[5/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.0965  Acc@1: 92.1875 (90.8701)  Acc@5: 100.0000 (99.2034)  time: 2.2592  data: 0.0010  max mem: 6438\n",
      "Train: Epoch[5/5]  [60/79]  eta: 0:00:43  Lr: 0.007500  Loss: -0.1928  Acc@1: 90.6250 (90.6762)  Acc@5: 100.0000 (99.1803)  time: 2.2611  data: 0.0005  max mem: 6438\n",
      "Train: Epoch[5/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.0318  Acc@1: 90.6250 (90.7350)  Acc@5: 100.0000 (99.2077)  time: 2.2611  data: 0.0007  max mem: 6438\n",
      "Train: Epoch[5/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.3615  Acc@1: 92.1875 (91.0000)  Acc@5: 100.0000 (99.2200)  time: 2.1601  data: 0.0006  max mem: 6438\n",
      "Train: Epoch[5/5] Total time: 0:02:57 (2.2424 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.3615  Acc@1: 92.1875 (91.0000)  Acc@5: 100.0000 (99.2200)\n",
      "Test: [Task 1]  [ 0/16]  eta: 0:00:32  Loss: 0.3608 (0.3608)  Acc@1: 98.4375 (98.4375)  Acc@5: 98.4375 (98.4375)  time: 2.0503  data: 0.5633  max mem: 6438\n",
      "Test: [Task 1]  [10/16]  eta: 0:00:09  Loss: 0.2067 (0.2386)  Acc@1: 98.4375 (98.5795)  Acc@5: 100.0000 (99.8580)  time: 1.5618  data: 0.0517  max mem: 6438\n",
      "Test: [Task 1]  [15/16]  eta: 0:00:01  Loss: 0.2000 (0.2284)  Acc@1: 98.4375 (98.7000)  Acc@5: 100.0000 (99.9000)  time: 1.5405  data: 0.0356  max mem: 6438\n",
      "Test: [Task 1] Total time: 0:00:24 (1.5449 s / it)\n",
      "* Acc@1 98.700 Acc@5 99.900 loss 0.228\n",
      "[Average accuracy till task1]\tAcc@1: 98.7000\tAcc@5: 99.9000\tLoss: 0.2284\n",
      "Train: Epoch[1/5]  [ 0/79]  eta: 0:03:35  Lr: 0.007500  Loss: 2.3049  Acc@1: 14.0625 (14.0625)  Acc@5: 50.0000 (50.0000)  time: 2.7221  data: 0.4939  max mem: 6438\n",
      "Train: Epoch[1/5]  [10/79]  eta: 0:02:38  Lr: 0.007500  Loss: 0.8215  Acc@1: 76.5625 (69.1761)  Acc@5: 95.3125 (90.3409)  time: 2.2954  data: 0.0454  max mem: 6440\n",
      "Train: Epoch[1/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: 0.2805  Acc@1: 79.6875 (76.8601)  Acc@5: 96.8750 (93.7500)  time: 2.2552  data: 0.0006  max mem: 6440\n",
      "Train: Epoch[1/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: 0.2607  Acc@1: 84.3750 (79.1835)  Acc@5: 96.8750 (94.9093)  time: 2.2527  data: 0.0006  max mem: 6440\n",
      "Train: Epoch[1/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: 0.2830  Acc@1: 85.9375 (81.2881)  Acc@5: 98.4375 (95.8460)  time: 2.2447  data: 0.0006  max mem: 6440\n",
      "Train: Epoch[1/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: 0.3550  Acc@1: 87.5000 (81.9853)  Acc@5: 98.4375 (96.4154)  time: 2.2399  data: 0.0007  max mem: 6440\n",
      "Train: Epoch[1/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: 0.1442  Acc@1: 85.9375 (82.4539)  Acc@5: 98.4375 (96.6701)  time: 2.2386  data: 0.0007  max mem: 6440\n",
      "Train: Epoch[1/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: 0.0284  Acc@1: 87.5000 (83.4947)  Acc@5: 98.4375 (96.9850)  time: 2.2420  data: 0.0004  max mem: 6440\n",
      "Train: Epoch[1/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: 0.0627  Acc@1: 85.9375 (83.4800)  Acc@5: 98.4375 (96.9800)  time: 2.1476  data: 0.0003  max mem: 6440\n",
      "Train: Epoch[1/5] Total time: 0:02:56 (2.2284 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: 0.0627  Acc@1: 85.9375 (83.4800)  Acc@5: 98.4375 (96.9800)\n",
      "Train: Epoch[2/5]  [ 0/79]  eta: 0:03:35  Lr: 0.007500  Loss: 0.0629  Acc@1: 89.0625 (89.0625)  Acc@5: 100.0000 (100.0000)  time: 2.7333  data: 0.4994  max mem: 6440\n",
      "Train: Epoch[2/5]  [10/79]  eta: 0:02:38  Lr: 0.007500  Loss: -0.0453  Acc@1: 87.5000 (87.7841)  Acc@5: 100.0000 (99.4318)  time: 2.2955  data: 0.0459  max mem: 6440\n",
      "Train: Epoch[2/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: 0.0437  Acc@1: 87.5000 (87.4256)  Acc@5: 100.0000 (98.9583)  time: 2.2562  data: 0.0005  max mem: 6440\n",
      "Train: Epoch[2/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: -0.0284  Acc@1: 85.9375 (86.5927)  Acc@5: 100.0000 (98.9415)  time: 2.2565  data: 0.0005  max mem: 6440\n",
      "Train: Epoch[2/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: 0.1192  Acc@1: 84.3750 (86.7759)  Acc@5: 98.4375 (98.7424)  time: 2.2499  data: 0.0006  max mem: 6440\n",
      "Train: Epoch[2/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: 0.0217  Acc@1: 87.5000 (87.3468)  Acc@5: 98.4375 (98.8051)  time: 2.2443  data: 0.0006  max mem: 6440\n",
      "Train: Epoch[2/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: -0.1106  Acc@1: 89.0625 (87.8074)  Acc@5: 100.0000 (98.8473)  time: 2.2401  data: 0.0006  max mem: 6440\n",
      "Train: Epoch[2/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.0026  Acc@1: 87.5000 (87.7201)  Acc@5: 98.4375 (98.8776)  time: 2.2399  data: 0.0007  max mem: 6440\n",
      "Train: Epoch[2/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.2982  Acc@1: 87.5000 (87.8000)  Acc@5: 98.4375 (98.8600)  time: 2.1441  data: 0.0005  max mem: 6440\n",
      "Train: Epoch[2/5] Total time: 0:02:56 (2.2298 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.2982  Acc@1: 87.5000 (87.8000)  Acc@5: 98.4375 (98.8600)\n",
      "Train: Epoch[3/5]  [ 0/79]  eta: 0:03:39  Lr: 0.007500  Loss: -0.1160  Acc@1: 89.0625 (89.0625)  Acc@5: 98.4375 (98.4375)  time: 2.7751  data: 0.5428  max mem: 6440\n",
      "Train: Epoch[3/5]  [10/79]  eta: 0:02:38  Lr: 0.007500  Loss: -0.2362  Acc@1: 87.5000 (86.7898)  Acc@5: 98.4375 (98.0114)  time: 2.2981  data: 0.0500  max mem: 6440\n",
      "Train: Epoch[3/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: 0.0547  Acc@1: 87.5000 (86.8304)  Acc@5: 98.4375 (98.2143)  time: 2.2551  data: 0.0006  max mem: 6440\n",
      "Train: Epoch[3/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: -0.0371  Acc@1: 87.5000 (87.3992)  Acc@5: 98.4375 (98.4879)  time: 2.2606  data: 0.0005  max mem: 6440\n",
      "Train: Epoch[3/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.2569  Acc@1: 89.0625 (87.6905)  Acc@5: 100.0000 (98.7043)  time: 2.2547  data: 0.0006  max mem: 6440\n",
      "Train: Epoch[3/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.0909  Acc@1: 89.0625 (88.0208)  Acc@5: 100.0000 (98.7745)  time: 2.2441  data: 0.0006  max mem: 6440\n",
      "Train: Epoch[3/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: -0.0545  Acc@1: 89.0625 (88.1660)  Acc@5: 98.4375 (98.7705)  time: 2.2419  data: 0.0006  max mem: 6440\n",
      "Train: Epoch[3/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.1270  Acc@1: 89.0625 (88.2482)  Acc@5: 98.4375 (98.7456)  time: 2.2453  data: 0.0007  max mem: 6440\n",
      "Train: Epoch[3/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.3345  Acc@1: 89.0625 (88.5000)  Acc@5: 98.4375 (98.7600)  time: 2.1469  data: 0.0005  max mem: 6440\n",
      "Train: Epoch[3/5] Total time: 0:02:56 (2.2321 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.3345  Acc@1: 89.0625 (88.5000)  Acc@5: 98.4375 (98.7600)\n",
      "Train: Epoch[4/5]  [ 0/79]  eta: 0:03:42  Lr: 0.007500  Loss: -0.1062  Acc@1: 89.0625 (89.0625)  Acc@5: 98.4375 (98.4375)  time: 2.8181  data: 0.5989  max mem: 6440\n",
      "Train: Epoch[4/5]  [10/79]  eta: 0:02:38  Lr: 0.007500  Loss: -0.0861  Acc@1: 92.1875 (91.6193)  Acc@5: 100.0000 (99.2898)  time: 2.2930  data: 0.0549  max mem: 6440\n",
      "Train: Epoch[4/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: -0.2745  Acc@1: 92.1875 (91.5923)  Acc@5: 100.0000 (99.3304)  time: 2.2471  data: 0.0005  max mem: 6440\n",
      "Train: Epoch[4/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: -0.1334  Acc@1: 90.6250 (90.8266)  Acc@5: 100.0000 (99.3952)  time: 2.2566  data: 0.0006  max mem: 6440\n",
      "Train: Epoch[4/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.1631  Acc@1: 89.0625 (90.6250)  Acc@5: 100.0000 (99.1616)  time: 2.2536  data: 0.0006  max mem: 6440\n",
      "Train: Epoch[4/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: 0.0039  Acc@1: 89.0625 (90.4105)  Acc@5: 98.4375 (99.0196)  time: 2.2428  data: 0.0007  max mem: 6440\n",
      "Train: Epoch[4/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: -0.1412  Acc@1: 89.0625 (90.3176)  Acc@5: 98.4375 (98.9754)  time: 2.2369  data: 0.0007  max mem: 6440\n",
      "Train: Epoch[4/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.0686  Acc@1: 89.0625 (90.3389)  Acc@5: 98.4375 (98.9877)  time: 2.2374  data: 0.0006  max mem: 6440\n",
      "Train: Epoch[4/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.0743  Acc@1: 90.6250 (90.3600)  Acc@5: 100.0000 (99.0200)  time: 2.1431  data: 0.0005  max mem: 6440\n",
      "Train: Epoch[4/5] Total time: 0:02:56 (2.2283 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.0743  Acc@1: 90.6250 (90.3600)  Acc@5: 100.0000 (99.0200)\n",
      "Train: Epoch[5/5]  [ 0/79]  eta: 0:03:40  Lr: 0.007500  Loss: -0.0754  Acc@1: 89.0625 (89.0625)  Acc@5: 96.8750 (96.8750)  time: 2.7918  data: 0.5655  max mem: 6440\n",
      "Train: Epoch[5/5]  [10/79]  eta: 0:02:38  Lr: 0.007500  Loss: -0.0757  Acc@1: 89.0625 (89.9148)  Acc@5: 98.4375 (98.4375)  time: 2.2968  data: 0.0520  max mem: 6440\n",
      "Train: Epoch[5/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: -0.1134  Acc@1: 89.0625 (89.8810)  Acc@5: 98.4375 (98.6607)  time: 2.2505  data: 0.0005  max mem: 6440\n",
      "Train: Epoch[5/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: -0.1121  Acc@1: 89.0625 (89.5161)  Acc@5: 98.4375 (98.7903)  time: 2.2565  data: 0.0006  max mem: 6440\n",
      "Train: Epoch[5/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.1255  Acc@1: 89.0625 (90.0534)  Acc@5: 100.0000 (98.8948)  time: 2.2568  data: 0.0006  max mem: 6440\n",
      "Train: Epoch[5/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.0214  Acc@1: 90.6250 (89.9510)  Acc@5: 98.4375 (98.8664)  time: 2.2510  data: 0.0004  max mem: 6440\n",
      "Train: Epoch[5/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: -0.2529  Acc@1: 89.0625 (90.0871)  Acc@5: 98.4375 (98.7961)  time: 2.2465  data: 0.0004  max mem: 6440\n",
      "Train: Epoch[5/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.1606  Acc@1: 89.0625 (89.8327)  Acc@5: 98.4375 (98.7676)  time: 2.2443  data: 0.0005  max mem: 6440\n",
      "Train: Epoch[5/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.3698  Acc@1: 90.6250 (90.1200)  Acc@5: 98.4375 (98.7800)  time: 2.1454  data: 0.0005  max mem: 6440\n",
      "Train: Epoch[5/5] Total time: 0:02:56 (2.2325 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.3698  Acc@1: 90.6250 (90.1200)  Acc@5: 98.4375 (98.7800)\n",
      "Test: [Task 1]  [ 0/16]  eta: 0:00:31  Loss: 0.4711 (0.4711)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 1.9625  data: 0.4880  max mem: 6440\n",
      "Test: [Task 1]  [10/16]  eta: 0:00:09  Loss: 0.3166 (0.3349)  Acc@1: 93.7500 (91.4773)  Acc@5: 100.0000 (100.0000)  time: 1.5472  data: 0.0448  max mem: 6440\n",
      "Test: [Task 1]  [15/16]  eta: 0:00:01  Loss: 0.3099 (0.3206)  Acc@1: 92.1875 (91.9000)  Acc@5: 100.0000 (100.0000)  time: 1.4967  data: 0.0309  max mem: 6440\n",
      "Test: [Task 1] Total time: 0:00:24 (1.5011 s / it)\n",
      "* Acc@1 91.900 Acc@5 100.000 loss 0.321\n",
      "Test: [Task 2]  [ 0/16]  eta: 0:00:31  Loss: 0.2596 (0.2596)  Acc@1: 98.4375 (98.4375)  Acc@5: 100.0000 (100.0000)  time: 1.9693  data: 0.4933  max mem: 6440\n",
      "Test: [Task 2]  [10/16]  eta: 0:00:09  Loss: 0.4437 (0.4141)  Acc@1: 95.3125 (94.4602)  Acc@5: 100.0000 (99.5739)  time: 1.5489  data: 0.0452  max mem: 6440\n",
      "Test: [Task 2]  [15/16]  eta: 0:00:01  Loss: 0.3385 (0.3926)  Acc@1: 95.3125 (94.8000)  Acc@5: 100.0000 (99.6000)  time: 1.4985  data: 0.0312  max mem: 6440\n",
      "Test: [Task 2] Total time: 0:00:24 (1.5030 s / it)\n",
      "* Acc@1 94.800 Acc@5 99.600 loss 0.393\n",
      "[Average accuracy till task2]\tAcc@1: 93.3500\tAcc@5: 99.8000\tLoss: 0.3566\tForgetting: 6.8000\tBackward: -6.8000\n",
      "Train: Epoch[1/5]  [ 0/79]  eta: 0:03:44  Lr: 0.007500  Loss: 2.2758  Acc@1: 21.8750 (21.8750)  Acc@5: 56.2500 (56.2500)  time: 2.8417  data: 0.6228  max mem: 6440\n",
      "Train: Epoch[1/5]  [10/79]  eta: 0:02:38  Lr: 0.007500  Loss: 0.7372  Acc@1: 81.2500 (73.4375)  Acc@5: 95.3125 (91.6193)  time: 2.3015  data: 0.0572  max mem: 6443\n",
      "Train: Epoch[1/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: 0.2899  Acc@1: 85.9375 (80.1339)  Acc@5: 98.4375 (94.9405)  time: 2.2509  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[1/5]  [30/79]  eta: 0:01:50  Lr: 0.007500  Loss: 0.4521  Acc@1: 87.5000 (82.2581)  Acc@5: 98.4375 (95.9677)  time: 2.2451  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[1/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: 0.3623  Acc@1: 87.5000 (83.4985)  Acc@5: 98.4375 (96.6845)  time: 2.2353  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[1/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: 0.1088  Acc@1: 87.5000 (84.1605)  Acc@5: 100.0000 (97.1201)  time: 2.2378  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[1/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: 0.2257  Acc@1: 87.5000 (85.0410)  Acc@5: 98.4375 (97.3873)  time: 2.2417  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[1/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: 0.0953  Acc@1: 87.5000 (85.2993)  Acc@5: 98.4375 (97.5792)  time: 2.2451  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[1/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.1881  Acc@1: 87.5000 (85.6000)  Acc@5: 98.4375 (97.6200)  time: 2.1504  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[1/5] Total time: 0:02:55 (2.2278 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.1881  Acc@1: 87.5000 (85.6000)  Acc@5: 98.4375 (97.6200)\n",
      "Train: Epoch[2/5]  [ 0/79]  eta: 0:03:40  Lr: 0.007500  Loss: 0.3663  Acc@1: 78.1250 (78.1250)  Acc@5: 96.8750 (96.8750)  time: 2.7893  data: 0.5683  max mem: 6443\n",
      "Train: Epoch[2/5]  [10/79]  eta: 0:02:38  Lr: 0.007500  Loss: -0.0563  Acc@1: 90.6250 (88.9205)  Acc@5: 98.4375 (98.8636)  time: 2.2971  data: 0.0520  max mem: 6443\n",
      "Train: Epoch[2/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: 0.0764  Acc@1: 90.6250 (89.2113)  Acc@5: 100.0000 (99.2560)  time: 2.2514  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[2/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: -0.1184  Acc@1: 89.0625 (89.1633)  Acc@5: 100.0000 (99.2440)  time: 2.2549  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[2/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: 0.1248  Acc@1: 89.0625 (89.2149)  Acc@5: 100.0000 (99.1235)  time: 2.2542  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[2/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: 0.0879  Acc@1: 90.6250 (89.4301)  Acc@5: 98.4375 (99.0196)  time: 2.2534  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[2/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: -0.0033  Acc@1: 89.0625 (89.3699)  Acc@5: 98.4375 (98.9754)  time: 2.2528  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[2/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.1639  Acc@1: 89.0625 (89.5246)  Acc@5: 98.4375 (98.9437)  time: 2.2523  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[2/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.2961  Acc@1: 90.6250 (89.6200)  Acc@5: 100.0000 (98.9400)  time: 2.1555  data: 0.0003  max mem: 6443\n",
      "Train: Epoch[2/5] Total time: 0:02:56 (2.2360 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.2961  Acc@1: 90.6250 (89.6200)  Acc@5: 100.0000 (98.9400)\n",
      "Train: Epoch[3/5]  [ 0/79]  eta: 0:03:40  Lr: 0.007500  Loss: -0.1384  Acc@1: 93.7500 (93.7500)  Acc@5: 98.4375 (98.4375)  time: 2.7849  data: 0.5517  max mem: 6443\n",
      "Train: Epoch[3/5]  [10/79]  eta: 0:02:38  Lr: 0.007500  Loss: -0.2694  Acc@1: 92.1875 (91.4773)  Acc@5: 98.4375 (98.8636)  time: 2.2990  data: 0.0505  max mem: 6443\n",
      "Train: Epoch[3/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: -0.0647  Acc@1: 90.6250 (90.9226)  Acc@5: 98.4375 (99.1071)  time: 2.2536  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[3/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: -0.1194  Acc@1: 90.6250 (90.4738)  Acc@5: 100.0000 (99.0927)  time: 2.2577  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[3/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.1124  Acc@1: 90.6250 (90.5488)  Acc@5: 100.0000 (99.0854)  time: 2.2553  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[3/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.1438  Acc@1: 89.0625 (90.3186)  Acc@5: 98.4375 (99.0502)  time: 2.2502  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[3/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: -0.0970  Acc@1: 89.0625 (90.2920)  Acc@5: 98.4375 (98.9754)  time: 2.2464  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[3/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: 0.0449  Acc@1: 89.0625 (90.0088)  Acc@5: 98.4375 (98.8116)  time: 2.2441  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[3/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.1362  Acc@1: 89.0625 (90.0200)  Acc@5: 98.4375 (98.8200)  time: 2.1456  data: 0.0003  max mem: 6443\n",
      "Train: Epoch[3/5] Total time: 0:02:56 (2.2328 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.1362  Acc@1: 89.0625 (90.0200)  Acc@5: 98.4375 (98.8200)\n",
      "Train: Epoch[4/5]  [ 0/79]  eta: 0:03:42  Lr: 0.007500  Loss: -0.2668  Acc@1: 95.3125 (95.3125)  Acc@5: 100.0000 (100.0000)  time: 2.8210  data: 0.6049  max mem: 6443\n",
      "Train: Epoch[4/5]  [10/79]  eta: 0:02:38  Lr: 0.007500  Loss: -0.1574  Acc@1: 92.1875 (91.1932)  Acc@5: 100.0000 (99.2898)  time: 2.2918  data: 0.0553  max mem: 6443\n",
      "Train: Epoch[4/5]  [20/79]  eta: 0:02:13  Lr: 0.007500  Loss: 0.0031  Acc@1: 89.0625 (90.1786)  Acc@5: 98.4375 (98.9583)  time: 2.2393  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[4/5]  [30/79]  eta: 0:01:50  Lr: 0.007500  Loss: 0.0017  Acc@1: 89.0625 (90.3226)  Acc@5: 98.4375 (98.7903)  time: 2.2399  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[4/5]  [40/79]  eta: 0:01:27  Lr: 0.007500  Loss: -0.2251  Acc@1: 89.0625 (90.2058)  Acc@5: 98.4375 (98.8567)  time: 2.2419  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[4/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.0248  Acc@1: 90.6250 (90.0123)  Acc@5: 100.0000 (98.9583)  time: 2.2433  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[4/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: -0.1708  Acc@1: 89.0625 (89.9590)  Acc@5: 100.0000 (98.9498)  time: 2.2435  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[4/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.1660  Acc@1: 90.6250 (90.1849)  Acc@5: 100.0000 (99.0097)  time: 2.2438  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[4/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: 0.1949  Acc@1: 90.6250 (90.2000)  Acc@5: 100.0000 (98.9800)  time: 2.1461  data: 0.0003  max mem: 6443\n",
      "Train: Epoch[4/5] Total time: 0:02:55 (2.2261 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: 0.1949  Acc@1: 90.6250 (90.2000)  Acc@5: 100.0000 (98.9800)\n",
      "Train: Epoch[5/5]  [ 0/79]  eta: 0:04:03  Lr: 0.007500  Loss: -0.0897  Acc@1: 87.5000 (87.5000)  Acc@5: 98.4375 (98.4375)  time: 3.0850  data: 0.8614  max mem: 6443\n",
      "Train: Epoch[5/5]  [10/79]  eta: 0:02:39  Lr: 0.007500  Loss: -0.1515  Acc@1: 87.5000 (88.4943)  Acc@5: 98.4375 (98.7216)  time: 2.3182  data: 0.0790  max mem: 6443\n",
      "Train: Epoch[5/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: -0.0833  Acc@1: 90.6250 (89.8810)  Acc@5: 98.4375 (98.7351)  time: 2.2456  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[5/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: -0.2183  Acc@1: 92.1875 (90.6754)  Acc@5: 100.0000 (98.9415)  time: 2.2528  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[5/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.1103  Acc@1: 92.1875 (90.8155)  Acc@5: 98.4375 (98.8948)  time: 2.2572  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[5/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.1323  Acc@1: 90.6250 (90.7782)  Acc@5: 98.4375 (98.6826)  time: 2.2545  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[5/5]  [60/79]  eta: 0:00:43  Lr: 0.007500  Loss: -0.3010  Acc@1: 90.6250 (90.5994)  Acc@5: 98.4375 (98.6680)  time: 2.2487  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[5/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.1608  Acc@1: 90.6250 (90.6910)  Acc@5: 98.4375 (98.7896)  time: 2.2450  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[5/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.3224  Acc@1: 92.1875 (90.6600)  Acc@5: 100.0000 (98.7800)  time: 2.1442  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[5/5] Total time: 0:02:56 (2.2353 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.3224  Acc@1: 92.1875 (90.6600)  Acc@5: 100.0000 (98.7800)\n",
      "Test: [Task 1]  [ 0/16]  eta: 0:00:29  Loss: 0.5375 (0.5375)  Acc@1: 84.3750 (84.3750)  Acc@5: 100.0000 (100.0000)  time: 1.8164  data: 0.3312  max mem: 6443\n",
      "Test: [Task 1]  [10/16]  eta: 0:00:09  Loss: 0.3551 (0.3802)  Acc@1: 89.0625 (88.4943)  Acc@5: 100.0000 (100.0000)  time: 1.5348  data: 0.0306  max mem: 6443\n",
      "Test: [Task 1]  [15/16]  eta: 0:00:01  Loss: 0.3522 (0.3607)  Acc@1: 89.0625 (89.4000)  Acc@5: 100.0000 (100.0000)  time: 1.4881  data: 0.0211  max mem: 6443\n",
      "Test: [Task 1] Total time: 0:00:23 (1.4924 s / it)\n",
      "* Acc@1 89.400 Acc@5 100.000 loss 0.361\n",
      "Test: [Task 2]  [ 0/16]  eta: 0:00:34  Loss: 0.2884 (0.2884)  Acc@1: 98.4375 (98.4375)  Acc@5: 98.4375 (98.4375)  time: 2.1846  data: 0.7102  max mem: 6443\n",
      "Test: [Task 2]  [10/16]  eta: 0:00:09  Loss: 0.4495 (0.4342)  Acc@1: 93.7500 (93.3239)  Acc@5: 98.4375 (98.0114)  time: 1.5710  data: 0.0651  max mem: 6443\n",
      "Test: [Task 2]  [15/16]  eta: 0:00:01  Loss: 0.3801 (0.4112)  Acc@1: 93.7500 (93.7000)  Acc@5: 98.4375 (98.4000)  time: 1.5156  data: 0.0448  max mem: 6443\n",
      "Test: [Task 2] Total time: 0:00:24 (1.5202 s / it)\n",
      "* Acc@1 93.700 Acc@5 98.400 loss 0.411\n",
      "Test: [Task 3]  [ 0/16]  eta: 0:00:31  Loss: 0.3094 (0.3094)  Acc@1: 93.7500 (93.7500)  Acc@5: 98.4375 (98.4375)  time: 1.9985  data: 0.5159  max mem: 6443\n",
      "Test: [Task 3]  [10/16]  eta: 0:00:09  Loss: 0.3198 (0.3185)  Acc@1: 93.7500 (93.0398)  Acc@5: 100.0000 (99.2898)  time: 1.5602  data: 0.0474  max mem: 6443\n",
      "Test: [Task 3]  [15/16]  eta: 0:00:01  Loss: 0.3198 (0.3572)  Acc@1: 90.6250 (92.0000)  Acc@5: 100.0000 (99.1000)  time: 1.5097  data: 0.0326  max mem: 6443\n",
      "Test: [Task 3] Total time: 0:00:24 (1.5140 s / it)\n",
      "* Acc@1 92.000 Acc@5 99.100 loss 0.357\n",
      "[Average accuracy till task3]\tAcc@1: 91.7000\tAcc@5: 99.1667\tLoss: 0.3764\tForgetting: 5.2000\tBackward: -5.2000\n",
      "Train: Epoch[1/5]  [ 0/79]  eta: 0:03:41  Lr: 0.007500  Loss: 2.2734  Acc@1: 17.1875 (17.1875)  Acc@5: 59.3750 (59.3750)  time: 2.8005  data: 0.5759  max mem: 6443\n",
      "Train: Epoch[1/5]  [10/79]  eta: 0:02:38  Lr: 0.007500  Loss: 0.6693  Acc@1: 78.1250 (73.1534)  Acc@5: 95.3125 (92.6136)  time: 2.3022  data: 0.0527  max mem: 6443\n",
      "Train: Epoch[1/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: 0.2461  Acc@1: 85.9375 (80.5804)  Acc@5: 96.8750 (95.3869)  time: 2.2522  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[1/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: 0.2177  Acc@1: 89.0625 (83.2661)  Acc@5: 98.4375 (96.3206)  time: 2.2527  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[1/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: 0.1583  Acc@1: 89.0625 (85.0229)  Acc@5: 98.4375 (96.9131)  time: 2.2521  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[1/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: 0.1932  Acc@1: 89.0625 (85.6924)  Acc@5: 98.4375 (97.3039)  time: 2.2502  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[1/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: 0.0164  Acc@1: 87.5000 (85.9887)  Acc@5: 98.4375 (97.5666)  time: 2.2486  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[1/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: 0.1769  Acc@1: 87.5000 (86.5757)  Acc@5: 98.4375 (97.6452)  time: 2.2473  data: 0.0011  max mem: 6443\n",
      "Train: Epoch[1/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: 0.0532  Acc@1: 89.0625 (86.8400)  Acc@5: 98.4375 (97.7600)  time: 2.1476  data: 0.0010  max mem: 6443\n",
      "Train: Epoch[1/5] Total time: 0:02:56 (2.2334 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: 0.0532  Acc@1: 89.0625 (86.8400)  Acc@5: 98.4375 (97.7600)\n",
      "Train: Epoch[2/5]  [ 0/79]  eta: 0:03:48  Lr: 0.007500  Loss: -0.0181  Acc@1: 90.6250 (90.6250)  Acc@5: 100.0000 (100.0000)  time: 2.8865  data: 0.6562  max mem: 6443\n",
      "Train: Epoch[2/5]  [10/79]  eta: 0:02:38  Lr: 0.007500  Loss: -0.1894  Acc@1: 89.0625 (89.7727)  Acc@5: 98.4375 (98.5795)  time: 2.3000  data: 0.0601  max mem: 6443\n",
      "Train: Epoch[2/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: -0.0534  Acc@1: 90.6250 (90.9970)  Acc@5: 98.4375 (98.7351)  time: 2.2419  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[2/5]  [30/79]  eta: 0:01:50  Lr: 0.007500  Loss: -0.0605  Acc@1: 92.1875 (91.4819)  Acc@5: 100.0000 (98.9415)  time: 2.2421  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[2/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.1086  Acc@1: 90.6250 (91.2348)  Acc@5: 100.0000 (99.1235)  time: 2.2418  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[2/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.0126  Acc@1: 90.6250 (91.1458)  Acc@5: 100.0000 (99.1115)  time: 2.2407  data: 0.0008  max mem: 6443\n",
      "Train: Epoch[2/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: -0.2546  Acc@1: 90.6250 (91.2910)  Acc@5: 100.0000 (99.1803)  time: 2.2401  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[2/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.1223  Acc@1: 90.6250 (91.3732)  Acc@5: 100.0000 (99.1417)  time: 2.2393  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[2/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.4060  Acc@1: 90.6250 (91.2600)  Acc@5: 100.0000 (99.1800)  time: 2.1399  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[2/5] Total time: 0:02:55 (2.2252 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.4060  Acc@1: 90.6250 (91.2600)  Acc@5: 100.0000 (99.1800)\n",
      "Train: Epoch[3/5]  [ 0/79]  eta: 0:03:29  Lr: 0.007500  Loss: -0.1934  Acc@1: 90.6250 (90.6250)  Acc@5: 100.0000 (100.0000)  time: 2.6570  data: 0.4247  max mem: 6443\n",
      "Train: Epoch[3/5]  [10/79]  eta: 0:02:37  Lr: 0.007500  Loss: -0.2532  Acc@1: 89.0625 (89.9148)  Acc@5: 100.0000 (98.7216)  time: 2.2806  data: 0.0393  max mem: 6443\n",
      "Train: Epoch[3/5]  [20/79]  eta: 0:02:13  Lr: 0.007500  Loss: -0.3120  Acc@1: 89.0625 (90.3274)  Acc@5: 100.0000 (98.9583)  time: 2.2487  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[3/5]  [30/79]  eta: 0:01:50  Lr: 0.007500  Loss: -0.0860  Acc@1: 89.0625 (90.2218)  Acc@5: 100.0000 (98.9415)  time: 2.2562  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[3/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.0413  Acc@1: 90.6250 (90.4345)  Acc@5: 100.0000 (99.0091)  time: 2.2564  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[3/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.2687  Acc@1: 92.1875 (90.9007)  Acc@5: 100.0000 (99.1115)  time: 2.2534  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[3/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: -0.0806  Acc@1: 90.6250 (90.7275)  Acc@5: 100.0000 (99.0523)  time: 2.2506  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[3/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.2681  Acc@1: 90.6250 (90.6690)  Acc@5: 98.4375 (99.0097)  time: 2.2481  data: 0.0008  max mem: 6443\n",
      "Train: Epoch[3/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.4466  Acc@1: 90.6250 (90.6600)  Acc@5: 98.4375 (99.0000)  time: 2.1482  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[3/5] Total time: 0:02:56 (2.2320 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.4466  Acc@1: 90.6250 (90.6600)  Acc@5: 98.4375 (99.0000)\n",
      "Train: Epoch[4/5]  [ 0/79]  eta: 0:03:32  Lr: 0.007500  Loss: -0.2011  Acc@1: 95.3125 (95.3125)  Acc@5: 100.0000 (100.0000)  time: 2.6854  data: 0.4654  max mem: 6443\n",
      "Train: Epoch[4/5]  [10/79]  eta: 0:02:37  Lr: 0.007500  Loss: -0.0486  Acc@1: 89.0625 (91.1932)  Acc@5: 98.4375 (98.4375)  time: 2.2878  data: 0.0432  max mem: 6443\n",
      "Train: Epoch[4/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: -0.1686  Acc@1: 90.6250 (91.6667)  Acc@5: 98.4375 (98.5863)  time: 2.2515  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[4/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: -0.2432  Acc@1: 92.1875 (91.7843)  Acc@5: 98.4375 (98.7399)  time: 2.2580  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[4/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.2566  Acc@1: 92.1875 (91.8064)  Acc@5: 98.4375 (98.7805)  time: 2.2579  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[4/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.3015  Acc@1: 93.7500 (91.9730)  Acc@5: 100.0000 (98.9277)  time: 2.2554  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[4/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: -0.3047  Acc@1: 92.1875 (91.9057)  Acc@5: 100.0000 (98.8730)  time: 2.2535  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[4/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.1147  Acc@1: 90.6250 (91.7694)  Acc@5: 98.4375 (98.8776)  time: 2.2471  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[4/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: 0.0286  Acc@1: 90.6250 (91.7800)  Acc@5: 98.4375 (98.8600)  time: 2.1451  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[4/5] Total time: 0:02:56 (2.2334 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: 0.0286  Acc@1: 90.6250 (91.7800)  Acc@5: 98.4375 (98.8600)\n",
      "Train: Epoch[5/5]  [ 0/79]  eta: 0:03:47  Lr: 0.007500  Loss: -0.2071  Acc@1: 95.3125 (95.3125)  Acc@5: 98.4375 (98.4375)  time: 2.8830  data: 0.6648  max mem: 6443\n",
      "Train: Epoch[5/5]  [10/79]  eta: 0:02:38  Lr: 0.007500  Loss: -0.1914  Acc@1: 92.1875 (91.9034)  Acc@5: 98.4375 (98.8636)  time: 2.2953  data: 0.0610  max mem: 6443\n",
      "Train: Epoch[5/5]  [20/79]  eta: 0:02:13  Lr: 0.007500  Loss: -0.2710  Acc@1: 92.1875 (91.5923)  Acc@5: 100.0000 (99.1815)  time: 2.2400  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[5/5]  [30/79]  eta: 0:01:50  Lr: 0.007500  Loss: -0.2063  Acc@1: 92.1875 (91.7843)  Acc@5: 100.0000 (99.2440)  time: 2.2446  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[5/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.1439  Acc@1: 90.6250 (91.2729)  Acc@5: 100.0000 (99.0854)  time: 2.2476  data: 0.0008  max mem: 6443\n",
      "Train: Epoch[5/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.2586  Acc@1: 90.6250 (91.2684)  Acc@5: 100.0000 (99.0196)  time: 2.2502  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[5/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: -0.3367  Acc@1: 92.1875 (91.3934)  Acc@5: 100.0000 (99.0523)  time: 2.2516  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[5/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.1617  Acc@1: 90.6250 (91.3732)  Acc@5: 100.0000 (99.1197)  time: 2.2515  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[5/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.2562  Acc@1: 92.1875 (91.6400)  Acc@5: 100.0000 (99.2000)  time: 2.1504  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[5/5] Total time: 0:02:56 (2.2312 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.2562  Acc@1: 92.1875 (91.6400)  Acc@5: 100.0000 (99.2000)\n",
      "Test: [Task 1]  [ 0/16]  eta: 0:00:31  Loss: 0.5938 (0.5938)  Acc@1: 82.8125 (82.8125)  Acc@5: 100.0000 (100.0000)  time: 1.9652  data: 0.4886  max mem: 6443\n",
      "Test: [Task 1]  [10/16]  eta: 0:00:09  Loss: 0.3921 (0.4153)  Acc@1: 90.6250 (88.6364)  Acc@5: 100.0000 (99.8580)  time: 1.5492  data: 0.0452  max mem: 6443\n",
      "Test: [Task 1]  [15/16]  eta: 0:00:01  Loss: 0.3631 (0.3896)  Acc@1: 89.0625 (89.1000)  Acc@5: 100.0000 (99.8000)  time: 1.4984  data: 0.0311  max mem: 6443\n",
      "Test: [Task 1] Total time: 0:00:24 (1.5035 s / it)\n",
      "* Acc@1 89.100 Acc@5 99.800 loss 0.390\n",
      "Test: [Task 2]  [ 0/16]  eta: 0:00:30  Loss: 0.3327 (0.3327)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 1.9215  data: 0.4359  max mem: 6443\n",
      "Test: [Task 2]  [10/16]  eta: 0:00:09  Loss: 0.4944 (0.4954)  Acc@1: 89.0625 (89.4886)  Acc@5: 98.4375 (97.7273)  time: 1.5442  data: 0.0401  max mem: 6443\n",
      "Test: [Task 2]  [15/16]  eta: 0:00:01  Loss: 0.4571 (0.4704)  Acc@1: 89.0625 (90.0000)  Acc@5: 98.4375 (98.1000)  time: 1.4947  data: 0.0276  max mem: 6443\n",
      "Test: [Task 2] Total time: 0:00:23 (1.4998 s / it)\n",
      "* Acc@1 90.000 Acc@5 98.100 loss 0.470\n",
      "Test: [Task 3]  [ 0/16]  eta: 0:00:33  Loss: 0.2948 (0.2948)  Acc@1: 93.7500 (93.7500)  Acc@5: 98.4375 (98.4375)  time: 2.0818  data: 0.5979  max mem: 6443\n",
      "Test: [Task 3]  [10/16]  eta: 0:00:09  Loss: 0.3120 (0.3290)  Acc@1: 90.6250 (92.3295)  Acc@5: 98.4375 (99.1477)  time: 1.5588  data: 0.0548  max mem: 6443\n",
      "Test: [Task 3]  [15/16]  eta: 0:00:01  Loss: 0.3254 (0.3746)  Acc@1: 90.6250 (91.3000)  Acc@5: 98.4375 (98.9000)  time: 1.5053  data: 0.0377  max mem: 6443\n",
      "Test: [Task 3] Total time: 0:00:24 (1.5106 s / it)\n",
      "* Acc@1 91.300 Acc@5 98.900 loss 0.375\n",
      "Test: [Task 4]  [ 0/16]  eta: 0:00:33  Loss: 0.4659 (0.4659)  Acc@1: 84.3750 (84.3750)  Acc@5: 96.8750 (96.8750)  time: 2.1064  data: 0.6321  max mem: 6443\n",
      "Test: [Task 4]  [10/16]  eta: 0:00:09  Loss: 0.3999 (0.3698)  Acc@1: 92.1875 (91.4773)  Acc@5: 98.4375 (98.5795)  time: 1.5623  data: 0.0579  max mem: 6443\n",
      "Test: [Task 4]  [15/16]  eta: 0:00:01  Loss: 0.4176 (0.3969)  Acc@1: 90.6250 (90.8000)  Acc@5: 97.5000 (98.3000)  time: 1.5077  data: 0.0399  max mem: 6443\n",
      "Test: [Task 4] Total time: 0:00:24 (1.5129 s / it)\n",
      "* Acc@1 90.800 Acc@5 98.300 loss 0.397\n",
      "[Average accuracy till task4]\tAcc@1: 90.3000\tAcc@5: 98.7750\tLoss: 0.4079\tForgetting: 5.0333\tBackward: -5.0333\n",
      "Train: Epoch[1/5]  [ 0/79]  eta: 0:03:40  Lr: 0.007500  Loss: 2.2873  Acc@1: 4.6875 (4.6875)  Acc@5: 42.1875 (42.1875)  time: 2.7946  data: 0.5789  max mem: 6443\n",
      "Train: Epoch[1/5]  [10/79]  eta: 0:02:37  Lr: 0.007500  Loss: 0.6707  Acc@1: 82.8125 (74.4318)  Acc@5: 96.8750 (91.1932)  time: 2.2839  data: 0.0534  max mem: 6443\n",
      "Train: Epoch[1/5]  [20/79]  eta: 0:02:13  Lr: 0.007500  Loss: 0.3817  Acc@1: 85.9375 (81.1756)  Acc@5: 98.4375 (95.2381)  time: 2.2373  data: 0.0008  max mem: 6443\n",
      "Train: Epoch[1/5]  [30/79]  eta: 0:01:50  Lr: 0.007500  Loss: 0.2894  Acc@1: 87.5000 (83.8206)  Acc@5: 100.0000 (96.5222)  time: 2.2471  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[1/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: 0.1994  Acc@1: 87.5000 (84.8323)  Acc@5: 98.4375 (97.0655)  time: 2.2540  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[1/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.0480  Acc@1: 89.0625 (85.9988)  Acc@5: 98.4375 (97.3958)  time: 2.2502  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[1/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: 0.1270  Acc@1: 89.0625 (86.7059)  Acc@5: 98.4375 (97.6947)  time: 2.2429  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[1/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: 0.0174  Acc@1: 89.0625 (87.1259)  Acc@5: 100.0000 (97.8873)  time: 2.2416  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[1/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.2507  Acc@1: 89.0625 (87.2800)  Acc@5: 100.0000 (97.9800)  time: 2.1453  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[1/5] Total time: 0:02:55 (2.2277 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.2507  Acc@1: 89.0625 (87.2800)  Acc@5: 100.0000 (97.9800)\n",
      "Train: Epoch[2/5]  [ 0/79]  eta: 0:03:37  Lr: 0.007500  Loss: -0.1041  Acc@1: 96.8750 (96.8750)  Acc@5: 98.4375 (98.4375)  time: 2.7593  data: 0.5296  max mem: 6443\n",
      "Train: Epoch[2/5]  [10/79]  eta: 0:02:37  Lr: 0.007500  Loss: -0.0329  Acc@1: 90.6250 (91.7614)  Acc@5: 100.0000 (99.0057)  time: 2.2879  data: 0.0488  max mem: 6443\n",
      "Train: Epoch[2/5]  [20/79]  eta: 0:02:13  Lr: 0.007500  Loss: 0.0170  Acc@1: 90.6250 (91.2202)  Acc@5: 100.0000 (99.0327)  time: 2.2421  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[2/5]  [30/79]  eta: 0:01:50  Lr: 0.007500  Loss: -0.0772  Acc@1: 90.6250 (91.1290)  Acc@5: 100.0000 (99.0423)  time: 2.2432  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[2/5]  [40/79]  eta: 0:01:27  Lr: 0.007500  Loss: -0.1351  Acc@1: 89.0625 (90.9299)  Acc@5: 100.0000 (99.0473)  time: 2.2423  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[2/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.1338  Acc@1: 90.6250 (90.9007)  Acc@5: 100.0000 (99.0502)  time: 2.2428  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[2/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: -0.2076  Acc@1: 92.1875 (91.0348)  Acc@5: 98.4375 (99.0523)  time: 2.2438  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[2/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.1055  Acc@1: 92.1875 (91.2632)  Acc@5: 100.0000 (99.1857)  time: 2.2443  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[2/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: 0.0759  Acc@1: 92.1875 (91.2200)  Acc@5: 100.0000 (99.1800)  time: 2.1465  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[2/5] Total time: 0:02:55 (2.2260 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: 0.0759  Acc@1: 92.1875 (91.2200)  Acc@5: 100.0000 (99.1800)\n",
      "Train: Epoch[3/5]  [ 0/79]  eta: 0:03:44  Lr: 0.007500  Loss: -0.2201  Acc@1: 92.1875 (92.1875)  Acc@5: 100.0000 (100.0000)  time: 2.8357  data: 0.6165  max mem: 6443\n",
      "Train: Epoch[3/5]  [10/79]  eta: 0:02:38  Lr: 0.007500  Loss: -0.0682  Acc@1: 90.6250 (90.6250)  Acc@5: 98.4375 (99.0057)  time: 2.2983  data: 0.0567  max mem: 6443\n",
      "Train: Epoch[3/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: -0.2105  Acc@1: 92.1875 (91.7411)  Acc@5: 100.0000 (99.2560)  time: 2.2473  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[3/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: -0.2569  Acc@1: 93.7500 (91.7843)  Acc@5: 100.0000 (99.0423)  time: 2.2530  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[3/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.0729  Acc@1: 92.1875 (91.8064)  Acc@5: 100.0000 (99.0473)  time: 2.2537  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[3/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.3496  Acc@1: 93.7500 (91.9118)  Acc@5: 100.0000 (99.2034)  time: 2.2501  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[3/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: -0.2059  Acc@1: 92.1875 (91.8801)  Acc@5: 100.0000 (99.1803)  time: 2.2475  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[3/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.2010  Acc@1: 92.1875 (91.8794)  Acc@5: 98.4375 (99.1417)  time: 2.2451  data: 0.0008  max mem: 6443\n",
      "Train: Epoch[3/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.3900  Acc@1: 93.7500 (91.9000)  Acc@5: 98.4375 (99.1200)  time: 2.1456  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[3/5] Total time: 0:02:56 (2.2319 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.3900  Acc@1: 93.7500 (91.9000)  Acc@5: 98.4375 (99.1200)\n",
      "Train: Epoch[4/5]  [ 0/79]  eta: 0:04:01  Lr: 0.007500  Loss: -0.0518  Acc@1: 84.3750 (84.3750)  Acc@5: 98.4375 (98.4375)  time: 3.0595  data: 0.8421  max mem: 6443\n",
      "Train: Epoch[4/5]  [10/79]  eta: 0:02:39  Lr: 0.007500  Loss: -0.2164  Acc@1: 93.7500 (92.0455)  Acc@5: 100.0000 (99.4318)  time: 2.3137  data: 0.0770  max mem: 6443\n",
      "Train: Epoch[4/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: -0.1032  Acc@1: 92.1875 (91.4435)  Acc@5: 100.0000 (99.4048)  time: 2.2432  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[4/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: -0.2112  Acc@1: 92.1875 (92.0363)  Acc@5: 100.0000 (99.3952)  time: 2.2502  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[4/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.2345  Acc@1: 92.1875 (91.9588)  Acc@5: 100.0000 (99.3902)  time: 2.2550  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[4/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.0889  Acc@1: 92.1875 (92.1875)  Acc@5: 100.0000 (99.4179)  time: 2.2534  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[4/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: -0.2941  Acc@1: 93.7500 (92.0594)  Acc@5: 100.0000 (99.4109)  time: 2.2464  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[4/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.1242  Acc@1: 90.6250 (92.1215)  Acc@5: 100.0000 (99.4058)  time: 2.2408  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[4/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.4422  Acc@1: 90.6250 (92.1800)  Acc@5: 100.0000 (99.3800)  time: 2.1395  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[4/5] Total time: 0:02:56 (2.2326 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.4422  Acc@1: 90.6250 (92.1800)  Acc@5: 100.0000 (99.3800)\n",
      "Train: Epoch[5/5]  [ 0/79]  eta: 0:03:45  Lr: 0.007500  Loss: -0.2780  Acc@1: 96.8750 (96.8750)  Acc@5: 100.0000 (100.0000)  time: 2.8496  data: 0.6343  max mem: 6443\n",
      "Train: Epoch[5/5]  [10/79]  eta: 0:02:38  Lr: 0.007500  Loss: -0.3124  Acc@1: 93.7500 (92.3295)  Acc@5: 98.4375 (98.2955)  time: 2.2919  data: 0.0580  max mem: 6443\n",
      "Train: Epoch[5/5]  [20/79]  eta: 0:02:13  Lr: 0.007500  Loss: -0.0885  Acc@1: 92.1875 (92.0387)  Acc@5: 100.0000 (98.8095)  time: 2.2409  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[5/5]  [30/79]  eta: 0:01:50  Lr: 0.007500  Loss: -0.0660  Acc@1: 92.1875 (91.8851)  Acc@5: 100.0000 (98.8911)  time: 2.2492  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[5/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.2078  Acc@1: 90.6250 (91.7302)  Acc@5: 98.4375 (98.8948)  time: 2.2538  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[5/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.1466  Acc@1: 90.6250 (91.7586)  Acc@5: 100.0000 (98.9890)  time: 2.2566  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[5/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: -0.2029  Acc@1: 92.1875 (91.9570)  Acc@5: 100.0000 (99.0266)  time: 2.2570  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[5/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.0876  Acc@1: 92.1875 (91.8354)  Acc@5: 98.4375 (98.9437)  time: 2.2546  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[5/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.3667  Acc@1: 90.6250 (91.7400)  Acc@5: 98.4375 (98.9200)  time: 2.1537  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[5/5] Total time: 0:02:56 (2.2347 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.3667  Acc@1: 90.6250 (91.7400)  Acc@5: 98.4375 (98.9200)\n",
      "Test: [Task 1]  [ 0/16]  eta: 0:00:33  Loss: 0.6373 (0.6373)  Acc@1: 78.1250 (78.1250)  Acc@5: 100.0000 (100.0000)  time: 2.0826  data: 0.6029  max mem: 6443\n",
      "Test: [Task 1]  [10/16]  eta: 0:00:09  Loss: 0.4383 (0.4533)  Acc@1: 89.0625 (87.2159)  Acc@5: 100.0000 (99.8580)  time: 1.5632  data: 0.0551  max mem: 6443\n",
      "Test: [Task 1]  [15/16]  eta: 0:00:01  Loss: 0.3888 (0.4213)  Acc@1: 87.5000 (88.0000)  Acc@5: 100.0000 (99.8000)  time: 1.5099  data: 0.0379  max mem: 6443\n",
      "Test: [Task 1] Total time: 0:00:24 (1.5147 s / it)\n",
      "* Acc@1 88.000 Acc@5 99.800 loss 0.421\n",
      "Test: [Task 2]  [ 0/16]  eta: 0:00:36  Loss: 0.3697 (0.3697)  Acc@1: 95.3125 (95.3125)  Acc@5: 98.4375 (98.4375)  time: 2.2694  data: 0.7970  max mem: 6443\n",
      "Test: [Task 2]  [10/16]  eta: 0:00:09  Loss: 0.5535 (0.5415)  Acc@1: 85.9375 (87.2159)  Acc@5: 98.4375 (97.4432)  time: 1.5821  data: 0.0728  max mem: 6443\n",
      "Test: [Task 2]  [15/16]  eta: 0:00:01  Loss: 0.4973 (0.5106)  Acc@1: 87.5000 (87.8000)  Acc@5: 98.4375 (97.9000)  time: 1.5239  data: 0.0501  max mem: 6443\n",
      "Test: [Task 2] Total time: 0:00:24 (1.5290 s / it)\n",
      "* Acc@1 87.800 Acc@5 97.900 loss 0.511\n",
      "Test: [Task 3]  [ 0/16]  eta: 0:00:32  Loss: 0.3994 (0.3994)  Acc@1: 87.5000 (87.5000)  Acc@5: 98.4375 (98.4375)  time: 2.0349  data: 0.5443  max mem: 6443\n",
      "Test: [Task 3]  [10/16]  eta: 0:00:09  Loss: 0.3994 (0.4050)  Acc@1: 90.6250 (89.7727)  Acc@5: 98.4375 (99.0057)  time: 1.5653  data: 0.0502  max mem: 6443\n",
      "Test: [Task 3]  [15/16]  eta: 0:00:01  Loss: 0.4449 (0.4480)  Acc@1: 87.5000 (89.0000)  Acc@5: 98.4375 (98.7000)  time: 1.5129  data: 0.0346  max mem: 6443\n",
      "Test: [Task 3] Total time: 0:00:24 (1.5191 s / it)\n",
      "* Acc@1 89.000 Acc@5 98.700 loss 0.448\n",
      "Test: [Task 4]  [ 0/16]  eta: 0:00:33  Loss: 0.4659 (0.4659)  Acc@1: 87.5000 (87.5000)  Acc@5: 98.4375 (98.4375)  time: 2.1104  data: 0.6234  max mem: 6443\n",
      "Test: [Task 4]  [10/16]  eta: 0:00:09  Loss: 0.4429 (0.4072)  Acc@1: 89.0625 (90.0568)  Acc@5: 98.4375 (97.8693)  time: 1.5697  data: 0.0570  max mem: 6443\n",
      "Test: [Task 4]  [15/16]  eta: 0:00:01  Loss: 0.4531 (0.4414)  Acc@1: 89.0625 (89.4000)  Acc@5: 97.5000 (97.6000)  time: 1.5147  data: 0.0393  max mem: 6443\n",
      "Test: [Task 4] Total time: 0:00:24 (1.5195 s / it)\n",
      "* Acc@1 89.400 Acc@5 97.600 loss 0.441\n",
      "Test: [Task 5]  [ 0/16]  eta: 0:00:31  Loss: 0.1726 (0.1726)  Acc@1: 98.4375 (98.4375)  Acc@5: 100.0000 (100.0000)  time: 1.9488  data: 0.4702  max mem: 6443\n",
      "Test: [Task 5]  [10/16]  eta: 0:00:09  Loss: 0.3414 (0.3305)  Acc@1: 92.1875 (91.9034)  Acc@5: 100.0000 (98.8636)  time: 1.5553  data: 0.0432  max mem: 6443\n",
      "Test: [Task 5]  [15/16]  eta: 0:00:01  Loss: 0.3111 (0.3524)  Acc@1: 92.1875 (91.6000)  Acc@5: 100.0000 (98.7000)  time: 1.5054  data: 0.0297  max mem: 6443\n",
      "Test: [Task 5] Total time: 0:00:24 (1.5105 s / it)\n",
      "* Acc@1 91.600 Acc@5 98.700 loss 0.352\n",
      "[Average accuracy till task5]\tAcc@1: 89.1600\tAcc@5: 98.5400\tLoss: 0.4347\tForgetting: 5.5250\tBackward: -5.5250\n",
      "Train: Epoch[1/5]  [ 0/79]  eta: 0:04:03  Lr: 0.007500  Loss: 2.3044  Acc@1: 14.0625 (14.0625)  Acc@5: 53.1250 (53.1250)  time: 3.0880  data: 0.8543  max mem: 6443\n",
      "Train: Epoch[1/5]  [10/79]  eta: 0:02:40  Lr: 0.007500  Loss: 0.6240  Acc@1: 85.9375 (76.2784)  Acc@5: 98.4375 (92.8977)  time: 2.3241  data: 0.0781  max mem: 6443\n",
      "Train: Epoch[1/5]  [20/79]  eta: 0:02:15  Lr: 0.007500  Loss: 0.2868  Acc@1: 84.3750 (80.3571)  Acc@5: 98.4375 (95.7589)  time: 2.2516  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[1/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: 0.1314  Acc@1: 84.3750 (82.6109)  Acc@5: 100.0000 (97.0766)  time: 2.2549  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[1/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: 0.4024  Acc@1: 89.0625 (83.9939)  Acc@5: 100.0000 (97.4848)  time: 2.2546  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[1/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: 0.1825  Acc@1: 89.0625 (85.0490)  Acc@5: 98.4375 (97.8248)  time: 2.2554  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[1/5]  [60/79]  eta: 0:00:43  Lr: 0.007500  Loss: 0.1171  Acc@1: 89.0625 (85.5789)  Acc@5: 98.4375 (98.0277)  time: 2.2541  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[1/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: 0.0274  Acc@1: 87.5000 (86.1796)  Acc@5: 98.4375 (98.1074)  time: 2.2512  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[1/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: 0.1770  Acc@1: 87.5000 (86.2800)  Acc@5: 98.4375 (98.2200)  time: 2.1520  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[1/5] Total time: 0:02:56 (2.2394 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: 0.1770  Acc@1: 87.5000 (86.2800)  Acc@5: 98.4375 (98.2200)\n",
      "Train: Epoch[2/5]  [ 0/79]  eta: 0:03:45  Lr: 0.007500  Loss: 0.0169  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 2.8584  data: 0.6365  max mem: 6443\n",
      "Train: Epoch[2/5]  [10/79]  eta: 0:02:38  Lr: 0.007500  Loss: -0.0193  Acc@1: 89.0625 (89.2045)  Acc@5: 100.0000 (99.4318)  time: 2.3035  data: 0.0586  max mem: 6443\n",
      "Train: Epoch[2/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: -0.0247  Acc@1: 89.0625 (89.8810)  Acc@5: 98.4375 (99.1071)  time: 2.2496  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[2/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: -0.0008  Acc@1: 90.6250 (89.6169)  Acc@5: 100.0000 (99.2440)  time: 2.2525  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[2/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.2551  Acc@1: 89.0625 (89.6341)  Acc@5: 100.0000 (99.2759)  time: 2.2551  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[2/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.0483  Acc@1: 89.0625 (89.6752)  Acc@5: 100.0000 (99.3260)  time: 2.2561  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[2/5]  [60/79]  eta: 0:00:43  Lr: 0.007500  Loss: -0.1800  Acc@1: 89.0625 (89.7541)  Acc@5: 100.0000 (99.3084)  time: 2.2556  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[2/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.1167  Acc@1: 90.6250 (89.7887)  Acc@5: 100.0000 (99.2958)  time: 2.2548  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[2/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.3595  Acc@1: 89.0625 (89.6600)  Acc@5: 98.4375 (99.1800)  time: 2.1575  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[2/5] Total time: 0:02:56 (2.2379 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.3595  Acc@1: 89.0625 (89.6600)  Acc@5: 98.4375 (99.1800)\n",
      "Train: Epoch[3/5]  [ 0/79]  eta: 0:03:48  Lr: 0.007500  Loss: -0.0496  Acc@1: 90.6250 (90.6250)  Acc@5: 98.4375 (98.4375)  time: 2.8964  data: 0.6597  max mem: 6443\n",
      "Train: Epoch[3/5]  [10/79]  eta: 0:02:39  Lr: 0.007500  Loss: -0.1035  Acc@1: 90.6250 (89.6307)  Acc@5: 100.0000 (99.4318)  time: 2.3100  data: 0.0603  max mem: 6443\n",
      "Train: Epoch[3/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: 0.0566  Acc@1: 89.0625 (89.8065)  Acc@5: 100.0000 (99.6280)  time: 2.2539  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[3/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: -0.0902  Acc@1: 90.6250 (90.2722)  Acc@5: 100.0000 (99.4960)  time: 2.2567  data: 0.0008  max mem: 6443\n",
      "Train: Epoch[3/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.0842  Acc@1: 92.1875 (90.4726)  Acc@5: 100.0000 (99.3902)  time: 2.2591  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[3/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.1365  Acc@1: 90.6250 (90.3493)  Acc@5: 100.0000 (99.3566)  time: 2.2599  data: 0.0008  max mem: 6443\n",
      "Train: Epoch[3/5]  [60/79]  eta: 0:00:43  Lr: 0.007500  Loss: -0.1576  Acc@1: 89.0625 (90.1127)  Acc@5: 100.0000 (99.2828)  time: 2.2569  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[3/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.2223  Acc@1: 89.0625 (90.2289)  Acc@5: 100.0000 (99.2958)  time: 2.2543  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[3/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.4104  Acc@1: 90.6250 (90.3200)  Acc@5: 100.0000 (99.3400)  time: 2.1557  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[3/5] Total time: 0:02:57 (2.2407 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.4104  Acc@1: 90.6250 (90.3200)  Acc@5: 100.0000 (99.3400)\n",
      "Train: Epoch[4/5]  [ 0/79]  eta: 0:04:07  Lr: 0.007500  Loss: -0.2463  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 3.1316  data: 0.9139  max mem: 6443\n",
      "Train: Epoch[4/5]  [10/79]  eta: 0:02:40  Lr: 0.007500  Loss: -0.1473  Acc@1: 92.1875 (92.3295)  Acc@5: 100.0000 (99.2898)  time: 2.3312  data: 0.0836  max mem: 6443\n",
      "Train: Epoch[4/5]  [20/79]  eta: 0:02:15  Lr: 0.007500  Loss: -0.0766  Acc@1: 92.1875 (92.0387)  Acc@5: 100.0000 (99.4048)  time: 2.2547  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[4/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: -0.2352  Acc@1: 92.1875 (91.8347)  Acc@5: 100.0000 (99.5968)  time: 2.2573  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[4/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.2926  Acc@1: 92.1875 (91.6159)  Acc@5: 100.0000 (99.5046)  time: 2.2524  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[4/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.1280  Acc@1: 92.1875 (91.7892)  Acc@5: 100.0000 (99.5711)  time: 2.2451  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[4/5]  [60/79]  eta: 0:00:43  Lr: 0.007500  Loss: -0.1652  Acc@1: 90.6250 (91.7264)  Acc@5: 100.0000 (99.5389)  time: 2.2397  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[4/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.1020  Acc@1: 92.1875 (91.7914)  Acc@5: 100.0000 (99.5819)  time: 2.2390  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[4/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.2415  Acc@1: 92.1875 (91.6000)  Acc@5: 100.0000 (99.5600)  time: 2.1435  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[4/5] Total time: 0:02:56 (2.2348 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.2415  Acc@1: 92.1875 (91.6000)  Acc@5: 100.0000 (99.5600)\n",
      "Train: Epoch[5/5]  [ 0/79]  eta: 0:03:34  Lr: 0.007500  Loss: -0.1404  Acc@1: 90.6250 (90.6250)  Acc@5: 100.0000 (100.0000)  time: 2.7128  data: 0.5003  max mem: 6443\n",
      "Train: Epoch[5/5]  [10/79]  eta: 0:02:37  Lr: 0.007500  Loss: -0.2164  Acc@1: 90.6250 (91.3352)  Acc@5: 100.0000 (99.1477)  time: 2.2846  data: 0.0458  max mem: 6443\n",
      "Train: Epoch[5/5]  [20/79]  eta: 0:02:13  Lr: 0.007500  Loss: -0.3213  Acc@1: 92.1875 (91.7411)  Acc@5: 100.0000 (99.2560)  time: 2.2455  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[5/5]  [30/79]  eta: 0:01:50  Lr: 0.007500  Loss: -0.0417  Acc@1: 92.1875 (91.4819)  Acc@5: 100.0000 (99.2944)  time: 2.2522  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[5/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.2237  Acc@1: 90.6250 (91.3491)  Acc@5: 100.0000 (99.3521)  time: 2.2556  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[5/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.1408  Acc@1: 90.6250 (90.9926)  Acc@5: 98.4375 (99.1422)  time: 2.2550  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[5/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: -0.1637  Acc@1: 90.6250 (90.9068)  Acc@5: 98.4375 (99.0010)  time: 2.2526  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[5/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.2173  Acc@1: 92.1875 (91.1092)  Acc@5: 100.0000 (99.1197)  time: 2.2500  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[5/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.3556  Acc@1: 93.7500 (91.2600)  Acc@5: 100.0000 (99.1400)  time: 2.1503  data: 0.0003  max mem: 6443\n",
      "Train: Epoch[5/5] Total time: 0:02:56 (2.2328 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.3556  Acc@1: 93.7500 (91.2600)  Acc@5: 100.0000 (99.1400)\n",
      "Test: [Task 1]  [ 0/16]  eta: 0:00:30  Loss: 0.6686 (0.6686)  Acc@1: 79.6875 (79.6875)  Acc@5: 96.8750 (96.8750)  time: 1.9173  data: 0.4436  max mem: 6443\n",
      "Test: [Task 1]  [10/16]  eta: 0:00:09  Loss: 0.4513 (0.4969)  Acc@1: 89.0625 (86.0795)  Acc@5: 100.0000 (99.4318)  time: 1.5472  data: 0.0406  max mem: 6443\n",
      "Test: [Task 1]  [15/16]  eta: 0:00:01  Loss: 0.4116 (0.4631)  Acc@1: 89.0625 (87.4000)  Acc@5: 100.0000 (99.4000)  time: 1.4990  data: 0.0280  max mem: 6443\n",
      "Test: [Task 1] Total time: 0:00:24 (1.5064 s / it)\n",
      "* Acc@1 87.400 Acc@5 99.400 loss 0.463\n",
      "Test: [Task 2]  [ 0/16]  eta: 0:00:36  Loss: 0.4116 (0.4116)  Acc@1: 95.3125 (95.3125)  Acc@5: 98.4375 (98.4375)  time: 2.2757  data: 0.8007  max mem: 6443\n",
      "Test: [Task 2]  [10/16]  eta: 0:00:09  Loss: 0.5771 (0.5938)  Acc@1: 84.3750 (85.5114)  Acc@5: 98.4375 (96.7330)  time: 1.5819  data: 0.0732  max mem: 6443\n",
      "Test: [Task 2]  [15/16]  eta: 0:00:01  Loss: 0.5497 (0.5608)  Acc@1: 84.3750 (85.8000)  Acc@5: 98.4375 (97.3000)  time: 1.5235  data: 0.0504  max mem: 6443\n",
      "Test: [Task 2] Total time: 0:00:24 (1.5322 s / it)\n",
      "* Acc@1 85.800 Acc@5 97.300 loss 0.561\n",
      "Test: [Task 3]  [ 0/16]  eta: 0:00:39  Loss: 0.4435 (0.4435)  Acc@1: 87.5000 (87.5000)  Acc@5: 98.4375 (98.4375)  time: 2.4907  data: 1.0127  max mem: 6443\n",
      "Test: [Task 3]  [10/16]  eta: 0:00:09  Loss: 0.4397 (0.4263)  Acc@1: 90.6250 (89.4886)  Acc@5: 98.4375 (98.7216)  time: 1.6028  data: 0.0924  max mem: 6443\n",
      "Test: [Task 3]  [15/16]  eta: 0:00:01  Loss: 0.4435 (0.4742)  Acc@1: 87.5000 (88.8000)  Acc@5: 98.4375 (98.1000)  time: 1.5380  data: 0.0636  max mem: 6443\n",
      "Test: [Task 3] Total time: 0:00:24 (1.5446 s / it)\n",
      "* Acc@1 88.800 Acc@5 98.100 loss 0.474\n",
      "Test: [Task 4]  [ 0/16]  eta: 0:00:43  Loss: 0.5663 (0.5663)  Acc@1: 82.8125 (82.8125)  Acc@5: 96.8750 (96.8750)  time: 2.7230  data: 1.2450  max mem: 6443\n",
      "Test: [Task 4]  [10/16]  eta: 0:00:09  Loss: 0.4617 (0.4578)  Acc@1: 89.0625 (88.6364)  Acc@5: 96.8750 (97.1591)  time: 1.6267  data: 0.1135  max mem: 6443\n",
      "Test: [Task 4]  [15/16]  eta: 0:00:01  Loss: 0.4860 (0.4943)  Acc@1: 89.0625 (87.9000)  Acc@5: 96.8750 (97.0000)  time: 1.5551  data: 0.0781  max mem: 6443\n",
      "Test: [Task 4] Total time: 0:00:25 (1.5633 s / it)\n",
      "* Acc@1 87.900 Acc@5 97.000 loss 0.494\n",
      "Test: [Task 5]  [ 0/16]  eta: 0:00:42  Loss: 0.1918 (0.1918)  Acc@1: 98.4375 (98.4375)  Acc@5: 100.0000 (100.0000)  time: 2.6548  data: 1.1766  max mem: 6443\n",
      "Test: [Task 5]  [10/16]  eta: 0:00:09  Loss: 0.3793 (0.3835)  Acc@1: 90.6250 (90.7670)  Acc@5: 98.4375 (98.8636)  time: 1.6200  data: 0.1073  max mem: 6443\n",
      "Test: [Task 5]  [15/16]  eta: 0:00:01  Loss: 0.3620 (0.4029)  Acc@1: 90.6250 (90.3000)  Acc@5: 98.4375 (98.7000)  time: 1.5503  data: 0.0739  max mem: 6443\n",
      "Test: [Task 5] Total time: 0:00:24 (1.5590 s / it)\n",
      "* Acc@1 90.300 Acc@5 98.700 loss 0.403\n",
      "Test: [Task 6]  [ 0/16]  eta: 0:00:42  Loss: 0.4562 (0.4562)  Acc@1: 87.5000 (87.5000)  Acc@5: 98.4375 (98.4375)  time: 2.6264  data: 1.1414  max mem: 6443\n",
      "Test: [Task 6]  [10/16]  eta: 0:00:09  Loss: 0.4658 (0.4765)  Acc@1: 85.9375 (84.9432)  Acc@5: 98.4375 (99.1477)  time: 1.6177  data: 0.1043  max mem: 6443\n",
      "Test: [Task 6]  [15/16]  eta: 0:00:01  Loss: 0.4801 (0.4867)  Acc@1: 84.3750 (85.1000)  Acc@5: 98.4375 (98.9000)  time: 1.5493  data: 0.0717  max mem: 6443\n",
      "Test: [Task 6] Total time: 0:00:24 (1.5570 s / it)\n",
      "* Acc@1 85.100 Acc@5 98.900 loss 0.487\n",
      "[Average accuracy till task6]\tAcc@1: 87.5500\tAcc@5: 98.2333\tLoss: 0.4803\tForgetting: 5.5400\tBackward: -5.5400\n",
      "Train: Epoch[1/5]  [ 0/79]  eta: 0:04:04  Lr: 0.007500  Loss: 2.2529  Acc@1: 12.5000 (12.5000)  Acc@5: 65.6250 (65.6250)  time: 3.0889  data: 0.8286  max mem: 6443\n",
      "Train: Epoch[1/5]  [10/79]  eta: 0:02:40  Lr: 0.007500  Loss: 0.6412  Acc@1: 78.1250 (73.8636)  Acc@5: 98.4375 (93.3239)  time: 2.3311  data: 0.0760  max mem: 6443\n",
      "Train: Epoch[1/5]  [20/79]  eta: 0:02:15  Lr: 0.007500  Loss: 0.3936  Acc@1: 84.3750 (79.3899)  Acc@5: 98.4375 (95.7589)  time: 2.2584  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[1/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: -0.0395  Acc@1: 85.9375 (81.7036)  Acc@5: 98.4375 (96.7238)  time: 2.2493  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[1/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: 0.2011  Acc@1: 87.5000 (83.2698)  Acc@5: 98.4375 (97.2561)  time: 2.2378  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[1/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: 0.1116  Acc@1: 89.0625 (84.2218)  Acc@5: 98.4375 (97.6103)  time: 2.2428  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[1/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: 0.1795  Acc@1: 89.0625 (85.0154)  Acc@5: 98.4375 (97.8227)  time: 2.2508  data: 0.0008  max mem: 6443\n",
      "Train: Epoch[1/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: 0.2221  Acc@1: 89.0625 (85.4533)  Acc@5: 98.4375 (98.0194)  time: 2.2568  data: 0.0009  max mem: 6443\n",
      "Train: Epoch[1/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: 0.6400  Acc@1: 89.0625 (85.8200)  Acc@5: 98.4375 (98.1400)  time: 2.1586  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[1/5] Total time: 0:02:56 (2.2377 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: 0.6400  Acc@1: 89.0625 (85.8200)  Acc@5: 98.4375 (98.1400)\n",
      "Train: Epoch[2/5]  [ 0/79]  eta: 0:03:55  Lr: 0.007500  Loss: 0.0111  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 2.9834  data: 0.7518  max mem: 6443\n",
      "Train: Epoch[2/5]  [10/79]  eta: 0:02:39  Lr: 0.007500  Loss: -0.1000  Acc@1: 90.6250 (90.1989)  Acc@5: 100.0000 (99.2898)  time: 2.3147  data: 0.0691  max mem: 6443\n",
      "Train: Epoch[2/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: -0.0078  Acc@1: 90.6250 (90.2530)  Acc@5: 100.0000 (99.3304)  time: 2.2478  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[2/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: -0.0378  Acc@1: 89.0625 (89.5665)  Acc@5: 100.0000 (99.2440)  time: 2.2471  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[2/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.0816  Acc@1: 89.0625 (89.4436)  Acc@5: 98.4375 (99.2759)  time: 2.2460  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[2/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: 0.0265  Acc@1: 90.6250 (89.6752)  Acc@5: 98.4375 (99.2341)  time: 2.2466  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[2/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: -0.1724  Acc@1: 89.0625 (89.3443)  Acc@5: 98.4375 (99.1035)  time: 2.2476  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[2/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.2568  Acc@1: 87.5000 (89.3706)  Acc@5: 98.4375 (99.0757)  time: 2.2472  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[2/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.4562  Acc@1: 89.0625 (89.5400)  Acc@5: 100.0000 (99.1200)  time: 2.1490  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[2/5] Total time: 0:02:56 (2.2326 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.4562  Acc@1: 89.0625 (89.5400)  Acc@5: 100.0000 (99.1200)\n",
      "Train: Epoch[3/5]  [ 0/79]  eta: 0:03:53  Lr: 0.007500  Loss: -0.1032  Acc@1: 90.6250 (90.6250)  Acc@5: 98.4375 (98.4375)  time: 2.9602  data: 0.7250  max mem: 6443\n",
      "Train: Epoch[3/5]  [10/79]  eta: 0:02:39  Lr: 0.007500  Loss: -0.0464  Acc@1: 90.6250 (89.4886)  Acc@5: 98.4375 (98.8636)  time: 2.3099  data: 0.0666  max mem: 6443\n",
      "Train: Epoch[3/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: -0.2523  Acc@1: 89.0625 (89.9554)  Acc@5: 100.0000 (98.9583)  time: 2.2444  data: 0.0008  max mem: 6443\n",
      "Train: Epoch[3/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: -0.1405  Acc@1: 89.0625 (90.2722)  Acc@5: 100.0000 (99.1431)  time: 2.2436  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[3/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.3169  Acc@1: 90.6250 (90.4726)  Acc@5: 100.0000 (99.1616)  time: 2.2439  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[3/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.1403  Acc@1: 92.1875 (90.7475)  Acc@5: 100.0000 (99.2341)  time: 2.2444  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[3/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: -0.2651  Acc@1: 92.1875 (90.7787)  Acc@5: 100.0000 (99.2572)  time: 2.2463  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[3/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.0817  Acc@1: 89.0625 (90.2949)  Acc@5: 98.4375 (99.1637)  time: 2.2482  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[3/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.1456  Acc@1: 89.0625 (90.2800)  Acc@5: 98.4375 (99.1400)  time: 2.1510  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[3/5] Total time: 0:02:56 (2.2312 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.1456  Acc@1: 89.0625 (90.2800)  Acc@5: 98.4375 (99.1400)\n",
      "Train: Epoch[4/5]  [ 0/79]  eta: 0:03:59  Lr: 0.007500  Loss: -0.2259  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 3.0345  data: 0.8155  max mem: 6443\n",
      "Train: Epoch[4/5]  [10/79]  eta: 0:02:39  Lr: 0.007500  Loss: -0.3081  Acc@1: 93.7500 (92.4716)  Acc@5: 100.0000 (99.8580)  time: 2.3167  data: 0.0746  max mem: 6443\n",
      "Train: Epoch[4/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: -0.2346  Acc@1: 92.1875 (92.4107)  Acc@5: 100.0000 (99.7768)  time: 2.2484  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[4/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: -0.3571  Acc@1: 92.1875 (91.6835)  Acc@5: 100.0000 (99.5464)  time: 2.2547  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[4/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.2426  Acc@1: 90.6250 (91.8826)  Acc@5: 100.0000 (99.5046)  time: 2.2568  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[4/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.2343  Acc@1: 89.0625 (91.0233)  Acc@5: 100.0000 (99.3873)  time: 2.2555  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[4/5]  [60/79]  eta: 0:00:43  Lr: 0.007500  Loss: -0.2056  Acc@1: 87.5000 (90.7787)  Acc@5: 98.4375 (99.3340)  time: 2.2547  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[4/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.1993  Acc@1: 90.6250 (91.0431)  Acc@5: 100.0000 (99.3838)  time: 2.2532  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[4/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.2170  Acc@1: 92.1875 (91.1600)  Acc@5: 100.0000 (99.4000)  time: 2.1531  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[4/5] Total time: 0:02:56 (2.2389 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.2170  Acc@1: 92.1875 (91.1600)  Acc@5: 100.0000 (99.4000)\n",
      "Train: Epoch[5/5]  [ 0/79]  eta: 0:03:38  Lr: 0.007500  Loss: -0.1467  Acc@1: 85.9375 (85.9375)  Acc@5: 98.4375 (98.4375)  time: 2.7702  data: 0.5391  max mem: 6443\n",
      "Train: Epoch[5/5]  [10/79]  eta: 0:02:38  Lr: 0.007500  Loss: -0.1485  Acc@1: 92.1875 (91.3352)  Acc@5: 98.4375 (99.1477)  time: 2.2942  data: 0.0499  max mem: 6443\n",
      "Train: Epoch[5/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: -0.1606  Acc@1: 92.1875 (91.5179)  Acc@5: 100.0000 (99.4792)  time: 2.2472  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[5/5]  [30/79]  eta: 0:01:50  Lr: 0.007500  Loss: -0.3099  Acc@1: 90.6250 (91.0786)  Acc@5: 100.0000 (99.5968)  time: 2.2463  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[5/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.1126  Acc@1: 90.6250 (91.1585)  Acc@5: 100.0000 (99.5808)  time: 2.2457  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[5/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.0702  Acc@1: 92.1875 (91.5135)  Acc@5: 98.4375 (99.4485)  time: 2.2463  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[5/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: -0.1078  Acc@1: 92.1875 (91.5727)  Acc@5: 98.4375 (99.4109)  time: 2.2453  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[5/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.2046  Acc@1: 92.1875 (91.3952)  Acc@5: 100.0000 (99.3618)  time: 2.2448  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[5/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.4066  Acc@1: 92.1875 (91.5000)  Acc@5: 100.0000 (99.4000)  time: 2.1478  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[5/5] Total time: 0:02:56 (2.2296 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.4066  Acc@1: 92.1875 (91.5000)  Acc@5: 100.0000 (99.4000)\n",
      "Test: [Task 1]  [ 0/16]  eta: 0:00:42  Loss: 0.7083 (0.7083)  Acc@1: 76.5625 (76.5625)  Acc@5: 98.4375 (98.4375)  time: 2.6799  data: 1.1984  max mem: 6443\n",
      "Test: [Task 1]  [10/16]  eta: 0:00:09  Loss: 0.4911 (0.5280)  Acc@1: 85.9375 (85.2273)  Acc@5: 100.0000 (99.2898)  time: 1.6174  data: 0.1093  max mem: 6443\n",
      "Test: [Task 1]  [15/16]  eta: 0:00:01  Loss: 0.4454 (0.4943)  Acc@1: 87.5000 (86.6000)  Acc@5: 100.0000 (99.3000)  time: 1.5475  data: 0.0752  max mem: 6443\n",
      "Test: [Task 1] Total time: 0:00:24 (1.5559 s / it)\n",
      "* Acc@1 86.600 Acc@5 99.300 loss 0.494\n",
      "Test: [Task 2]  [ 0/16]  eta: 0:00:41  Loss: 0.4222 (0.4222)  Acc@1: 93.7500 (93.7500)  Acc@5: 98.4375 (98.4375)  time: 2.6249  data: 1.1516  max mem: 6443\n",
      "Test: [Task 2]  [10/16]  eta: 0:00:09  Loss: 0.6252 (0.6312)  Acc@1: 82.8125 (84.0909)  Acc@5: 98.4375 (96.7330)  time: 1.6161  data: 0.1052  max mem: 6443\n",
      "Test: [Task 2]  [15/16]  eta: 0:00:01  Loss: 0.5859 (0.5940)  Acc@1: 84.3750 (85.4000)  Acc@5: 98.4375 (97.2000)  time: 1.5477  data: 0.0724  max mem: 6443\n",
      "Test: [Task 2] Total time: 0:00:24 (1.5556 s / it)\n",
      "* Acc@1 85.400 Acc@5 97.200 loss 0.594\n",
      "Test: [Task 3]  [ 0/16]  eta: 0:00:41  Loss: 0.5208 (0.5208)  Acc@1: 82.8125 (82.8125)  Acc@5: 98.4375 (98.4375)  time: 2.5876  data: 1.0871  max mem: 6443\n",
      "Test: [Task 3]  [10/16]  eta: 0:00:09  Loss: 0.4757 (0.4696)  Acc@1: 87.5000 (86.7898)  Acc@5: 98.4375 (98.5795)  time: 1.6142  data: 0.0994  max mem: 6443\n",
      "Test: [Task 3]  [15/16]  eta: 0:00:01  Loss: 0.4757 (0.5267)  Acc@1: 85.9375 (86.4000)  Acc@5: 98.4375 (97.5000)  time: 1.5463  data: 0.0684  max mem: 6443\n",
      "Test: [Task 3] Total time: 0:00:24 (1.5518 s / it)\n",
      "* Acc@1 86.400 Acc@5 97.500 loss 0.527\n",
      "Test: [Task 4]  [ 0/16]  eta: 0:00:40  Loss: 0.6398 (0.6398)  Acc@1: 79.6875 (79.6875)  Acc@5: 95.3125 (95.3125)  time: 2.5249  data: 1.0398  max mem: 6443\n",
      "Test: [Task 4]  [10/16]  eta: 0:00:09  Loss: 0.5182 (0.4761)  Acc@1: 89.0625 (88.4943)  Acc@5: 96.8750 (97.0170)  time: 1.6096  data: 0.0950  max mem: 6443\n",
      "Test: [Task 4]  [15/16]  eta: 0:00:01  Loss: 0.5182 (0.5115)  Acc@1: 87.5000 (87.3000)  Acc@5: 96.8750 (96.9000)  time: 1.5424  data: 0.0654  max mem: 6443\n",
      "Test: [Task 4] Total time: 0:00:24 (1.5497 s / it)\n",
      "* Acc@1 87.300 Acc@5 96.900 loss 0.511\n",
      "Test: [Task 5]  [ 0/16]  eta: 0:00:34  Loss: 0.2655 (0.2655)  Acc@1: 95.3125 (95.3125)  Acc@5: 100.0000 (100.0000)  time: 2.1674  data: 0.6755  max mem: 6443\n",
      "Test: [Task 5]  [10/16]  eta: 0:00:09  Loss: 0.4407 (0.4208)  Acc@1: 90.6250 (89.6307)  Acc@5: 98.4375 (98.2955)  time: 1.5745  data: 0.0619  max mem: 6443\n",
      "Test: [Task 5]  [15/16]  eta: 0:00:01  Loss: 0.4407 (0.4541)  Acc@1: 87.5000 (88.5000)  Acc@5: 98.4375 (98.0000)  time: 1.5173  data: 0.0426  max mem: 6443\n",
      "Test: [Task 5] Total time: 0:00:24 (1.5227 s / it)\n",
      "* Acc@1 88.500 Acc@5 98.000 loss 0.454\n",
      "Test: [Task 6]  [ 0/16]  eta: 0:00:34  Loss: 0.5157 (0.5157)  Acc@1: 85.9375 (85.9375)  Acc@5: 96.8750 (96.8750)  time: 2.1374  data: 0.6582  max mem: 6443\n",
      "Test: [Task 6]  [10/16]  eta: 0:00:09  Loss: 0.5157 (0.5148)  Acc@1: 84.3750 (84.8011)  Acc@5: 98.4375 (98.8636)  time: 1.5664  data: 0.0601  max mem: 6443\n",
      "Test: [Task 6]  [15/16]  eta: 0:00:01  Loss: 0.5157 (0.5271)  Acc@1: 84.3750 (84.1000)  Acc@5: 98.4375 (98.8000)  time: 1.5113  data: 0.0414  max mem: 6443\n",
      "Test: [Task 6] Total time: 0:00:24 (1.5172 s / it)\n",
      "* Acc@1 84.100 Acc@5 98.800 loss 0.527\n",
      "Test: [Task 7]  [ 0/16]  eta: 0:00:30  Loss: 0.2786 (0.2786)  Acc@1: 93.7500 (93.7500)  Acc@5: 98.4375 (98.4375)  time: 1.9035  data: 0.4250  max mem: 6443\n",
      "Test: [Task 7]  [10/16]  eta: 0:00:09  Loss: 0.4277 (0.4315)  Acc@1: 90.6250 (88.9205)  Acc@5: 100.0000 (98.2955)  time: 1.5476  data: 0.0394  max mem: 6443\n",
      "Test: [Task 7]  [15/16]  eta: 0:00:01  Loss: 0.4277 (0.4444)  Acc@1: 89.0625 (88.4000)  Acc@5: 98.4375 (98.0000)  time: 1.4996  data: 0.0272  max mem: 6443\n",
      "Test: [Task 7] Total time: 0:00:24 (1.5051 s / it)\n",
      "* Acc@1 88.400 Acc@5 98.000 loss 0.444\n",
      "[Average accuracy till task7]\tAcc@1: 86.6714\tAcc@5: 97.9571\tLoss: 0.5074\tForgetting: 5.7833\tBackward: -5.7833\n",
      "Train: Epoch[1/5]  [ 0/79]  eta: 0:03:40  Lr: 0.007500  Loss: 2.2962  Acc@1: 20.3125 (20.3125)  Acc@5: 60.9375 (60.9375)  time: 2.7967  data: 0.5698  max mem: 6443\n",
      "Train: Epoch[1/5]  [10/79]  eta: 0:02:38  Lr: 0.007500  Loss: 0.5028  Acc@1: 82.8125 (77.9830)  Acc@5: 96.8750 (94.0341)  time: 2.2959  data: 0.0525  max mem: 6443\n",
      "Train: Epoch[1/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: 0.3127  Acc@1: 84.3750 (82.8125)  Acc@5: 98.4375 (95.9821)  time: 2.2499  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[1/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: 0.2759  Acc@1: 89.0625 (84.8286)  Acc@5: 98.4375 (96.4214)  time: 2.2551  data: 0.0008  max mem: 6443\n",
      "Train: Epoch[1/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: 0.2160  Acc@1: 87.5000 (85.5183)  Acc@5: 98.4375 (97.0274)  time: 2.2523  data: 0.0009  max mem: 6443\n",
      "Train: Epoch[1/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: 0.1383  Acc@1: 87.5000 (85.9681)  Acc@5: 98.4375 (97.3346)  time: 2.2458  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[1/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: 0.0920  Acc@1: 89.0625 (86.8596)  Acc@5: 98.4375 (97.6691)  time: 2.2414  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[1/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: 0.1251  Acc@1: 89.0625 (86.9938)  Acc@5: 100.0000 (97.9093)  time: 2.2412  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[1/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.0744  Acc@1: 90.6250 (87.5400)  Acc@5: 100.0000 (98.0400)  time: 2.1451  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[1/5] Total time: 0:02:56 (2.2301 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.0744  Acc@1: 90.6250 (87.5400)  Acc@5: 100.0000 (98.0400)\n",
      "Train: Epoch[2/5]  [ 0/79]  eta: 0:03:41  Lr: 0.007500  Loss: 0.0120  Acc@1: 89.0625 (89.0625)  Acc@5: 96.8750 (96.8750)  time: 2.8019  data: 0.5683  max mem: 6443\n",
      "Train: Epoch[2/5]  [10/79]  eta: 0:02:38  Lr: 0.007500  Loss: -0.0120  Acc@1: 89.0625 (89.3466)  Acc@5: 98.4375 (99.0057)  time: 2.2952  data: 0.0520  max mem: 6443\n",
      "Train: Epoch[2/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: -0.0384  Acc@1: 90.6250 (89.8810)  Acc@5: 98.4375 (98.8095)  time: 2.2452  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[2/5]  [30/79]  eta: 0:01:50  Lr: 0.007500  Loss: -0.0635  Acc@1: 90.6250 (90.4738)  Acc@5: 100.0000 (98.9919)  time: 2.2455  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[2/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.1806  Acc@1: 90.6250 (90.7012)  Acc@5: 98.4375 (98.8948)  time: 2.2457  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[2/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.1094  Acc@1: 90.6250 (90.9926)  Acc@5: 100.0000 (98.9890)  time: 2.2456  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[2/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: -0.1252  Acc@1: 89.0625 (90.4713)  Acc@5: 98.4375 (98.8986)  time: 2.2447  data: 0.0008  max mem: 6443\n",
      "Train: Epoch[2/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.1058  Acc@1: 89.0625 (90.3389)  Acc@5: 98.4375 (98.8996)  time: 2.2436  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[2/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.2534  Acc@1: 90.6250 (90.4000)  Acc@5: 98.4375 (98.9200)  time: 2.1443  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[2/5] Total time: 0:02:56 (2.2281 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.2534  Acc@1: 90.6250 (90.4000)  Acc@5: 98.4375 (98.9200)\n",
      "Train: Epoch[3/5]  [ 0/79]  eta: 0:04:18  Lr: 0.007500  Loss: -0.2097  Acc@1: 93.7500 (93.7500)  Acc@5: 98.4375 (98.4375)  time: 3.2736  data: 1.0519  max mem: 6443\n",
      "Train: Epoch[3/5]  [10/79]  eta: 0:02:40  Lr: 0.007500  Loss: -0.1404  Acc@1: 90.6250 (90.9091)  Acc@5: 100.0000 (99.2898)  time: 2.3320  data: 0.0960  max mem: 6443\n",
      "Train: Epoch[3/5]  [20/79]  eta: 0:02:15  Lr: 0.007500  Loss: -0.0267  Acc@1: 90.6250 (91.2202)  Acc@5: 100.0000 (99.1071)  time: 2.2415  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[3/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: -0.2761  Acc@1: 90.6250 (91.8347)  Acc@5: 100.0000 (99.2440)  time: 2.2483  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[3/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.1257  Acc@1: 92.1875 (91.9588)  Acc@5: 100.0000 (99.3140)  time: 2.2536  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[3/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.0272  Acc@1: 92.1875 (91.7279)  Acc@5: 100.0000 (99.3566)  time: 2.2543  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[3/5]  [60/79]  eta: 0:00:43  Lr: 0.007500  Loss: -0.2868  Acc@1: 90.6250 (91.6240)  Acc@5: 100.0000 (99.4365)  time: 2.2527  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[3/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.1040  Acc@1: 92.1875 (91.6153)  Acc@5: 100.0000 (99.4498)  time: 2.2533  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[3/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: 0.5367  Acc@1: 90.6250 (91.6000)  Acc@5: 100.0000 (99.4600)  time: 2.1550  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[3/5] Total time: 0:02:56 (2.2400 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: 0.5367  Acc@1: 90.6250 (91.6000)  Acc@5: 100.0000 (99.4600)\n",
      "Train: Epoch[4/5]  [ 0/79]  eta: 0:04:03  Lr: 0.007500  Loss: -0.1761  Acc@1: 89.0625 (89.0625)  Acc@5: 96.8750 (96.8750)  time: 3.0765  data: 0.8565  max mem: 6443\n",
      "Train: Epoch[4/5]  [10/79]  eta: 0:02:40  Lr: 0.007500  Loss: -0.2207  Acc@1: 92.1875 (91.3352)  Acc@5: 100.0000 (99.1477)  time: 2.3248  data: 0.0785  max mem: 6443\n",
      "Train: Epoch[4/5]  [20/79]  eta: 0:02:15  Lr: 0.007500  Loss: -0.1821  Acc@1: 90.6250 (91.2946)  Acc@5: 100.0000 (99.1071)  time: 2.2524  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[4/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: -0.1652  Acc@1: 92.1875 (91.8851)  Acc@5: 100.0000 (99.2944)  time: 2.2540  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[4/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.2737  Acc@1: 93.7500 (91.8445)  Acc@5: 100.0000 (99.2759)  time: 2.2512  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[4/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.2170  Acc@1: 93.7500 (91.9118)  Acc@5: 100.0000 (99.1728)  time: 2.2489  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[4/5]  [60/79]  eta: 0:00:43  Lr: 0.007500  Loss: -0.2952  Acc@1: 92.1875 (91.6752)  Acc@5: 100.0000 (99.1291)  time: 2.2488  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[4/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.1503  Acc@1: 90.6250 (91.6373)  Acc@5: 100.0000 (99.1197)  time: 2.2480  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[4/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.4435  Acc@1: 93.7500 (91.7400)  Acc@5: 100.0000 (99.1400)  time: 2.1487  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[4/5] Total time: 0:02:56 (2.2366 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.4435  Acc@1: 93.7500 (91.7400)  Acc@5: 100.0000 (99.1400)\n",
      "Train: Epoch[5/5]  [ 0/79]  eta: 0:03:34  Lr: 0.007500  Loss: -0.1481  Acc@1: 90.6250 (90.6250)  Acc@5: 98.4375 (98.4375)  time: 2.7161  data: 0.4865  max mem: 6443\n",
      "Train: Epoch[5/5]  [10/79]  eta: 0:02:37  Lr: 0.007500  Loss: 0.0705  Acc@1: 92.1875 (89.7727)  Acc@5: 98.4375 (98.7216)  time: 2.2845  data: 0.0450  max mem: 6443\n",
      "Train: Epoch[5/5]  [20/79]  eta: 0:02:13  Lr: 0.007500  Loss: -0.3137  Acc@1: 92.1875 (90.1786)  Acc@5: 98.4375 (98.8095)  time: 2.2417  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[5/5]  [30/79]  eta: 0:01:50  Lr: 0.007500  Loss: -0.2507  Acc@1: 92.1875 (90.5242)  Acc@5: 100.0000 (98.9919)  time: 2.2425  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[5/5]  [40/79]  eta: 0:01:27  Lr: 0.007500  Loss: -0.2861  Acc@1: 92.1875 (90.8918)  Acc@5: 100.0000 (99.1235)  time: 2.2436  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[5/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.0242  Acc@1: 92.1875 (90.9314)  Acc@5: 98.4375 (98.9277)  time: 2.2434  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[5/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: -0.2961  Acc@1: 90.6250 (91.0348)  Acc@5: 98.4375 (98.9498)  time: 2.2419  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[5/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.1661  Acc@1: 90.6250 (91.2192)  Acc@5: 100.0000 (98.9217)  time: 2.2396  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[5/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.4141  Acc@1: 92.1875 (91.3400)  Acc@5: 100.0000 (99.0000)  time: 2.1401  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[5/5] Total time: 0:02:55 (2.2236 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.4141  Acc@1: 92.1875 (91.3400)  Acc@5: 100.0000 (99.0000)\n",
      "Test: [Task 1]  [ 0/16]  eta: 0:00:31  Loss: 0.7184 (0.7184)  Acc@1: 78.1250 (78.1250)  Acc@5: 96.8750 (96.8750)  time: 1.9982  data: 0.5260  max mem: 6443\n",
      "Test: [Task 1]  [10/16]  eta: 0:00:09  Loss: 0.4852 (0.5398)  Acc@1: 85.9375 (85.5114)  Acc@5: 100.0000 (99.1477)  time: 1.5508  data: 0.0483  max mem: 6443\n",
      "Test: [Task 1]  [15/16]  eta: 0:00:01  Loss: 0.4586 (0.5052)  Acc@1: 87.5000 (86.7000)  Acc@5: 100.0000 (99.1000)  time: 1.5003  data: 0.0333  max mem: 6443\n",
      "Test: [Task 1] Total time: 0:00:24 (1.5054 s / it)\n",
      "* Acc@1 86.700 Acc@5 99.100 loss 0.505\n",
      "Test: [Task 2]  [ 0/16]  eta: 0:00:31  Loss: 0.4360 (0.4360)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.4375 (98.4375)  time: 1.9571  data: 0.4698  max mem: 6443\n",
      "Test: [Task 2]  [10/16]  eta: 0:00:09  Loss: 0.6139 (0.6313)  Acc@1: 82.8125 (84.5170)  Acc@5: 98.4375 (96.8750)  time: 1.5535  data: 0.0434  max mem: 6443\n",
      "Test: [Task 2]  [15/16]  eta: 0:00:01  Loss: 0.5958 (0.6034)  Acc@1: 84.3750 (85.3000)  Acc@5: 98.4375 (97.3000)  time: 1.5042  data: 0.0299  max mem: 6443\n",
      "Test: [Task 2] Total time: 0:00:24 (1.5095 s / it)\n",
      "* Acc@1 85.300 Acc@5 97.300 loss 0.603\n",
      "Test: [Task 3]  [ 0/16]  eta: 0:00:33  Loss: 0.5575 (0.5575)  Acc@1: 85.9375 (85.9375)  Acc@5: 98.4375 (98.4375)  time: 2.0847  data: 0.6060  max mem: 6443\n",
      "Test: [Task 3]  [10/16]  eta: 0:00:09  Loss: 0.5241 (0.5071)  Acc@1: 85.9375 (85.9375)  Acc@5: 98.4375 (98.4375)  time: 1.5667  data: 0.0554  max mem: 6443\n",
      "Test: [Task 3]  [15/16]  eta: 0:00:01  Loss: 0.5241 (0.5553)  Acc@1: 85.9375 (85.3000)  Acc@5: 98.4375 (97.5000)  time: 1.5130  data: 0.0382  max mem: 6443\n",
      "Test: [Task 3] Total time: 0:00:24 (1.5181 s / it)\n",
      "* Acc@1 85.300 Acc@5 97.500 loss 0.555\n",
      "Test: [Task 4]  [ 0/16]  eta: 0:00:34  Loss: 0.6146 (0.6146)  Acc@1: 82.8125 (82.8125)  Acc@5: 96.8750 (96.8750)  time: 2.1322  data: 0.6422  max mem: 6443\n",
      "Test: [Task 4]  [10/16]  eta: 0:00:09  Loss: 0.5285 (0.4999)  Acc@1: 89.0625 (87.6420)  Acc@5: 96.8750 (96.8750)  time: 1.5738  data: 0.0589  max mem: 6443\n",
      "Test: [Task 4]  [15/16]  eta: 0:00:01  Loss: 0.5285 (0.5287)  Acc@1: 87.5000 (86.8000)  Acc@5: 96.8750 (96.5000)  time: 1.5188  data: 0.0406  max mem: 6443\n",
      "Test: [Task 4] Total time: 0:00:24 (1.5241 s / it)\n",
      "* Acc@1 86.800 Acc@5 96.500 loss 0.529\n",
      "Test: [Task 5]  [ 0/16]  eta: 0:00:38  Loss: 0.2104 (0.2104)  Acc@1: 95.3125 (95.3125)  Acc@5: 100.0000 (100.0000)  time: 2.3818  data: 0.9040  max mem: 6443\n",
      "Test: [Task 5]  [10/16]  eta: 0:00:09  Loss: 0.4208 (0.4144)  Acc@1: 90.6250 (90.1989)  Acc@5: 98.4375 (98.5795)  time: 1.5907  data: 0.0826  max mem: 6443\n",
      "Test: [Task 5]  [15/16]  eta: 0:00:01  Loss: 0.4208 (0.4415)  Acc@1: 87.5000 (89.1000)  Acc@5: 98.4375 (98.4000)  time: 1.5287  data: 0.0569  max mem: 6443\n",
      "Test: [Task 5] Total time: 0:00:24 (1.5337 s / it)\n",
      "* Acc@1 89.100 Acc@5 98.400 loss 0.442\n",
      "Test: [Task 6]  [ 0/16]  eta: 0:00:34  Loss: 0.5089 (0.5089)  Acc@1: 85.9375 (85.9375)  Acc@5: 96.8750 (96.8750)  time: 2.1810  data: 0.6989  max mem: 6443\n",
      "Test: [Task 6]  [10/16]  eta: 0:00:09  Loss: 0.5264 (0.5365)  Acc@1: 84.3750 (83.9489)  Acc@5: 98.4375 (98.7216)  time: 1.5723  data: 0.0641  max mem: 6443\n",
      "Test: [Task 6]  [15/16]  eta: 0:00:01  Loss: 0.5264 (0.5459)  Acc@1: 84.3750 (83.4000)  Acc@5: 98.4375 (98.6000)  time: 1.5157  data: 0.0442  max mem: 6443\n",
      "Test: [Task 6] Total time: 0:00:24 (1.5209 s / it)\n",
      "* Acc@1 83.400 Acc@5 98.600 loss 0.546\n",
      "Test: [Task 7]  [ 0/16]  eta: 0:00:31  Loss: 0.3407 (0.3407)  Acc@1: 90.6250 (90.6250)  Acc@5: 100.0000 (100.0000)  time: 1.9433  data: 0.4640  max mem: 6443\n",
      "Test: [Task 7]  [10/16]  eta: 0:00:09  Loss: 0.4508 (0.4682)  Acc@1: 89.0625 (87.5000)  Acc@5: 98.4375 (98.1534)  time: 1.5503  data: 0.0427  max mem: 6443\n",
      "Test: [Task 7]  [15/16]  eta: 0:00:01  Loss: 0.4508 (0.4838)  Acc@1: 87.5000 (86.8000)  Acc@5: 98.4375 (97.9000)  time: 1.5008  data: 0.0294  max mem: 6443\n",
      "Test: [Task 7] Total time: 0:00:24 (1.5059 s / it)\n",
      "* Acc@1 86.800 Acc@5 97.900 loss 0.484\n",
      "Test: [Task 8]  [ 0/16]  eta: 0:00:33  Loss: 0.4982 (0.4982)  Acc@1: 87.5000 (87.5000)  Acc@5: 98.4375 (98.4375)  time: 2.0742  data: 0.5862  max mem: 6443\n",
      "Test: [Task 8]  [10/16]  eta: 0:00:09  Loss: 0.5149 (0.5000)  Acc@1: 87.5000 (87.5000)  Acc@5: 98.4375 (98.0114)  time: 1.5640  data: 0.0537  max mem: 6443\n",
      "Test: [Task 8]  [15/16]  eta: 0:00:01  Loss: 0.4982 (0.4963)  Acc@1: 87.5000 (87.3000)  Acc@5: 98.4375 (98.0000)  time: 1.5110  data: 0.0370  max mem: 6443\n",
      "Test: [Task 8] Total time: 0:00:24 (1.5162 s / it)\n",
      "* Acc@1 87.300 Acc@5 98.000 loss 0.496\n",
      "[Average accuracy till task8]\tAcc@1: 86.3375\tAcc@5: 97.9125\tLoss: 0.5200\tForgetting: 5.4286\tBackward: -5.4286\n",
      "Train: Epoch[1/5]  [ 0/79]  eta: 0:03:54  Lr: 0.007500  Loss: 2.3041  Acc@1: 4.6875 (4.6875)  Acc@5: 51.5625 (51.5625)  time: 2.9626  data: 0.7501  max mem: 6443\n",
      "Train: Epoch[1/5]  [10/79]  eta: 0:02:40  Lr: 0.007500  Loss: 0.5187  Acc@1: 84.3750 (75.2841)  Acc@5: 96.8750 (92.4716)  time: 2.3299  data: 0.0687  max mem: 6443\n",
      "Train: Epoch[1/5]  [20/79]  eta: 0:02:16  Lr: 0.007500  Loss: 0.3247  Acc@1: 85.9375 (81.2500)  Acc@5: 98.4375 (95.2381)  time: 2.2764  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[1/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: 0.1995  Acc@1: 90.6250 (84.3750)  Acc@5: 98.4375 (96.3710)  time: 2.2570  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[1/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: 0.0915  Acc@1: 90.6250 (85.9375)  Acc@5: 98.4375 (97.0274)  time: 2.2295  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[1/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: 0.1509  Acc@1: 90.6250 (86.5196)  Acc@5: 98.4375 (97.3346)  time: 2.2425  data: 0.0009  max mem: 6443\n",
      "Train: Epoch[1/5]  [60/79]  eta: 0:00:43  Lr: 0.007500  Loss: 0.1771  Acc@1: 89.0625 (86.9109)  Acc@5: 98.4375 (97.5666)  time: 2.2560  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[1/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.1395  Acc@1: 89.0625 (87.4120)  Acc@5: 98.4375 (97.7993)  time: 2.2521  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[1/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: 0.0062  Acc@1: 89.0625 (87.6600)  Acc@5: 98.4375 (97.8800)  time: 2.1482  data: 0.0003  max mem: 6443\n",
      "Train: Epoch[1/5] Total time: 0:02:56 (2.2376 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: 0.0062  Acc@1: 89.0625 (87.6600)  Acc@5: 98.4375 (97.8800)\n",
      "Train: Epoch[2/5]  [ 0/79]  eta: 0:04:08  Lr: 0.007500  Loss: 0.0260  Acc@1: 89.0625 (89.0625)  Acc@5: 100.0000 (100.0000)  time: 3.1478  data: 0.9154  max mem: 6443\n",
      "Train: Epoch[2/5]  [10/79]  eta: 0:02:40  Lr: 0.007500  Loss: -0.1100  Acc@1: 93.7500 (92.1875)  Acc@5: 100.0000 (99.4318)  time: 2.3234  data: 0.0838  max mem: 6443\n",
      "Train: Epoch[2/5]  [20/79]  eta: 0:02:15  Lr: 0.007500  Loss: 0.0134  Acc@1: 92.1875 (92.1875)  Acc@5: 100.0000 (99.2560)  time: 2.2458  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[2/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: -0.1072  Acc@1: 92.1875 (91.8851)  Acc@5: 100.0000 (99.2944)  time: 2.2522  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[2/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: 0.0495  Acc@1: 89.0625 (91.0823)  Acc@5: 98.4375 (99.1235)  time: 2.2553  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[2/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.1316  Acc@1: 90.6250 (91.4216)  Acc@5: 98.4375 (99.1422)  time: 2.2536  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[2/5]  [60/79]  eta: 0:00:43  Lr: 0.007500  Loss: -0.1176  Acc@1: 92.1875 (91.6496)  Acc@5: 100.0000 (99.2059)  time: 2.2476  data: 0.0008  max mem: 6443\n",
      "Train: Epoch[2/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: 0.0007  Acc@1: 92.1875 (91.5493)  Acc@5: 100.0000 (99.2518)  time: 2.2433  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[2/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.3597  Acc@1: 92.1875 (91.5200)  Acc@5: 100.0000 (99.3000)  time: 2.1426  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[2/5] Total time: 0:02:56 (2.2351 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.3597  Acc@1: 92.1875 (91.5200)  Acc@5: 100.0000 (99.3000)\n",
      "Train: Epoch[3/5]  [ 0/79]  eta: 0:03:43  Lr: 0.007500  Loss: -0.2001  Acc@1: 93.7500 (93.7500)  Acc@5: 98.4375 (98.4375)  time: 2.8304  data: 0.6117  max mem: 6443\n",
      "Train: Epoch[3/5]  [10/79]  eta: 0:02:38  Lr: 0.007500  Loss: -0.2127  Acc@1: 92.1875 (92.7557)  Acc@5: 98.4375 (99.0057)  time: 2.2929  data: 0.0565  max mem: 6443\n",
      "Train: Epoch[3/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: -0.1799  Acc@1: 92.1875 (92.3363)  Acc@5: 100.0000 (99.2560)  time: 2.2450  data: 0.0008  max mem: 6443\n",
      "Train: Epoch[3/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: -0.3129  Acc@1: 92.1875 (92.3387)  Acc@5: 100.0000 (99.3448)  time: 2.2551  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[3/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.2550  Acc@1: 92.1875 (91.8445)  Acc@5: 100.0000 (99.1235)  time: 2.2567  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[3/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.2425  Acc@1: 92.1875 (92.2488)  Acc@5: 100.0000 (99.2341)  time: 2.2527  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[3/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: -0.1179  Acc@1: 93.7500 (92.1107)  Acc@5: 100.0000 (99.3084)  time: 2.2502  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[3/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.2880  Acc@1: 92.1875 (92.0995)  Acc@5: 100.0000 (99.2958)  time: 2.2489  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[3/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: 0.3248  Acc@1: 92.1875 (92.0600)  Acc@5: 100.0000 (99.2400)  time: 2.1504  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[3/5] Total time: 0:02:56 (2.2339 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: 0.3248  Acc@1: 92.1875 (92.0600)  Acc@5: 100.0000 (99.2400)\n",
      "Train: Epoch[4/5]  [ 0/79]  eta: 0:03:30  Lr: 0.007500  Loss: -0.0578  Acc@1: 87.5000 (87.5000)  Acc@5: 96.8750 (96.8750)  time: 2.6621  data: 0.4388  max mem: 6443\n",
      "Train: Epoch[4/5]  [10/79]  eta: 0:02:37  Lr: 0.007500  Loss: -0.2661  Acc@1: 92.1875 (92.0455)  Acc@5: 98.4375 (98.8636)  time: 2.2828  data: 0.0405  max mem: 6443\n",
      "Train: Epoch[4/5]  [20/79]  eta: 0:02:13  Lr: 0.007500  Loss: -0.3549  Acc@1: 90.6250 (90.9226)  Acc@5: 100.0000 (99.2560)  time: 2.2481  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[4/5]  [30/79]  eta: 0:01:50  Lr: 0.007500  Loss: -0.2495  Acc@1: 92.1875 (91.5827)  Acc@5: 100.0000 (99.3448)  time: 2.2534  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[4/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.2117  Acc@1: 93.7500 (91.9970)  Acc@5: 100.0000 (99.3521)  time: 2.2550  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[4/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.2937  Acc@1: 93.7500 (92.3100)  Acc@5: 100.0000 (99.3566)  time: 2.2500  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[4/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: -0.0281  Acc@1: 92.1875 (92.2900)  Acc@5: 100.0000 (99.4109)  time: 2.2418  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[4/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.2184  Acc@1: 93.7500 (92.6717)  Acc@5: 100.0000 (99.4718)  time: 2.2377  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[4/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.2052  Acc@1: 93.7500 (92.6600)  Acc@5: 100.0000 (99.4800)  time: 2.1421  data: 0.0003  max mem: 6443\n",
      "Train: Epoch[4/5] Total time: 0:02:56 (2.2281 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.2052  Acc@1: 93.7500 (92.6600)  Acc@5: 100.0000 (99.4800)\n",
      "Train: Epoch[5/5]  [ 0/79]  eta: 0:03:47  Lr: 0.007500  Loss: -0.1825  Acc@1: 92.1875 (92.1875)  Acc@5: 100.0000 (100.0000)  time: 2.8790  data: 0.6565  max mem: 6443\n",
      "Train: Epoch[5/5]  [10/79]  eta: 0:02:38  Lr: 0.007500  Loss: -0.1431  Acc@1: 90.6250 (92.1875)  Acc@5: 100.0000 (99.7159)  time: 2.3019  data: 0.0602  max mem: 6443\n",
      "Train: Epoch[5/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: -0.1916  Acc@1: 90.6250 (92.2619)  Acc@5: 100.0000 (99.4048)  time: 2.2479  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[5/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: -0.3464  Acc@1: 93.7500 (92.7419)  Acc@5: 100.0000 (99.3952)  time: 2.2539  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[5/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.2989  Acc@1: 92.1875 (92.4924)  Acc@5: 100.0000 (99.3521)  time: 2.2570  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[5/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.3325  Acc@1: 92.1875 (92.5858)  Acc@5: 100.0000 (99.3260)  time: 2.2555  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[5/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: -0.2776  Acc@1: 92.1875 (92.5717)  Acc@5: 98.4375 (99.2828)  time: 2.2516  data: 0.0008  max mem: 6443\n",
      "Train: Epoch[5/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.2308  Acc@1: 93.7500 (92.6717)  Acc@5: 100.0000 (99.2958)  time: 2.2491  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[5/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.4475  Acc@1: 93.7500 (92.9000)  Acc@5: 100.0000 (99.3000)  time: 2.1491  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[5/5] Total time: 0:02:56 (2.2352 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.4475  Acc@1: 93.7500 (92.9000)  Acc@5: 100.0000 (99.3000)\n",
      "Test: [Task 1]  [ 0/16]  eta: 0:00:37  Loss: 0.7517 (0.7517)  Acc@1: 79.6875 (79.6875)  Acc@5: 98.4375 (98.4375)  time: 2.3170  data: 0.8451  max mem: 6443\n",
      "Test: [Task 1]  [10/16]  eta: 0:00:09  Loss: 0.4888 (0.5421)  Acc@1: 85.9375 (84.9432)  Acc@5: 100.0000 (99.2898)  time: 1.5832  data: 0.0771  max mem: 6443\n",
      "Test: [Task 1]  [15/16]  eta: 0:00:01  Loss: 0.4719 (0.5071)  Acc@1: 85.9375 (86.3000)  Acc@5: 98.4375 (99.2000)  time: 1.5234  data: 0.0531  max mem: 6443\n",
      "Test: [Task 1] Total time: 0:00:24 (1.5285 s / it)\n",
      "* Acc@1 86.300 Acc@5 99.200 loss 0.507\n",
      "Test: [Task 2]  [ 0/16]  eta: 0:00:33  Loss: 0.4708 (0.4708)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.4375 (98.4375)  time: 2.0846  data: 0.6111  max mem: 6443\n",
      "Test: [Task 2]  [10/16]  eta: 0:00:09  Loss: 0.6556 (0.6616)  Acc@1: 82.8125 (84.2330)  Acc@5: 96.8750 (96.1648)  time: 1.5669  data: 0.0562  max mem: 6443\n",
      "Test: [Task 2]  [15/16]  eta: 0:00:01  Loss: 0.6205 (0.6322)  Acc@1: 82.8125 (84.7000)  Acc@5: 96.8750 (96.7000)  time: 1.5133  data: 0.0387  max mem: 6443\n",
      "Test: [Task 2] Total time: 0:00:24 (1.5197 s / it)\n",
      "* Acc@1 84.700 Acc@5 96.700 loss 0.632\n",
      "Test: [Task 3]  [ 0/16]  eta: 0:00:33  Loss: 0.5434 (0.5434)  Acc@1: 85.9375 (85.9375)  Acc@5: 98.4375 (98.4375)  time: 2.1144  data: 0.6328  max mem: 6443\n",
      "Test: [Task 3]  [10/16]  eta: 0:00:09  Loss: 0.5141 (0.5107)  Acc@1: 85.9375 (86.5057)  Acc@5: 98.4375 (98.2955)  time: 1.5715  data: 0.0580  max mem: 6443\n",
      "Test: [Task 3]  [15/16]  eta: 0:00:01  Loss: 0.5208 (0.5638)  Acc@1: 85.9375 (85.9000)  Acc@5: 98.4375 (97.7000)  time: 1.5170  data: 0.0400  max mem: 6443\n",
      "Test: [Task 3] Total time: 0:00:24 (1.5221 s / it)\n",
      "* Acc@1 85.900 Acc@5 97.700 loss 0.564\n",
      "Test: [Task 4]  [ 0/16]  eta: 0:00:33  Loss: 0.6445 (0.6445)  Acc@1: 81.2500 (81.2500)  Acc@5: 96.8750 (96.8750)  time: 2.0701  data: 0.5938  max mem: 6443\n",
      "Test: [Task 4]  [10/16]  eta: 0:00:09  Loss: 0.5660 (0.5269)  Acc@1: 87.5000 (86.6477)  Acc@5: 96.8750 (97.0170)  time: 1.5651  data: 0.0547  max mem: 6443\n",
      "Test: [Task 4]  [15/16]  eta: 0:00:01  Loss: 0.5660 (0.5572)  Acc@1: 85.9375 (86.1000)  Acc@5: 96.8750 (96.6000)  time: 1.5117  data: 0.0377  max mem: 6443\n",
      "Test: [Task 4] Total time: 0:00:24 (1.5170 s / it)\n",
      "* Acc@1 86.100 Acc@5 96.600 loss 0.557\n",
      "Test: [Task 5]  [ 0/16]  eta: 0:00:34  Loss: 0.2543 (0.2543)  Acc@1: 96.8750 (96.8750)  Acc@5: 100.0000 (100.0000)  time: 2.1727  data: 0.6857  max mem: 6443\n",
      "Test: [Task 5]  [10/16]  eta: 0:00:09  Loss: 0.4458 (0.4452)  Acc@1: 87.5000 (88.4943)  Acc@5: 98.4375 (98.2955)  time: 1.5729  data: 0.0628  max mem: 6443\n",
      "Test: [Task 5]  [15/16]  eta: 0:00:01  Loss: 0.4458 (0.4810)  Acc@1: 87.5000 (87.7000)  Acc@5: 98.4375 (97.9000)  time: 1.5159  data: 0.0432  max mem: 6443\n",
      "Test: [Task 5] Total time: 0:00:24 (1.5212 s / it)\n",
      "* Acc@1 87.700 Acc@5 97.900 loss 0.481\n",
      "Test: [Task 6]  [ 0/16]  eta: 0:00:33  Loss: 0.5299 (0.5299)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.3125 (95.3125)  time: 2.0929  data: 0.6133  max mem: 6443\n",
      "Test: [Task 6]  [10/16]  eta: 0:00:09  Loss: 0.5580 (0.5590)  Acc@1: 82.8125 (82.8125)  Acc@5: 98.4375 (98.7216)  time: 1.5627  data: 0.0564  max mem: 6443\n",
      "Test: [Task 6]  [15/16]  eta: 0:00:01  Loss: 0.5580 (0.5675)  Acc@1: 82.8125 (82.3000)  Acc@5: 98.4375 (98.5000)  time: 1.5092  data: 0.0389  max mem: 6443\n",
      "Test: [Task 6] Total time: 0:00:24 (1.5145 s / it)\n",
      "* Acc@1 82.300 Acc@5 98.500 loss 0.567\n",
      "Test: [Task 7]  [ 0/16]  eta: 0:00:35  Loss: 0.3596 (0.3596)  Acc@1: 87.5000 (87.5000)  Acc@5: 98.4375 (98.4375)  time: 2.2242  data: 0.7474  max mem: 6443\n",
      "Test: [Task 7]  [10/16]  eta: 0:00:09  Loss: 0.4441 (0.4852)  Acc@1: 87.5000 (86.9318)  Acc@5: 98.4375 (97.5852)  time: 1.5763  data: 0.0687  max mem: 6443\n",
      "Test: [Task 7]  [15/16]  eta: 0:00:01  Loss: 0.4830 (0.5048)  Acc@1: 87.5000 (86.4000)  Acc@5: 98.4375 (97.5000)  time: 1.5193  data: 0.0473  max mem: 6443\n",
      "Test: [Task 7] Total time: 0:00:24 (1.5245 s / it)\n",
      "* Acc@1 86.400 Acc@5 97.500 loss 0.505\n",
      "Test: [Task 8]  [ 0/16]  eta: 0:00:33  Loss: 0.5012 (0.5012)  Acc@1: 87.5000 (87.5000)  Acc@5: 98.4375 (98.4375)  time: 2.1142  data: 0.6235  max mem: 6443\n",
      "Test: [Task 8]  [10/16]  eta: 0:00:09  Loss: 0.5305 (0.5011)  Acc@1: 87.5000 (87.6420)  Acc@5: 98.4375 (97.8693)  time: 1.5706  data: 0.0571  max mem: 6443\n",
      "Test: [Task 8]  [15/16]  eta: 0:00:01  Loss: 0.5012 (0.4958)  Acc@1: 87.5000 (87.8000)  Acc@5: 98.4375 (97.8000)  time: 1.5163  data: 0.0394  max mem: 6443\n",
      "Test: [Task 8] Total time: 0:00:24 (1.5227 s / it)\n",
      "* Acc@1 87.800 Acc@5 97.800 loss 0.496\n",
      "Test: [Task 9]  [ 0/16]  eta: 0:00:32  Loss: 0.2653 (0.2653)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.4375 (98.4375)  time: 2.0613  data: 0.5730  max mem: 6443\n",
      "Test: [Task 9]  [10/16]  eta: 0:00:09  Loss: 0.3450 (0.3356)  Acc@1: 89.0625 (89.9148)  Acc@5: 100.0000 (99.0057)  time: 1.5683  data: 0.0529  max mem: 6443\n",
      "Test: [Task 9]  [15/16]  eta: 0:00:01  Loss: 0.2565 (0.3024)  Acc@1: 92.1875 (91.2000)  Acc@5: 100.0000 (99.2000)  time: 1.5149  data: 0.0364  max mem: 6443\n",
      "Test: [Task 9] Total time: 0:00:24 (1.5200 s / it)\n",
      "* Acc@1 91.200 Acc@5 99.200 loss 0.302\n",
      "[Average accuracy till task9]\tAcc@1: 86.4889\tAcc@5: 97.9000\tLoss: 0.5124\tForgetting: 5.2500\tBackward: -5.1875\n",
      "Train: Epoch[1/5]  [ 0/79]  eta: 0:03:33  Lr: 0.007500  Loss: 2.2842  Acc@1: 15.6250 (15.6250)  Acc@5: 45.3125 (45.3125)  time: 2.7031  data: 0.4679  max mem: 6443\n",
      "Train: Epoch[1/5]  [10/79]  eta: 0:02:37  Lr: 0.007500  Loss: 0.4295  Acc@1: 85.9375 (77.6989)  Acc@5: 98.4375 (92.1875)  time: 2.2878  data: 0.0430  max mem: 6443\n",
      "Train: Epoch[1/5]  [20/79]  eta: 0:02:13  Lr: 0.007500  Loss: 0.3059  Acc@1: 90.6250 (84.9702)  Acc@5: 98.4375 (95.6101)  time: 2.2479  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[1/5]  [30/79]  eta: 0:01:50  Lr: 0.007500  Loss: 0.1667  Acc@1: 92.1875 (87.0968)  Acc@5: 100.0000 (96.6734)  time: 2.2496  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[1/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: 0.0306  Acc@1: 92.1875 (88.5290)  Acc@5: 98.4375 (97.1799)  time: 2.2500  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[1/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.0572  Acc@1: 93.7500 (89.6752)  Acc@5: 100.0000 (97.6716)  time: 2.2501  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[1/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: 0.0260  Acc@1: 93.7500 (90.2664)  Acc@5: 100.0000 (98.0277)  time: 2.2512  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[1/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: 0.1566  Acc@1: 93.7500 (90.7350)  Acc@5: 100.0000 (98.2174)  time: 2.2513  data: 0.0008  max mem: 6443\n",
      "Train: Epoch[1/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.2686  Acc@1: 93.7500 (91.0000)  Acc@5: 100.0000 (98.2600)  time: 2.1526  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[1/5] Total time: 0:02:56 (2.2319 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.2686  Acc@1: 93.7500 (91.0000)  Acc@5: 100.0000 (98.2600)\n",
      "Train: Epoch[2/5]  [ 0/79]  eta: 0:03:33  Lr: 0.007500  Loss: -0.1225  Acc@1: 96.8750 (96.8750)  Acc@5: 100.0000 (100.0000)  time: 2.7077  data: 0.4829  max mem: 6443\n",
      "Train: Epoch[2/5]  [10/79]  eta: 0:02:37  Lr: 0.007500  Loss: -0.1035  Acc@1: 93.7500 (93.1818)  Acc@5: 100.0000 (99.8580)  time: 2.2897  data: 0.0444  max mem: 6443\n",
      "Train: Epoch[2/5]  [20/79]  eta: 0:02:13  Lr: 0.007500  Loss: -0.0090  Acc@1: 93.7500 (93.6756)  Acc@5: 100.0000 (99.5536)  time: 2.2493  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[2/5]  [30/79]  eta: 0:01:50  Lr: 0.007500  Loss: -0.1956  Acc@1: 93.7500 (93.3468)  Acc@5: 100.0000 (99.4456)  time: 2.2513  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[2/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.2230  Acc@1: 92.1875 (93.2165)  Acc@5: 100.0000 (99.5427)  time: 2.2526  data: 0.0008  max mem: 6443\n",
      "Train: Epoch[2/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.1031  Acc@1: 93.7500 (93.4130)  Acc@5: 100.0000 (99.5098)  time: 2.2536  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[2/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: -0.2368  Acc@1: 93.7500 (93.4682)  Acc@5: 100.0000 (99.4621)  time: 2.2542  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[2/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.2327  Acc@1: 93.7500 (93.5960)  Acc@5: 100.0000 (99.4498)  time: 2.2528  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[2/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.3107  Acc@1: 93.7500 (93.4800)  Acc@5: 100.0000 (99.4000)  time: 2.1532  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[2/5] Total time: 0:02:56 (2.2339 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.3107  Acc@1: 93.7500 (93.4800)  Acc@5: 100.0000 (99.4000)\n",
      "Train: Epoch[3/5]  [ 0/79]  eta: 0:03:39  Lr: 0.007500  Loss: -0.1447  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 2.7723  data: 0.5490  max mem: 6443\n",
      "Train: Epoch[3/5]  [10/79]  eta: 0:02:38  Lr: 0.007500  Loss: -0.1187  Acc@1: 93.7500 (93.3239)  Acc@5: 100.0000 (98.8636)  time: 2.2963  data: 0.0507  max mem: 6443\n",
      "Train: Epoch[3/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: -0.1754  Acc@1: 93.7500 (93.8244)  Acc@5: 98.4375 (98.8839)  time: 2.2500  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[3/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: -0.3085  Acc@1: 93.7500 (93.8004)  Acc@5: 98.4375 (99.0423)  time: 2.2522  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[3/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.1219  Acc@1: 93.7500 (93.5595)  Acc@5: 100.0000 (99.0473)  time: 2.2538  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[3/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.2816  Acc@1: 93.7500 (93.6581)  Acc@5: 100.0000 (99.2034)  time: 2.2560  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[3/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: -0.2545  Acc@1: 93.7500 (93.4939)  Acc@5: 100.0000 (99.2828)  time: 2.2574  data: 0.0004  max mem: 6443\n",
      "Train: Epoch[3/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.2325  Acc@1: 92.1875 (93.3979)  Acc@5: 100.0000 (99.3178)  time: 2.2553  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[3/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: 0.5602  Acc@1: 92.1875 (93.4200)  Acc@5: 100.0000 (99.3000)  time: 2.1548  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[3/5] Total time: 0:02:56 (2.2369 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: 0.5602  Acc@1: 92.1875 (93.4200)  Acc@5: 100.0000 (99.3000)\n",
      "Train: Epoch[4/5]  [ 0/79]  eta: 0:04:31  Lr: 0.007500  Loss: -0.2187  Acc@1: 92.1875 (92.1875)  Acc@5: 100.0000 (100.0000)  time: 3.4426  data: 1.2165  max mem: 6443\n",
      "Train: Epoch[4/5]  [10/79]  eta: 0:02:42  Lr: 0.007500  Loss: -0.1855  Acc@1: 93.7500 (93.4659)  Acc@5: 100.0000 (99.7159)  time: 2.3567  data: 0.1111  max mem: 6443\n",
      "Train: Epoch[4/5]  [20/79]  eta: 0:02:16  Lr: 0.007500  Loss: -0.1863  Acc@1: 93.7500 (94.1964)  Acc@5: 100.0000 (99.5536)  time: 2.2504  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[4/5]  [30/79]  eta: 0:01:52  Lr: 0.007500  Loss: -0.3128  Acc@1: 93.7500 (94.0020)  Acc@5: 100.0000 (99.5464)  time: 2.2554  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[4/5]  [40/79]  eta: 0:01:29  Lr: 0.007500  Loss: -0.1051  Acc@1: 93.7500 (94.0930)  Acc@5: 100.0000 (99.4665)  time: 2.2575  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[4/5]  [50/79]  eta: 0:01:06  Lr: 0.007500  Loss: -0.3874  Acc@1: 95.3125 (94.2402)  Acc@5: 100.0000 (99.5404)  time: 2.2551  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[4/5]  [60/79]  eta: 0:00:43  Lr: 0.007500  Loss: -0.1874  Acc@1: 95.3125 (94.2879)  Acc@5: 100.0000 (99.4877)  time: 2.2544  data: 0.0003  max mem: 6443\n",
      "Train: Epoch[4/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.3182  Acc@1: 95.3125 (94.3442)  Acc@5: 100.0000 (99.5379)  time: 2.2567  data: 0.0006  max mem: 6443\n",
      "Train: Epoch[4/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.3106  Acc@1: 93.7500 (94.2000)  Acc@5: 100.0000 (99.5200)  time: 2.1586  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[4/5] Total time: 0:02:57 (2.2463 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.3106  Acc@1: 93.7500 (94.2000)  Acc@5: 100.0000 (99.5200)\n",
      "Train: Epoch[5/5]  [ 0/79]  eta: 0:03:50  Lr: 0.007500  Loss: -0.2939  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 2.9160  data: 0.6889  max mem: 6443\n",
      "Train: Epoch[5/5]  [10/79]  eta: 0:02:39  Lr: 0.007500  Loss: -0.3399  Acc@1: 93.7500 (94.3182)  Acc@5: 100.0000 (99.2898)  time: 2.3100  data: 0.0635  max mem: 6443\n",
      "Train: Epoch[5/5]  [20/79]  eta: 0:02:14  Lr: 0.007500  Loss: -0.3376  Acc@1: 93.7500 (94.4940)  Acc@5: 100.0000 (99.3304)  time: 2.2492  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[5/5]  [30/79]  eta: 0:01:51  Lr: 0.007500  Loss: -0.2566  Acc@1: 93.7500 (94.3548)  Acc@5: 100.0000 (99.3952)  time: 2.2497  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[5/5]  [40/79]  eta: 0:01:28  Lr: 0.007500  Loss: -0.2974  Acc@1: 93.7500 (94.2454)  Acc@5: 100.0000 (99.3140)  time: 2.2513  data: 0.0008  max mem: 6443\n",
      "Train: Epoch[5/5]  [50/79]  eta: 0:01:05  Lr: 0.007500  Loss: -0.2326  Acc@1: 93.7500 (94.0870)  Acc@5: 98.4375 (99.2341)  time: 2.2526  data: 0.0008  max mem: 6443\n",
      "Train: Epoch[5/5]  [60/79]  eta: 0:00:42  Lr: 0.007500  Loss: -0.3117  Acc@1: 93.7500 (94.3135)  Acc@5: 100.0000 (99.2828)  time: 2.2529  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[5/5]  [70/79]  eta: 0:00:20  Lr: 0.007500  Loss: -0.2830  Acc@1: 93.7500 (94.0361)  Acc@5: 100.0000 (99.3618)  time: 2.2542  data: 0.0007  max mem: 6443\n",
      "Train: Epoch[5/5]  [78/79]  eta: 0:00:02  Lr: 0.007500  Loss: -0.2212  Acc@1: 92.1875 (94.2000)  Acc@5: 100.0000 (99.4200)  time: 2.1577  data: 0.0005  max mem: 6443\n",
      "Train: Epoch[5/5] Total time: 0:02:56 (2.2369 s / it)\n",
      "Averaged stats: Lr: 0.007500  Loss: -0.2212  Acc@1: 92.1875 (94.2000)  Acc@5: 100.0000 (99.4200)\n",
      "Test: [Task 1]  [ 0/16]  eta: 0:00:34  Loss: 0.8256 (0.8256)  Acc@1: 76.5625 (76.5625)  Acc@5: 96.8750 (96.8750)  time: 2.1766  data: 0.6937  max mem: 6443\n",
      "Test: [Task 1]  [10/16]  eta: 0:00:09  Loss: 0.5111 (0.5728)  Acc@1: 84.3750 (83.2386)  Acc@5: 98.4375 (98.7216)  time: 1.5759  data: 0.0634  max mem: 6443\n",
      "Test: [Task 1]  [15/16]  eta: 0:00:01  Loss: 0.4804 (0.5309)  Acc@1: 85.0000 (84.7000)  Acc@5: 98.4375 (98.7000)  time: 1.5195  data: 0.0436  max mem: 6443\n",
      "Test: [Task 1] Total time: 0:00:24 (1.5246 s / it)\n",
      "* Acc@1 84.700 Acc@5 98.700 loss 0.531\n",
      "Test: [Task 2]  [ 0/16]  eta: 0:00:33  Loss: 0.4405 (0.4405)  Acc@1: 90.6250 (90.6250)  Acc@5: 98.4375 (98.4375)  time: 2.1062  data: 0.6266  max mem: 6443\n",
      "Test: [Task 2]  [10/16]  eta: 0:00:09  Loss: 0.6945 (0.6920)  Acc@1: 82.8125 (82.8125)  Acc@5: 96.8750 (96.3068)  time: 1.5678  data: 0.0575  max mem: 6443\n",
      "Test: [Task 2]  [15/16]  eta: 0:00:01  Loss: 0.6394 (0.6537)  Acc@1: 82.8125 (83.4000)  Acc@5: 96.8750 (96.8000)  time: 1.5135  data: 0.0396  max mem: 6443\n",
      "Test: [Task 2] Total time: 0:00:24 (1.5185 s / it)\n",
      "* Acc@1 83.400 Acc@5 96.800 loss 0.654\n",
      "Test: [Task 3]  [ 0/16]  eta: 0:00:33  Loss: 0.5407 (0.5407)  Acc@1: 84.3750 (84.3750)  Acc@5: 98.4375 (98.4375)  time: 2.1080  data: 0.6261  max mem: 6443\n",
      "Test: [Task 3]  [10/16]  eta: 0:00:09  Loss: 0.5292 (0.5061)  Acc@1: 87.5000 (86.6477)  Acc@5: 98.4375 (98.0114)  time: 1.5685  data: 0.0572  max mem: 6443\n",
      "Test: [Task 3]  [15/16]  eta: 0:00:01  Loss: 0.5407 (0.5687)  Acc@1: 85.9375 (85.9000)  Acc@5: 98.4375 (97.2000)  time: 1.5143  data: 0.0394  max mem: 6443\n",
      "Test: [Task 3] Total time: 0:00:24 (1.5197 s / it)\n",
      "* Acc@1 85.900 Acc@5 97.200 loss 0.569\n",
      "Test: [Task 4]  [ 0/16]  eta: 0:00:33  Loss: 0.7073 (0.7073)  Acc@1: 81.2500 (81.2500)  Acc@5: 95.3125 (95.3125)  time: 2.0719  data: 0.5860  max mem: 6443\n",
      "Test: [Task 4]  [10/16]  eta: 0:00:09  Loss: 0.5778 (0.5598)  Acc@1: 87.5000 (86.3636)  Acc@5: 95.3125 (96.7330)  time: 1.5668  data: 0.0536  max mem: 6443\n",
      "Test: [Task 4]  [15/16]  eta: 0:00:01  Loss: 0.5778 (0.5966)  Acc@1: 85.9375 (85.7000)  Acc@5: 95.3125 (96.3000)  time: 1.5133  data: 0.0369  max mem: 6443\n",
      "Test: [Task 4] Total time: 0:00:24 (1.5186 s / it)\n",
      "* Acc@1 85.700 Acc@5 96.300 loss 0.597\n",
      "Test: [Task 5]  [ 0/16]  eta: 0:00:33  Loss: 0.3352 (0.3352)  Acc@1: 92.1875 (92.1875)  Acc@5: 100.0000 (100.0000)  time: 2.0799  data: 0.5974  max mem: 6443\n",
      "Test: [Task 5]  [10/16]  eta: 0:00:09  Loss: 0.5034 (0.5045)  Acc@1: 85.9375 (86.3636)  Acc@5: 96.8750 (97.7273)  time: 1.5675  data: 0.0549  max mem: 6443\n",
      "Test: [Task 5]  [15/16]  eta: 0:00:01  Loss: 0.5034 (0.5390)  Acc@1: 84.3750 (85.6000)  Acc@5: 96.8750 (97.3000)  time: 1.5138  data: 0.0378  max mem: 6443\n",
      "Test: [Task 5] Total time: 0:00:24 (1.5188 s / it)\n",
      "* Acc@1 85.600 Acc@5 97.300 loss 0.539\n",
      "Test: [Task 6]  [ 0/16]  eta: 0:00:31  Loss: 0.5838 (0.5838)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.3125 (95.3125)  time: 1.9752  data: 0.4947  max mem: 6443\n",
      "Test: [Task 6]  [10/16]  eta: 0:00:09  Loss: 0.5838 (0.5857)  Acc@1: 82.8125 (82.8125)  Acc@5: 98.4375 (98.0114)  time: 1.5587  data: 0.0455  max mem: 6443\n",
      "Test: [Task 6]  [15/16]  eta: 0:00:01  Loss: 0.5838 (0.5918)  Acc@1: 82.8125 (83.0000)  Acc@5: 98.4375 (97.7000)  time: 1.5083  data: 0.0313  max mem: 6443\n",
      "Test: [Task 6] Total time: 0:00:24 (1.5134 s / it)\n",
      "* Acc@1 83.000 Acc@5 97.700 loss 0.592\n",
      "Test: [Task 7]  [ 0/16]  eta: 0:00:32  Loss: 0.3532 (0.3532)  Acc@1: 90.6250 (90.6250)  Acc@5: 98.4375 (98.4375)  time: 2.0109  data: 0.5214  max mem: 6443\n",
      "Test: [Task 7]  [10/16]  eta: 0:00:09  Loss: 0.4564 (0.4925)  Acc@1: 87.5000 (86.6477)  Acc@5: 98.4375 (97.4432)  time: 1.5631  data: 0.0480  max mem: 6443\n",
      "Test: [Task 7]  [15/16]  eta: 0:00:01  Loss: 0.4918 (0.5161)  Acc@1: 85.9375 (85.9000)  Acc@5: 96.8750 (97.3000)  time: 1.5110  data: 0.0331  max mem: 6443\n",
      "Test: [Task 7] Total time: 0:00:24 (1.5207 s / it)\n",
      "* Acc@1 85.900 Acc@5 97.300 loss 0.516\n",
      "Test: [Task 8]  [ 0/16]  eta: 0:00:35  Loss: 0.5535 (0.5535)  Acc@1: 85.9375 (85.9375)  Acc@5: 98.4375 (98.4375)  time: 2.1924  data: 0.7099  max mem: 6443\n",
      "Test: [Task 8]  [10/16]  eta: 0:00:09  Loss: 0.5986 (0.5785)  Acc@1: 87.5000 (86.2216)  Acc@5: 96.8750 (97.4432)  time: 1.5776  data: 0.0650  max mem: 6443\n",
      "Test: [Task 8]  [15/16]  eta: 0:00:01  Loss: 0.5535 (0.5541)  Acc@1: 87.5000 (86.9000)  Acc@5: 96.8750 (97.5000)  time: 1.5208  data: 0.0448  max mem: 6443\n",
      "Test: [Task 8] Total time: 0:00:24 (1.5285 s / it)\n",
      "* Acc@1 86.900 Acc@5 97.500 loss 0.554\n",
      "Test: [Task 9]  [ 0/16]  eta: 0:00:37  Loss: 0.3165 (0.3165)  Acc@1: 89.0625 (89.0625)  Acc@5: 98.4375 (98.4375)  time: 2.3451  data: 0.8624  max mem: 6443\n",
      "Test: [Task 9]  [10/16]  eta: 0:00:09  Loss: 0.3865 (0.4423)  Acc@1: 84.3750 (85.5114)  Acc@5: 98.4375 (98.8636)  time: 1.5904  data: 0.0787  max mem: 6443\n",
      "Test: [Task 9]  [15/16]  eta: 0:00:01  Loss: 0.3632 (0.3947)  Acc@1: 89.0625 (87.4000)  Acc@5: 98.4375 (99.0000)  time: 1.5291  data: 0.0542  max mem: 6443\n",
      "Test: [Task 9] Total time: 0:00:24 (1.5391 s / it)\n",
      "* Acc@1 87.400 Acc@5 99.000 loss 0.395\n",
      "Test: [Task 10]  [ 0/16]  eta: 0:00:37  Loss: 0.2422 (0.2422)  Acc@1: 96.8750 (96.8750)  Acc@5: 100.0000 (100.0000)  time: 2.3747  data: 0.8870  max mem: 6443\n",
      "Test: [Task 10]  [10/16]  eta: 0:00:09  Loss: 0.4105 (0.4199)  Acc@1: 89.0625 (88.7784)  Acc@5: 98.4375 (99.0057)  time: 1.5920  data: 0.0809  max mem: 6443\n",
      "Test: [Task 10]  [15/16]  eta: 0:00:01  Loss: 0.4086 (0.4194)  Acc@1: 89.0625 (88.0000)  Acc@5: 98.4375 (99.0000)  time: 1.5298  data: 0.0557  max mem: 6443\n",
      "Test: [Task 10] Total time: 0:00:24 (1.5381 s / it)\n",
      "* Acc@1 88.000 Acc@5 99.000 loss 0.419\n",
      "[Average accuracy till task10]\tAcc@1: 85.6500\tAcc@5: 97.6800\tLoss: 0.5365\tForgetting: 5.7667\tBackward: -5.7111\n",
      "Total training time: 2:49:41\n",
      "[rank0]:[W122 18:58:46.372260180 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model Evaluation\n",
    "\n",
    "After training, the model is evaluated in inference mode.\n",
    "This step verifies that training has completed successfully and loads the saved model weights for subsequent experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!python -m torch.distributed.run \\\n",
    "    --nproc-per-node=1 \\\n",
    "    main.py cifar100_dualprompt \\\n",
    "    --eval \\\n",
    "    --model vit_base_patch16_224 \\\n",
    "    --data-path ./data \\\n",
    "    --output_dir ./output\n",
    "# modify main.py adding weights_only=False at row 107"
   ],
   "metadata": {
    "id": "eB8caUCTMcx4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1cf55aaa-033c-499e-c4e5-fb663aea5bd9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "| distributed init (rank 0): env://\n",
      "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "[rank0]:[W122 19:00:43.004228735 ProcessGroupNCCL.cpp:5068] Guessing device ID based on global rank. This can cause a hang if rank to GPU mapping is heterogeneous. You can specify device_id in init_process_group()\n",
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "Creating original model: vit_base_patch16_224\n",
      "Creating model: vit_base_patch16_224\n",
      "Namespace(subparser_name='cifar100_dualprompt', batch_size=24, epochs=5, model='vit_base_patch16_224', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='constant', lr=0.03, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='./data', dataset='Split-CIFAR100', shuffle=False, output_dir='./output', device='cuda', seed=42, eval=True, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, use_g_prompt=True, g_prompt_length=5, g_prompt_layer_idx=[0, 1], use_prefix_tune_for_g_prompt=True, use_e_prompt=True, e_prompt_layer_idx=[2, 3, 4], use_prefix_tune_for_e_prompt=True, prompt_pool=True, size=10, length=5, top_k=1, initializer='uniform', prompt_key=True, prompt_key_init='uniform', use_prompt_mask=True, mask_first_epoch=False, shared_prompt_pool=True, shared_prompt_key=False, batchwise_prompt=True, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=1.0, same_key_value=False, global_pool='token', head_type='token', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], print_freq=10, rank=0, gpu=0, distributed=True, dist_backend='nccl', nb_classes=100)\n",
      "Loading checkpoint from: ./output/checkpoint/task1_checkpoint.pth\n",
      "Test: [Task 1]  [ 0/42]  eta: 0:01:31  Loss: 0.2256 (0.2256)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 2.1852  data: 0.5068  max mem: 1215\n",
      "Test: [Task 1]  [10/42]  eta: 0:00:21  Loss: 0.2478 (0.2655)  Acc@1: 100.0000 (98.4848)  Acc@5: 100.0000 (99.6212)  time: 0.6844  data: 0.0465  max mem: 1215\n",
      "Test: [Task 1]  [20/42]  eta: 0:00:13  Loss: 0.2217 (0.2579)  Acc@1: 100.0000 (98.0159)  Acc@5: 100.0000 (99.8016)  time: 0.5407  data: 0.0007  max mem: 1215\n",
      "Test: [Task 1]  [30/42]  eta: 0:00:07  Loss: 0.1975 (0.2367)  Acc@1: 100.0000 (98.5215)  Acc@5: 100.0000 (99.8656)  time: 0.5517  data: 0.0007  max mem: 1215\n",
      "Test: [Task 1]  [40/42]  eta: 0:00:01  Loss: 0.1993 (0.2311)  Acc@1: 100.0000 (98.3740)  Acc@5: 100.0000 (99.8984)  time: 0.5623  data: 0.0003  max mem: 1215\n",
      "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.1975 (0.2280)  Acc@1: 100.0000 (98.4000)  Acc@5: 100.0000 (99.9000)  time: 0.5613  data: 0.0003  max mem: 1215\n",
      "Test: [Task 1] Total time: 0:00:24 (0.5920 s / it)\n",
      "* Acc@1 98.400 Acc@5 99.900 loss 0.228\n",
      "[Average accuracy till task1]\tAcc@1: 98.4000\tAcc@5: 99.9000\tLoss: 0.2280\n",
      "Loading checkpoint from: ./output/checkpoint/task2_checkpoint.pth\n",
      "Test: [Task 1]  [ 0/42]  eta: 0:00:35  Loss: 0.3070 (0.3070)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  time: 0.8358  data: 0.3031  max mem: 1331\n",
      "Test: [Task 1]  [10/42]  eta: 0:00:18  Loss: 0.3070 (0.3330)  Acc@1: 91.6667 (92.0455)  Acc@5: 100.0000 (100.0000)  time: 0.5908  data: 0.0279  max mem: 1331\n",
      "Test: [Task 1]  [20/42]  eta: 0:00:12  Loss: 0.2903 (0.3417)  Acc@1: 91.6667 (91.0714)  Acc@5: 100.0000 (100.0000)  time: 0.5713  data: 0.0003  max mem: 1331\n",
      "Test: [Task 1]  [30/42]  eta: 0:00:07  Loss: 0.3021 (0.3333)  Acc@1: 91.6667 (91.2634)  Acc@5: 100.0000 (100.0000)  time: 0.5814  data: 0.0004  max mem: 1331\n",
      "Test: [Task 1]  [40/42]  eta: 0:00:01  Loss: 0.3077 (0.3234)  Acc@1: 91.6667 (91.7683)  Acc@5: 100.0000 (100.0000)  time: 0.5923  data: 0.0003  max mem: 1331\n",
      "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.2942 (0.3207)  Acc@1: 91.6667 (91.7000)  Acc@5: 100.0000 (100.0000)  time: 0.5838  data: 0.0003  max mem: 1331\n",
      "Test: [Task 1] Total time: 0:00:24 (0.5863 s / it)\n",
      "* Acc@1 91.700 Acc@5 100.000 loss 0.321\n",
      "Test: [Task 2]  [ 0/42]  eta: 0:00:38  Loss: 0.3993 (0.3993)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  time: 0.9216  data: 0.3583  max mem: 1331\n",
      "Test: [Task 2]  [10/42]  eta: 0:00:20  Loss: 0.3868 (0.3843)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  time: 0.6385  data: 0.0329  max mem: 1331\n",
      "Test: [Task 2]  [20/42]  eta: 0:00:13  Loss: 0.3757 (0.4040)  Acc@1: 95.8333 (94.8413)  Acc@5: 100.0000 (99.8016)  time: 0.6160  data: 0.0006  max mem: 1331\n",
      "Test: [Task 2]  [30/42]  eta: 0:00:07  Loss: 0.3757 (0.4016)  Acc@1: 95.8333 (94.8925)  Acc@5: 100.0000 (99.7312)  time: 0.6255  data: 0.0006  max mem: 1331\n",
      "Test: [Task 2]  [40/42]  eta: 0:00:01  Loss: 0.3444 (0.3918)  Acc@1: 95.8333 (94.9187)  Acc@5: 100.0000 (99.6951)  time: 0.6275  data: 0.0003  max mem: 1331\n",
      "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.3404 (0.3886)  Acc@1: 95.8333 (95.0000)  Acc@5: 100.0000 (99.7000)  time: 0.6173  data: 0.0003  max mem: 1331\n",
      "Test: [Task 2] Total time: 0:00:26 (0.6261 s / it)\n",
      "* Acc@1 95.000 Acc@5 99.700 loss 0.389\n",
      "[Average accuracy till task2]\tAcc@1: 93.3500\tAcc@5: 99.8500\tLoss: 0.3546\tForgetting: 6.7000\tBackward: -6.7000\n",
      "Loading checkpoint from: ./output/checkpoint/task3_checkpoint.pth\n",
      "Test: [Task 1]  [ 0/42]  eta: 0:00:40  Loss: 0.3115 (0.3115)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  time: 0.9701  data: 0.3597  max mem: 1331\n",
      "Test: [Task 1]  [10/42]  eta: 0:00:20  Loss: 0.3756 (0.3764)  Acc@1: 87.5000 (89.0152)  Acc@5: 100.0000 (100.0000)  time: 0.6376  data: 0.0332  max mem: 1331\n",
      "Test: [Task 1]  [20/42]  eta: 0:00:13  Loss: 0.3756 (0.3888)  Acc@1: 87.5000 (88.2937)  Acc@5: 100.0000 (100.0000)  time: 0.6018  data: 0.0005  max mem: 1331\n",
      "Test: [Task 1]  [30/42]  eta: 0:00:07  Loss: 0.3603 (0.3771)  Acc@1: 91.6667 (88.8441)  Acc@5: 100.0000 (100.0000)  time: 0.5973  data: 0.0005  max mem: 1331\n",
      "Test: [Task 1]  [40/42]  eta: 0:00:01  Loss: 0.3511 (0.3646)  Acc@1: 91.6667 (89.4309)  Acc@5: 100.0000 (100.0000)  time: 0.5934  data: 0.0003  max mem: 1331\n",
      "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.3511 (0.3609)  Acc@1: 91.6667 (89.6000)  Acc@5: 100.0000 (100.0000)  time: 0.5835  data: 0.0003  max mem: 1331\n",
      "Test: [Task 1] Total time: 0:00:25 (0.6038 s / it)\n",
      "* Acc@1 89.600 Acc@5 100.000 loss 0.361\n",
      "Test: [Task 2]  [ 0/42]  eta: 0:00:40  Loss: 0.4369 (0.4369)  Acc@1: 95.8333 (95.8333)  Acc@5: 95.8333 (95.8333)  time: 0.9723  data: 0.4222  max mem: 1331\n",
      "Test: [Task 2]  [10/42]  eta: 0:00:19  Loss: 0.3468 (0.3845)  Acc@1: 95.8333 (94.6970)  Acc@5: 100.0000 (98.8636)  time: 0.6247  data: 0.0387  max mem: 1331\n",
      "Test: [Task 2]  [20/42]  eta: 0:00:13  Loss: 0.3985 (0.4299)  Acc@1: 91.6667 (93.0556)  Acc@5: 100.0000 (98.4127)  time: 0.5905  data: 0.0007  max mem: 1331\n",
      "Test: [Task 2]  [30/42]  eta: 0:00:07  Loss: 0.4435 (0.4265)  Acc@1: 91.6667 (93.2796)  Acc@5: 100.0000 (98.1183)  time: 0.5917  data: 0.0007  max mem: 1331\n",
      "Test: [Task 2]  [40/42]  eta: 0:00:01  Loss: 0.3618 (0.4155)  Acc@1: 95.8333 (93.4959)  Acc@5: 100.0000 (98.3740)  time: 0.5939  data: 0.0004  max mem: 1331\n",
      "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.3583 (0.4101)  Acc@1: 95.8333 (93.6000)  Acc@5: 100.0000 (98.4000)  time: 0.5846  data: 0.0004  max mem: 1331\n",
      "Test: [Task 2] Total time: 0:00:25 (0.5989 s / it)\n",
      "* Acc@1 93.600 Acc@5 98.400 loss 0.410\n",
      "Test: [Task 3]  [ 0/42]  eta: 0:00:34  Loss: 0.2433 (0.2433)  Acc@1: 95.8333 (95.8333)  Acc@5: 95.8333 (95.8333)  time: 0.8167  data: 0.2619  max mem: 1331\n",
      "Test: [Task 3]  [10/42]  eta: 0:00:19  Loss: 0.3231 (0.3680)  Acc@1: 91.6667 (92.8030)  Acc@5: 100.0000 (98.8636)  time: 0.6184  data: 0.0243  max mem: 1331\n",
      "Test: [Task 3]  [20/42]  eta: 0:00:13  Loss: 0.3085 (0.3299)  Acc@1: 91.6667 (93.0556)  Acc@5: 100.0000 (99.2063)  time: 0.5997  data: 0.0005  max mem: 1331\n",
      "Test: [Task 3]  [30/42]  eta: 0:00:07  Loss: 0.2711 (0.3238)  Acc@1: 91.6667 (92.7419)  Acc@5: 100.0000 (99.4624)  time: 0.6011  data: 0.0004  max mem: 1331\n",
      "Test: [Task 3]  [40/42]  eta: 0:00:01  Loss: 0.3177 (0.3602)  Acc@1: 91.6667 (91.7683)  Acc@5: 100.0000 (99.1870)  time: 0.6019  data: 0.0005  max mem: 1331\n",
      "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.3077 (0.3545)  Acc@1: 91.6667 (91.9000)  Acc@5: 100.0000 (99.2000)  time: 0.5923  data: 0.0005  max mem: 1331\n",
      "Test: [Task 3] Total time: 0:00:25 (0.6034 s / it)\n",
      "* Acc@1 91.900 Acc@5 99.200 loss 0.354\n",
      "[Average accuracy till task3]\tAcc@1: 91.7000\tAcc@5: 99.2000\tLoss: 0.3752\tForgetting: 5.1000\tBackward: -5.1000\n",
      "Loading checkpoint from: ./output/checkpoint/task4_checkpoint.pth\n",
      "Test: [Task 1]  [ 0/42]  eta: 0:00:38  Loss: 0.3894 (0.3894)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  time: 0.9207  data: 0.3490  max mem: 1331\n",
      "Test: [Task 1]  [10/42]  eta: 0:00:20  Loss: 0.4127 (0.4118)  Acc@1: 91.6667 (90.1515)  Acc@5: 100.0000 (99.6212)  time: 0.6320  data: 0.0322  max mem: 1331\n",
      "Test: [Task 1]  [20/42]  eta: 0:00:13  Loss: 0.3837 (0.4240)  Acc@1: 91.6667 (88.6905)  Acc@5: 100.0000 (99.8016)  time: 0.6036  data: 0.0006  max mem: 1331\n",
      "Test: [Task 1]  [30/42]  eta: 0:00:07  Loss: 0.3835 (0.4102)  Acc@1: 91.6667 (89.1129)  Acc@5: 100.0000 (99.8656)  time: 0.6029  data: 0.0005  max mem: 1331\n",
      "Test: [Task 1]  [40/42]  eta: 0:00:01  Loss: 0.3835 (0.3926)  Acc@1: 91.6667 (89.2276)  Acc@5: 100.0000 (99.7967)  time: 0.6022  data: 0.0005  max mem: 1331\n",
      "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.3835 (0.3893)  Acc@1: 87.5000 (89.2000)  Acc@5: 100.0000 (99.8000)  time: 0.5923  data: 0.0005  max mem: 1331\n",
      "Test: [Task 1] Total time: 0:00:25 (0.6084 s / it)\n",
      "* Acc@1 89.200 Acc@5 99.800 loss 0.389\n",
      "Test: [Task 2]  [ 0/42]  eta: 0:00:41  Loss: 0.4453 (0.4453)  Acc@1: 95.8333 (95.8333)  Acc@5: 95.8333 (95.8333)  time: 0.9826  data: 0.4223  max mem: 1331\n",
      "Test: [Task 2]  [10/42]  eta: 0:00:20  Loss: 0.4453 (0.4480)  Acc@1: 87.5000 (90.1515)  Acc@5: 100.0000 (98.4848)  time: 0.6361  data: 0.0388  max mem: 1331\n",
      "Test: [Task 2]  [20/42]  eta: 0:00:13  Loss: 0.4790 (0.4924)  Acc@1: 87.5000 (89.0873)  Acc@5: 100.0000 (97.6190)  time: 0.6011  data: 0.0004  max mem: 1331\n",
      "Test: [Task 2]  [30/42]  eta: 0:00:07  Loss: 0.4790 (0.4903)  Acc@1: 87.5000 (89.3817)  Acc@5: 100.0000 (97.7151)  time: 0.6010  data: 0.0006  max mem: 1331\n",
      "Test: [Task 2]  [40/42]  eta: 0:00:01  Loss: 0.4177 (0.4736)  Acc@1: 91.6667 (90.0407)  Acc@5: 100.0000 (97.9675)  time: 0.6011  data: 0.0005  max mem: 1331\n",
      "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.3845 (0.4693)  Acc@1: 91.6667 (90.1000)  Acc@5: 100.0000 (98.0000)  time: 0.5914  data: 0.0005  max mem: 1331\n",
      "Test: [Task 2] Total time: 0:00:25 (0.6086 s / it)\n",
      "* Acc@1 90.100 Acc@5 98.000 loss 0.469\n",
      "Test: [Task 3]  [ 0/42]  eta: 0:00:32  Loss: 0.2552 (0.2552)  Acc@1: 95.8333 (95.8333)  Acc@5: 95.8333 (95.8333)  time: 0.7740  data: 0.2149  max mem: 1331\n",
      "Test: [Task 3]  [10/42]  eta: 0:00:19  Loss: 0.3143 (0.3676)  Acc@1: 91.6667 (92.0455)  Acc@5: 100.0000 (98.8636)  time: 0.6159  data: 0.0203  max mem: 1331\n",
      "Test: [Task 3]  [20/42]  eta: 0:00:13  Loss: 0.3070 (0.3352)  Acc@1: 91.6667 (92.0635)  Acc@5: 100.0000 (99.0079)  time: 0.6005  data: 0.0006  max mem: 1331\n",
      "Test: [Task 3]  [30/42]  eta: 0:00:07  Loss: 0.2693 (0.3330)  Acc@1: 91.6667 (91.8011)  Acc@5: 100.0000 (99.1935)  time: 0.6006  data: 0.0006  max mem: 1331\n",
      "Test: [Task 3]  [40/42]  eta: 0:00:01  Loss: 0.3950 (0.3773)  Acc@1: 91.6667 (90.8537)  Acc@5: 100.0000 (98.7805)  time: 0.6002  data: 0.0005  max mem: 1331\n",
      "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.3490 (0.3712)  Acc@1: 91.6667 (91.0000)  Acc@5: 100.0000 (98.8000)  time: 0.5907  data: 0.0004  max mem: 1331\n",
      "Test: [Task 3] Total time: 0:00:25 (0.6024 s / it)\n",
      "* Acc@1 91.000 Acc@5 98.800 loss 0.371\n",
      "Test: [Task 4]  [ 0/42]  eta: 0:00:32  Loss: 0.7601 (0.7601)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  time: 0.7787  data: 0.2187  max mem: 1331\n",
      "Test: [Task 4]  [10/42]  eta: 0:00:19  Loss: 0.3755 (0.4379)  Acc@1: 91.6667 (89.0152)  Acc@5: 100.0000 (97.3485)  time: 0.6166  data: 0.0204  max mem: 1331\n",
      "Test: [Task 4]  [20/42]  eta: 0:00:13  Loss: 0.3707 (0.4148)  Acc@1: 91.6667 (89.6825)  Acc@5: 100.0000 (98.2143)  time: 0.6011  data: 0.0006  max mem: 1331\n",
      "Test: [Task 4]  [30/42]  eta: 0:00:07  Loss: 0.2620 (0.3688)  Acc@1: 95.8333 (91.3979)  Acc@5: 100.0000 (98.6559)  time: 0.6014  data: 0.0005  max mem: 1331\n",
      "Test: [Task 4]  [40/42]  eta: 0:00:01  Loss: 0.3220 (0.4030)  Acc@1: 91.6667 (90.3455)  Acc@5: 100.0000 (98.2724)  time: 0.6013  data: 0.0003  max mem: 1331\n",
      "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.2759 (0.3981)  Acc@1: 91.6667 (90.5000)  Acc@5: 100.0000 (98.3000)  time: 0.5915  data: 0.0003  max mem: 1331\n",
      "Test: [Task 4] Total time: 0:00:25 (0.6030 s / it)\n",
      "* Acc@1 90.500 Acc@5 98.300 loss 0.398\n",
      "[Average accuracy till task4]\tAcc@1: 90.2000\tAcc@5: 98.7250\tLoss: 0.4070\tForgetting: 5.0000\tBackward: -5.0000\n",
      "Loading checkpoint from: ./output/checkpoint/task5_checkpoint.pth\n",
      "Test: [Task 1]  [ 0/42]  eta: 0:00:32  Loss: 0.4445 (0.4445)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  time: 0.7833  data: 0.2204  max mem: 1331\n",
      "Test: [Task 1]  [10/42]  eta: 0:00:19  Loss: 0.4272 (0.4393)  Acc@1: 87.5000 (86.7424)  Acc@5: 100.0000 (99.6212)  time: 0.6178  data: 0.0205  max mem: 1331\n",
      "Test: [Task 1]  [20/42]  eta: 0:00:13  Loss: 0.4132 (0.4615)  Acc@1: 87.5000 (86.5079)  Acc@5: 100.0000 (99.8016)  time: 0.6012  data: 0.0005  max mem: 1331\n",
      "Test: [Task 1]  [30/42]  eta: 0:00:07  Loss: 0.4247 (0.4481)  Acc@1: 87.5000 (87.3656)  Acc@5: 100.0000 (99.8656)  time: 0.6008  data: 0.0007  max mem: 1331\n",
      "Test: [Task 1]  [40/42]  eta: 0:00:01  Loss: 0.4022 (0.4258)  Acc@1: 91.6667 (88.0081)  Acc@5: 100.0000 (99.7967)  time: 0.6002  data: 0.0007  max mem: 1331\n",
      "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.3963 (0.4211)  Acc@1: 91.6667 (88.1000)  Acc@5: 100.0000 (99.8000)  time: 0.5905  data: 0.0007  max mem: 1331\n",
      "Test: [Task 1] Total time: 0:00:25 (0.6026 s / it)\n",
      "* Acc@1 88.100 Acc@5 99.800 loss 0.421\n",
      "Test: [Task 2]  [ 0/42]  eta: 0:00:35  Loss: 0.4718 (0.4718)  Acc@1: 95.8333 (95.8333)  Acc@5: 95.8333 (95.8333)  time: 0.8336  data: 0.2717  max mem: 1331\n",
      "Test: [Task 2]  [10/42]  eta: 0:00:19  Loss: 0.4923 (0.4994)  Acc@1: 87.5000 (89.3939)  Acc@5: 100.0000 (98.1061)  time: 0.6214  data: 0.0252  max mem: 1331\n",
      "Test: [Task 2]  [20/42]  eta: 0:00:13  Loss: 0.5748 (0.5402)  Acc@1: 83.3333 (86.7064)  Acc@5: 100.0000 (97.6190)  time: 0.6009  data: 0.0005  max mem: 1331\n",
      "Test: [Task 2]  [30/42]  eta: 0:00:07  Loss: 0.5611 (0.5341)  Acc@1: 83.3333 (87.2312)  Acc@5: 100.0000 (97.7151)  time: 0.6016  data: 0.0004  max mem: 1331\n",
      "Test: [Task 2]  [40/42]  eta: 0:00:01  Loss: 0.4447 (0.5128)  Acc@1: 91.6667 (87.8049)  Acc@5: 100.0000 (97.8659)  time: 0.6018  data: 0.0003  max mem: 1331\n",
      "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.4406 (0.5100)  Acc@1: 91.6667 (87.8000)  Acc@5: 100.0000 (97.9000)  time: 0.5922  data: 0.0003  max mem: 1331\n",
      "Test: [Task 2] Total time: 0:00:25 (0.6045 s / it)\n",
      "* Acc@1 87.800 Acc@5 97.900 loss 0.510\n",
      "Test: [Task 3]  [ 0/42]  eta: 0:00:33  Loss: 0.2946 (0.2946)  Acc@1: 95.8333 (95.8333)  Acc@5: 95.8333 (95.8333)  time: 0.7876  data: 0.2285  max mem: 1331\n",
      "Test: [Task 3]  [10/42]  eta: 0:00:19  Loss: 0.4203 (0.4312)  Acc@1: 87.5000 (89.7727)  Acc@5: 100.0000 (98.8636)  time: 0.6183  data: 0.0213  max mem: 1331\n",
      "Test: [Task 3]  [20/42]  eta: 0:00:13  Loss: 0.3740 (0.4106)  Acc@1: 87.5000 (89.8810)  Acc@5: 100.0000 (98.8095)  time: 0.6021  data: 0.0005  max mem: 1331\n",
      "Test: [Task 3]  [30/42]  eta: 0:00:07  Loss: 0.3570 (0.4128)  Acc@1: 91.6667 (89.5161)  Acc@5: 100.0000 (99.0591)  time: 0.6029  data: 0.0005  max mem: 1331\n",
      "Test: [Task 3]  [40/42]  eta: 0:00:01  Loss: 0.4353 (0.4499)  Acc@1: 91.6667 (89.0244)  Acc@5: 100.0000 (98.5772)  time: 0.6032  data: 0.0004  max mem: 1331\n",
      "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.4020 (0.4431)  Acc@1: 91.6667 (89.2000)  Acc@5: 100.0000 (98.6000)  time: 0.5936  data: 0.0003  max mem: 1331\n",
      "Test: [Task 3] Total time: 0:00:25 (0.6047 s / it)\n",
      "* Acc@1 89.200 Acc@5 98.600 loss 0.443\n",
      "Test: [Task 4]  [ 0/42]  eta: 0:00:39  Loss: 0.7253 (0.7253)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  time: 0.9380  data: 0.3798  max mem: 1331\n",
      "Test: [Task 4]  [10/42]  eta: 0:00:20  Loss: 0.4416 (0.4716)  Acc@1: 87.5000 (89.3939)  Acc@5: 95.8333 (96.2121)  time: 0.6305  data: 0.0352  max mem: 1331\n",
      "Test: [Task 4]  [20/42]  eta: 0:00:13  Loss: 0.3906 (0.4567)  Acc@1: 91.6667 (88.4921)  Acc@5: 95.8333 (97.0238)  time: 0.6006  data: 0.0006  max mem: 1331\n",
      "Test: [Task 4]  [30/42]  eta: 0:00:07  Loss: 0.2731 (0.4061)  Acc@1: 95.8333 (90.4570)  Acc@5: 100.0000 (97.8495)  time: 0.6010  data: 0.0007  max mem: 1331\n",
      "Test: [Task 4]  [40/42]  eta: 0:00:01  Loss: 0.3125 (0.4491)  Acc@1: 91.6667 (89.1260)  Acc@5: 100.0000 (97.4594)  time: 0.6009  data: 0.0006  max mem: 1331\n",
      "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.3021 (0.4437)  Acc@1: 91.6667 (89.3000)  Acc@5: 100.0000 (97.5000)  time: 0.5911  data: 0.0006  max mem: 1331\n",
      "Test: [Task 4] Total time: 0:00:25 (0.6064 s / it)\n",
      "* Acc@1 89.300 Acc@5 97.500 loss 0.444\n",
      "Test: [Task 5]  [ 0/42]  eta: 0:00:37  Loss: 0.1241 (0.1241)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.9000  data: 0.3366  max mem: 1331\n",
      "Test: [Task 5]  [10/42]  eta: 0:00:20  Loss: 0.2714 (0.3146)  Acc@1: 100.0000 (95.0758)  Acc@5: 100.0000 (98.8636)  time: 0.6267  data: 0.0313  max mem: 1331\n",
      "Test: [Task 5]  [20/42]  eta: 0:00:13  Loss: 0.2892 (0.3403)  Acc@1: 91.6667 (92.4603)  Acc@5: 100.0000 (98.8095)  time: 0.5996  data: 0.0006  max mem: 1331\n",
      "Test: [Task 5]  [30/42]  eta: 0:00:07  Loss: 0.2648 (0.3275)  Acc@1: 87.5000 (91.9355)  Acc@5: 100.0000 (98.9247)  time: 0.5997  data: 0.0006  max mem: 1331\n",
      "Test: [Task 5]  [40/42]  eta: 0:00:01  Loss: 0.2648 (0.3422)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (98.7805)  time: 0.5989  data: 0.0005  max mem: 1331\n",
      "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.2815 (0.3564)  Acc@1: 91.6667 (91.4000)  Acc@5: 100.0000 (98.7000)  time: 0.5891  data: 0.0005  max mem: 1331\n",
      "Test: [Task 5] Total time: 0:00:25 (0.6040 s / it)\n",
      "* Acc@1 91.400 Acc@5 98.700 loss 0.356\n",
      "[Average accuracy till task5]\tAcc@1: 89.1600\tAcc@5: 98.5000\tLoss: 0.4349\tForgetting: 5.3500\tBackward: -5.3500\n",
      "Loading checkpoint from: ./output/checkpoint/task6_checkpoint.pth\n",
      "Test: [Task 1]  [ 0/42]  eta: 0:00:45  Loss: 0.4931 (0.4931)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (95.8333)  time: 1.0936  data: 0.4981  max mem: 1331\n",
      "Test: [Task 1]  [10/42]  eta: 0:00:20  Loss: 0.4931 (0.4683)  Acc@1: 87.5000 (87.1212)  Acc@5: 100.0000 (99.2424)  time: 0.6390  data: 0.0460  max mem: 1331\n",
      "Test: [Task 1]  [20/42]  eta: 0:00:13  Loss: 0.4669 (0.5114)  Acc@1: 87.5000 (85.7143)  Acc@5: 100.0000 (99.4048)  time: 0.5951  data: 0.0006  max mem: 1331\n",
      "Test: [Task 1]  [30/42]  eta: 0:00:07  Loss: 0.4530 (0.4909)  Acc@1: 87.5000 (86.6936)  Acc@5: 100.0000 (99.4624)  time: 0.5989  data: 0.0007  max mem: 1331\n",
      "Test: [Task 1]  [40/42]  eta: 0:00:01  Loss: 0.4252 (0.4701)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (99.3902)  time: 0.6027  data: 0.0006  max mem: 1331\n",
      "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.4173 (0.4656)  Acc@1: 87.5000 (87.6000)  Acc@5: 100.0000 (99.4000)  time: 0.5933  data: 0.0006  max mem: 1331\n",
      "Test: [Task 1] Total time: 0:00:25 (0.6083 s / it)\n",
      "* Acc@1 87.600 Acc@5 99.400 loss 0.466\n",
      "Test: [Task 2]  [ 0/42]  eta: 0:00:34  Loss: 0.5791 (0.5791)  Acc@1: 95.8333 (95.8333)  Acc@5: 95.8333 (95.8333)  time: 0.8213  data: 0.2527  max mem: 1331\n",
      "Test: [Task 2]  [10/42]  eta: 0:00:19  Loss: 0.5761 (0.5492)  Acc@1: 87.5000 (88.6364)  Acc@5: 100.0000 (97.3485)  time: 0.6244  data: 0.0235  max mem: 1331\n",
      "Test: [Task 2]  [20/42]  eta: 0:00:13  Loss: 0.5996 (0.5952)  Acc@1: 83.3333 (85.3175)  Acc@5: 95.8333 (96.6270)  time: 0.6046  data: 0.0005  max mem: 1331\n",
      "Test: [Task 2]  [30/42]  eta: 0:00:07  Loss: 0.6061 (0.5876)  Acc@1: 83.3333 (85.7527)  Acc@5: 95.8333 (96.9086)  time: 0.6040  data: 0.0006  max mem: 1331\n",
      "Test: [Task 2]  [40/42]  eta: 0:00:01  Loss: 0.5049 (0.5646)  Acc@1: 87.5000 (86.1789)  Acc@5: 100.0000 (97.2561)  time: 0.6024  data: 0.0005  max mem: 1331\n",
      "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.4888 (0.5605)  Acc@1: 87.5000 (86.2000)  Acc@5: 100.0000 (97.3000)  time: 0.5920  data: 0.0004  max mem: 1331\n",
      "Test: [Task 2] Total time: 0:00:25 (0.6060 s / it)\n",
      "* Acc@1 86.200 Acc@5 97.300 loss 0.560\n",
      "Test: [Task 3]  [ 0/42]  eta: 0:00:34  Loss: 0.3188 (0.3188)  Acc@1: 95.8333 (95.8333)  Acc@5: 95.8333 (95.8333)  time: 0.8188  data: 0.2492  max mem: 1331\n",
      "Test: [Task 3]  [10/42]  eta: 0:00:19  Loss: 0.4179 (0.4655)  Acc@1: 91.6667 (90.1515)  Acc@5: 100.0000 (98.8636)  time: 0.6175  data: 0.0236  max mem: 1331\n",
      "Test: [Task 3]  [20/42]  eta: 0:00:13  Loss: 0.3787 (0.4345)  Acc@1: 91.6667 (89.8810)  Acc@5: 100.0000 (98.6111)  time: 0.5974  data: 0.0007  max mem: 1331\n",
      "Test: [Task 3]  [30/42]  eta: 0:00:07  Loss: 0.3383 (0.4307)  Acc@1: 91.6667 (89.2473)  Acc@5: 100.0000 (98.7903)  time: 0.5969  data: 0.0005  max mem: 1331\n",
      "Test: [Task 3]  [40/42]  eta: 0:00:01  Loss: 0.4555 (0.4730)  Acc@1: 87.5000 (88.8211)  Acc@5: 100.0000 (97.9675)  time: 0.5958  data: 0.0004  max mem: 1331\n",
      "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.4500 (0.4672)  Acc@1: 87.5000 (88.9000)  Acc@5: 100.0000 (98.0000)  time: 0.5860  data: 0.0004  max mem: 1331\n",
      "Test: [Task 3] Total time: 0:00:25 (0.5995 s / it)\n",
      "* Acc@1 88.900 Acc@5 98.000 loss 0.467\n",
      "Test: [Task 4]  [ 0/42]  eta: 0:00:40  Loss: 0.8731 (0.8731)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  time: 0.9647  data: 0.4067  max mem: 1331\n",
      "Test: [Task 4]  [10/42]  eta: 0:00:20  Loss: 0.4778 (0.5273)  Acc@1: 87.5000 (87.1212)  Acc@5: 95.8333 (96.2121)  time: 0.6276  data: 0.0377  max mem: 1331\n",
      "Test: [Task 4]  [20/42]  eta: 0:00:13  Loss: 0.4530 (0.5117)  Acc@1: 87.5000 (87.1032)  Acc@5: 95.8333 (96.6270)  time: 0.5949  data: 0.0006  max mem: 1331\n",
      "Test: [Task 4]  [30/42]  eta: 0:00:07  Loss: 0.3211 (0.4546)  Acc@1: 91.6667 (88.9785)  Acc@5: 100.0000 (97.4462)  time: 0.5970  data: 0.0005  max mem: 1331\n",
      "Test: [Task 4]  [40/42]  eta: 0:00:01  Loss: 0.3389 (0.5001)  Acc@1: 91.6667 (87.7033)  Acc@5: 100.0000 (97.0528)  time: 0.5985  data: 0.0004  max mem: 1331\n",
      "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.3270 (0.4955)  Acc@1: 91.6667 (87.9000)  Acc@5: 100.0000 (97.1000)  time: 0.5888  data: 0.0004  max mem: 1331\n",
      "Test: [Task 4] Total time: 0:00:25 (0.6033 s / it)\n",
      "* Acc@1 87.900 Acc@5 97.100 loss 0.496\n",
      "Test: [Task 5]  [ 0/42]  eta: 0:00:40  Loss: 0.1563 (0.1563)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.9632  data: 0.3903  max mem: 1331\n",
      "Test: [Task 5]  [10/42]  eta: 0:00:20  Loss: 0.2974 (0.3737)  Acc@1: 95.8333 (93.1818)  Acc@5: 100.0000 (98.8636)  time: 0.6331  data: 0.0362  max mem: 1331\n",
      "Test: [Task 5]  [20/42]  eta: 0:00:13  Loss: 0.3695 (0.3907)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (99.0079)  time: 0.6008  data: 0.0006  max mem: 1331\n",
      "Test: [Task 5]  [30/42]  eta: 0:00:07  Loss: 0.3443 (0.3816)  Acc@1: 91.6667 (90.9946)  Acc@5: 100.0000 (99.0591)  time: 0.6019  data: 0.0006  max mem: 1331\n",
      "Test: [Task 5]  [40/42]  eta: 0:00:01  Loss: 0.3406 (0.3959)  Acc@1: 91.6667 (90.5488)  Acc@5: 100.0000 (98.9837)  time: 0.6030  data: 0.0006  max mem: 1331\n",
      "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.3443 (0.4097)  Acc@1: 87.5000 (90.2000)  Acc@5: 100.0000 (98.8000)  time: 0.5931  data: 0.0005  max mem: 1331\n",
      "Test: [Task 5] Total time: 0:00:25 (0.6085 s / it)\n",
      "* Acc@1 90.200 Acc@5 98.800 loss 0.410\n",
      "Test: [Task 6]  [ 0/42]  eta: 0:00:35  Loss: 0.4873 (0.4873)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.8479  data: 0.2729  max mem: 1331\n",
      "Test: [Task 6]  [10/42]  eta: 0:00:19  Loss: 0.4951 (0.4897)  Acc@1: 87.5000 (84.8485)  Acc@5: 100.0000 (99.6212)  time: 0.6241  data: 0.0253  max mem: 1331\n",
      "Test: [Task 6]  [20/42]  eta: 0:00:13  Loss: 0.4511 (0.4757)  Acc@1: 87.5000 (85.1191)  Acc@5: 100.0000 (99.4048)  time: 0.6013  data: 0.0005  max mem: 1331\n",
      "Test: [Task 6]  [30/42]  eta: 0:00:07  Loss: 0.4511 (0.4792)  Acc@1: 83.3333 (84.5430)  Acc@5: 100.0000 (99.3280)  time: 0.6003  data: 0.0005  max mem: 1331\n",
      "Test: [Task 6]  [40/42]  eta: 0:00:01  Loss: 0.4711 (0.4892)  Acc@1: 83.3333 (84.5528)  Acc@5: 100.0000 (98.9837)  time: 0.5994  data: 0.0004  max mem: 1331\n",
      "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.4577 (0.4857)  Acc@1: 83.3333 (84.6000)  Acc@5: 100.0000 (99.0000)  time: 0.5898  data: 0.0004  max mem: 1331\n",
      "Test: [Task 6] Total time: 0:00:25 (0.6038 s / it)\n",
      "* Acc@1 84.600 Acc@5 99.000 loss 0.486\n",
      "[Average accuracy till task6]\tAcc@1: 87.5667\tAcc@5: 98.2667\tLoss: 0.4807\tForgetting: 5.2800\tBackward: -5.2800\n",
      "Loading checkpoint from: ./output/checkpoint/task7_checkpoint.pth\n",
      "Test: [Task 1]  [ 0/42]  eta: 0:00:54  Loss: 0.4927 (0.4927)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 1.2926  data: 0.6613  max mem: 1331\n",
      "Test: [Task 1]  [10/42]  eta: 0:00:21  Loss: 0.5077 (0.5130)  Acc@1: 87.5000 (85.6061)  Acc@5: 100.0000 (99.2424)  time: 0.6569  data: 0.0605  max mem: 1331\n",
      "Test: [Task 1]  [20/42]  eta: 0:00:13  Loss: 0.5077 (0.5432)  Acc@1: 83.3333 (84.9206)  Acc@5: 100.0000 (99.0079)  time: 0.5951  data: 0.0007  max mem: 1331\n",
      "Test: [Task 1]  [30/42]  eta: 0:00:07  Loss: 0.4929 (0.5267)  Acc@1: 87.5000 (85.3495)  Acc@5: 100.0000 (99.3280)  time: 0.5981  data: 0.0007  max mem: 1331\n",
      "Test: [Task 1]  [40/42]  eta: 0:00:01  Loss: 0.4700 (0.5021)  Acc@1: 87.5000 (86.2805)  Acc@5: 100.0000 (99.2886)  time: 0.6006  data: 0.0003  max mem: 1331\n",
      "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.4524 (0.4976)  Acc@1: 87.5000 (86.3000)  Acc@5: 100.0000 (99.3000)  time: 0.5913  data: 0.0003  max mem: 1331\n",
      "Test: [Task 1] Total time: 0:00:25 (0.6129 s / it)\n",
      "* Acc@1 86.300 Acc@5 99.300 loss 0.498\n",
      "Test: [Task 2]  [ 0/42]  eta: 0:00:43  Loss: 0.5748 (0.5748)  Acc@1: 91.6667 (91.6667)  Acc@5: 95.8333 (95.8333)  time: 1.0283  data: 0.4682  max mem: 1331\n",
      "Test: [Task 2]  [10/42]  eta: 0:00:20  Loss: 0.5495 (0.5859)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (96.9697)  time: 0.6422  data: 0.0429  max mem: 1331\n",
      "Test: [Task 2]  [20/42]  eta: 0:00:13  Loss: 0.6279 (0.6325)  Acc@1: 83.3333 (84.3254)  Acc@5: 100.0000 (96.8254)  time: 0.6037  data: 0.0004  max mem: 1331\n",
      "Test: [Task 2]  [30/42]  eta: 0:00:07  Loss: 0.5973 (0.6229)  Acc@1: 83.3333 (84.8118)  Acc@5: 100.0000 (97.1774)  time: 0.6026  data: 0.0005  max mem: 1331\n",
      "Test: [Task 2]  [40/42]  eta: 0:00:01  Loss: 0.5404 (0.5975)  Acc@1: 87.5000 (85.8740)  Acc@5: 100.0000 (97.3577)  time: 0.6009  data: 0.0005  max mem: 1331\n",
      "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.4955 (0.5925)  Acc@1: 87.5000 (85.9000)  Acc@5: 100.0000 (97.4000)  time: 0.5908  data: 0.0005  max mem: 1331\n",
      "Test: [Task 2] Total time: 0:00:25 (0.6103 s / it)\n",
      "* Acc@1 85.900 Acc@5 97.400 loss 0.592\n",
      "Test: [Task 3]  [ 0/42]  eta: 0:00:38  Loss: 0.3599 (0.3599)  Acc@1: 95.8333 (95.8333)  Acc@5: 95.8333 (95.8333)  time: 0.9260  data: 0.3631  max mem: 1331\n",
      "Test: [Task 3]  [10/42]  eta: 0:00:20  Loss: 0.4891 (0.5132)  Acc@1: 87.5000 (86.3636)  Acc@5: 100.0000 (98.4848)  time: 0.6295  data: 0.0334  max mem: 1331\n",
      "Test: [Task 3]  [20/42]  eta: 0:00:13  Loss: 0.4494 (0.4824)  Acc@1: 87.5000 (86.9048)  Acc@5: 100.0000 (98.6111)  time: 0.5993  data: 0.0004  max mem: 1331\n",
      "Test: [Task 3]  [30/42]  eta: 0:00:07  Loss: 0.3935 (0.4679)  Acc@1: 87.5000 (86.8280)  Acc@5: 100.0000 (98.7903)  time: 0.5990  data: 0.0004  max mem: 1331\n",
      "Test: [Task 3]  [40/42]  eta: 0:00:01  Loss: 0.4850 (0.5230)  Acc@1: 87.5000 (86.6870)  Acc@5: 95.8333 (97.6626)  time: 0.5992  data: 0.0004  max mem: 1331\n",
      "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.4596 (0.5163)  Acc@1: 87.5000 (86.9000)  Acc@5: 95.8333 (97.7000)  time: 0.5890  data: 0.0004  max mem: 1331\n",
      "Test: [Task 3] Total time: 0:00:25 (0.6045 s / it)\n",
      "* Acc@1 86.900 Acc@5 97.700 loss 0.516\n",
      "Test: [Task 4]  [ 0/42]  eta: 0:00:41  Loss: 0.9897 (0.9897)  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  time: 0.9784  data: 0.4189  max mem: 1331\n",
      "Test: [Task 4]  [10/42]  eta: 0:00:20  Loss: 0.4880 (0.5549)  Acc@1: 87.5000 (85.2273)  Acc@5: 95.8333 (96.2121)  time: 0.6322  data: 0.0384  max mem: 1331\n",
      "Test: [Task 4]  [20/42]  eta: 0:00:13  Loss: 0.4865 (0.5365)  Acc@1: 87.5000 (86.3095)  Acc@5: 95.8333 (96.4286)  time: 0.5975  data: 0.0005  max mem: 1331\n",
      "Test: [Task 4]  [30/42]  eta: 0:00:07  Loss: 0.3108 (0.4727)  Acc@1: 91.6667 (88.5753)  Acc@5: 100.0000 (97.0430)  time: 0.5975  data: 0.0007  max mem: 1331\n",
      "Test: [Task 4]  [40/42]  eta: 0:00:01  Loss: 0.4418 (0.5138)  Acc@1: 87.5000 (86.9919)  Acc@5: 95.8333 (96.7480)  time: 0.5971  data: 0.0005  max mem: 1331\n",
      "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.3428 (0.5083)  Acc@1: 87.5000 (87.1000)  Acc@5: 100.0000 (96.8000)  time: 0.5876  data: 0.0005  max mem: 1331\n",
      "Test: [Task 4] Total time: 0:00:25 (0.6041 s / it)\n",
      "* Acc@1 87.100 Acc@5 96.800 loss 0.508\n",
      "Test: [Task 5]  [ 0/42]  eta: 0:00:40  Loss: 0.2389 (0.2389)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  time: 0.9548  data: 0.4002  max mem: 1331\n",
      "Test: [Task 5]  [10/42]  eta: 0:00:20  Loss: 0.3597 (0.4106)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (98.4848)  time: 0.6298  data: 0.0367  max mem: 1331\n",
      "Test: [Task 5]  [20/42]  eta: 0:00:13  Loss: 0.3597 (0.4236)  Acc@1: 91.6667 (90.0794)  Acc@5: 100.0000 (98.2143)  time: 0.5971  data: 0.0007  max mem: 1331\n",
      "Test: [Task 5]  [30/42]  eta: 0:00:07  Loss: 0.3531 (0.4200)  Acc@1: 87.5000 (89.6505)  Acc@5: 100.0000 (98.2527)  time: 0.5968  data: 0.0008  max mem: 1331\n",
      "Test: [Task 5]  [40/42]  eta: 0:00:01  Loss: 0.3890 (0.4410)  Acc@1: 87.5000 (88.8211)  Acc@5: 100.0000 (98.1707)  time: 0.5969  data: 0.0005  max mem: 1331\n",
      "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.4451 (0.4562)  Acc@1: 87.5000 (88.5000)  Acc@5: 100.0000 (98.0000)  time: 0.5871  data: 0.0004  max mem: 1331\n",
      "Test: [Task 5] Total time: 0:00:25 (0.6030 s / it)\n",
      "* Acc@1 88.500 Acc@5 98.000 loss 0.456\n",
      "Test: [Task 6]  [ 0/42]  eta: 0:00:35  Loss: 0.5181 (0.5181)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.8402  data: 0.2810  max mem: 1331\n",
      "Test: [Task 6]  [10/42]  eta: 0:00:19  Loss: 0.5181 (0.5316)  Acc@1: 83.3333 (84.4697)  Acc@5: 100.0000 (98.4848)  time: 0.6188  data: 0.0259  max mem: 1331\n",
      "Test: [Task 6]  [20/42]  eta: 0:00:13  Loss: 0.5167 (0.5119)  Acc@1: 83.3333 (85.3175)  Acc@5: 100.0000 (98.8095)  time: 0.5967  data: 0.0004  max mem: 1331\n",
      "Test: [Task 6]  [30/42]  eta: 0:00:07  Loss: 0.5171 (0.5189)  Acc@1: 87.5000 (84.2742)  Acc@5: 100.0000 (98.7903)  time: 0.5969  data: 0.0005  max mem: 1331\n",
      "Test: [Task 6]  [40/42]  eta: 0:00:01  Loss: 0.5492 (0.5326)  Acc@1: 83.3333 (84.2480)  Acc@5: 100.0000 (98.3740)  time: 0.5966  data: 0.0004  max mem: 1331\n",
      "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.5241 (0.5285)  Acc@1: 83.3333 (84.3000)  Acc@5: 100.0000 (98.4000)  time: 0.5869  data: 0.0004  max mem: 1331\n",
      "Test: [Task 6] Total time: 0:00:25 (0.6001 s / it)\n",
      "* Acc@1 84.300 Acc@5 98.400 loss 0.529\n",
      "Test: [Task 7]  [ 0/42]  eta: 0:00:42  Loss: 0.3287 (0.3287)  Acc@1: 91.6667 (91.6667)  Acc@5: 95.8333 (95.8333)  time: 1.0176  data: 0.4602  max mem: 1331\n",
      "Test: [Task 7]  [10/42]  eta: 0:00:20  Loss: 0.3202 (0.3809)  Acc@1: 91.6667 (89.7727)  Acc@5: 100.0000 (98.1061)  time: 0.6348  data: 0.0423  max mem: 1331\n",
      "Test: [Task 7]  [20/42]  eta: 0:00:13  Loss: 0.3557 (0.4198)  Acc@1: 87.5000 (88.6905)  Acc@5: 100.0000 (98.4127)  time: 0.5962  data: 0.0007  max mem: 1331\n",
      "Test: [Task 7]  [30/42]  eta: 0:00:07  Loss: 0.4215 (0.4589)  Acc@1: 87.5000 (88.7097)  Acc@5: 100.0000 (97.8495)  time: 0.5962  data: 0.0007  max mem: 1331\n",
      "Test: [Task 7]  [40/42]  eta: 0:00:01  Loss: 0.4215 (0.4473)  Acc@1: 87.5000 (88.5163)  Acc@5: 100.0000 (97.9675)  time: 0.5969  data: 0.0004  max mem: 1331\n",
      "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.4168 (0.4448)  Acc@1: 87.5000 (88.5000)  Acc@5: 100.0000 (98.0000)  time: 0.5874  data: 0.0004  max mem: 1331\n",
      "Test: [Task 7] Total time: 0:00:25 (0.6042 s / it)\n",
      "* Acc@1 88.500 Acc@5 98.000 loss 0.445\n",
      "[Average accuracy till task7]\tAcc@1: 86.7857\tAcc@5: 97.9429\tLoss: 0.5063\tForgetting: 5.4667\tBackward: -5.4667\n",
      "Loading checkpoint from: ./output/checkpoint/task8_checkpoint.pth\n",
      "Test: [Task 1]  [ 0/42]  eta: 0:00:37  Loss: 0.5243 (0.5243)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (95.8333)  time: 0.8998  data: 0.3462  max mem: 1331\n",
      "Test: [Task 1]  [10/42]  eta: 0:00:19  Loss: 0.5272 (0.5139)  Acc@1: 87.5000 (87.1212)  Acc@5: 100.0000 (98.4848)  time: 0.6234  data: 0.0320  max mem: 1331\n",
      "Test: [Task 1]  [20/42]  eta: 0:00:13  Loss: 0.5272 (0.5492)  Acc@1: 87.5000 (85.3175)  Acc@5: 100.0000 (99.2063)  time: 0.5970  data: 0.0006  max mem: 1331\n",
      "Test: [Task 1]  [30/42]  eta: 0:00:07  Loss: 0.4931 (0.5352)  Acc@1: 87.5000 (85.8871)  Acc@5: 100.0000 (99.3280)  time: 0.5990  data: 0.0006  max mem: 1331\n",
      "Test: [Task 1]  [40/42]  eta: 0:00:01  Loss: 0.4759 (0.5122)  Acc@1: 87.5000 (86.5854)  Acc@5: 100.0000 (99.0854)  time: 0.5994  data: 0.0004  max mem: 1331\n",
      "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.4723 (0.5071)  Acc@1: 87.5000 (86.6000)  Acc@5: 100.0000 (99.1000)  time: 0.5896  data: 0.0004  max mem: 1331\n",
      "Test: [Task 1] Total time: 0:00:25 (0.6030 s / it)\n",
      "* Acc@1 86.600 Acc@5 99.100 loss 0.507\n",
      "Test: [Task 2]  [ 0/42]  eta: 0:00:39  Loss: 0.6133 (0.6133)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (95.8333)  time: 0.9495  data: 0.3909  max mem: 1331\n",
      "Test: [Task 2]  [10/42]  eta: 0:00:20  Loss: 0.5885 (0.5802)  Acc@1: 87.5000 (86.3636)  Acc@5: 100.0000 (96.9697)  time: 0.6342  data: 0.0359  max mem: 1331\n",
      "Test: [Task 2]  [20/42]  eta: 0:00:13  Loss: 0.5885 (0.6228)  Acc@1: 83.3333 (84.1270)  Acc@5: 100.0000 (96.6270)  time: 0.6023  data: 0.0004  max mem: 1331\n",
      "Test: [Task 2]  [30/42]  eta: 0:00:07  Loss: 0.5854 (0.6245)  Acc@1: 83.3333 (84.6774)  Acc@5: 100.0000 (96.9086)  time: 0.6029  data: 0.0005  max mem: 1331\n",
      "Test: [Task 2]  [40/42]  eta: 0:00:01  Loss: 0.5351 (0.6051)  Acc@1: 87.5000 (85.4675)  Acc@5: 100.0000 (97.1545)  time: 0.6031  data: 0.0007  max mem: 1331\n",
      "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.5347 (0.5998)  Acc@1: 87.5000 (85.5000)  Acc@5: 100.0000 (97.2000)  time: 0.5932  data: 0.0007  max mem: 1331\n",
      "Test: [Task 2] Total time: 0:00:25 (0.6086 s / it)\n",
      "* Acc@1 85.500 Acc@5 97.200 loss 0.600\n",
      "Test: [Task 3]  [ 0/42]  eta: 0:00:39  Loss: 0.3869 (0.3869)  Acc@1: 95.8333 (95.8333)  Acc@5: 95.8333 (95.8333)  time: 0.9343  data: 0.3738  max mem: 1331\n",
      "Test: [Task 3]  [10/42]  eta: 0:00:20  Loss: 0.4712 (0.5455)  Acc@1: 83.3333 (87.1212)  Acc@5: 100.0000 (98.1061)  time: 0.6308  data: 0.0343  max mem: 1331\n",
      "Test: [Task 3]  [20/42]  eta: 0:00:13  Loss: 0.4712 (0.5151)  Acc@1: 83.3333 (86.7064)  Acc@5: 100.0000 (98.2143)  time: 0.6005  data: 0.0006  max mem: 1331\n",
      "Test: [Task 3]  [30/42]  eta: 0:00:07  Loss: 0.4099 (0.5073)  Acc@1: 87.5000 (86.2903)  Acc@5: 100.0000 (98.5215)  time: 0.6002  data: 0.0007  max mem: 1331\n",
      "Test: [Task 3]  [40/42]  eta: 0:00:01  Loss: 0.6095 (0.5563)  Acc@1: 83.3333 (85.5691)  Acc@5: 95.8333 (97.5610)  time: 0.5997  data: 0.0004  max mem: 1331\n",
      "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5553 (0.5486)  Acc@1: 87.5000 (85.7000)  Acc@5: 95.8333 (97.6000)  time: 0.5900  data: 0.0004  max mem: 1331\n",
      "Test: [Task 3] Total time: 0:00:25 (0.6057 s / it)\n",
      "* Acc@1 85.700 Acc@5 97.600 loss 0.549\n",
      "Test: [Task 4]  [ 0/42]  eta: 0:00:33  Loss: 0.8460 (0.8460)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.8087  data: 0.2457  max mem: 1331\n",
      "Test: [Task 4]  [10/42]  eta: 0:00:19  Loss: 0.5062 (0.5490)  Acc@1: 87.5000 (85.6061)  Acc@5: 95.8333 (96.5909)  time: 0.6190  data: 0.0228  max mem: 1331\n",
      "Test: [Task 4]  [20/42]  eta: 0:00:13  Loss: 0.4606 (0.5593)  Acc@1: 87.5000 (85.5159)  Acc@5: 95.8333 (96.2302)  time: 0.5994  data: 0.0006  max mem: 1331\n",
      "Test: [Task 4]  [30/42]  eta: 0:00:07  Loss: 0.3580 (0.4926)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (96.9086)  time: 0.5991  data: 0.0005  max mem: 1331\n",
      "Test: [Task 4]  [40/42]  eta: 0:00:01  Loss: 0.3558 (0.5331)  Acc@1: 87.5000 (86.2805)  Acc@5: 95.8333 (96.1382)  time: 0.5990  data: 0.0007  max mem: 1331\n",
      "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.3463 (0.5278)  Acc@1: 87.5000 (86.4000)  Acc@5: 95.8333 (96.2000)  time: 0.5890  data: 0.0007  max mem: 1331\n",
      "Test: [Task 4] Total time: 0:00:25 (0.6018 s / it)\n",
      "* Acc@1 86.400 Acc@5 96.200 loss 0.528\n",
      "Test: [Task 5]  [ 0/42]  eta: 0:00:34  Loss: 0.1678 (0.1678)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.8171  data: 0.2576  max mem: 1331\n",
      "Test: [Task 5]  [10/42]  eta: 0:00:19  Loss: 0.3634 (0.3938)  Acc@1: 95.8333 (92.0455)  Acc@5: 100.0000 (98.8636)  time: 0.6174  data: 0.0238  max mem: 1331\n",
      "Test: [Task 5]  [20/42]  eta: 0:00:13  Loss: 0.3933 (0.4247)  Acc@1: 91.6667 (90.4762)  Acc@5: 100.0000 (98.8095)  time: 0.5977  data: 0.0006  max mem: 1331\n",
      "Test: [Task 5]  [30/42]  eta: 0:00:07  Loss: 0.3933 (0.4180)  Acc@1: 87.5000 (89.9194)  Acc@5: 100.0000 (98.6559)  time: 0.5980  data: 0.0006  max mem: 1331\n",
      "Test: [Task 5]  [40/42]  eta: 0:00:01  Loss: 0.3773 (0.4352)  Acc@1: 87.5000 (89.4309)  Acc@5: 100.0000 (98.4756)  time: 0.5983  data: 0.0005  max mem: 1331\n",
      "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.4620 (0.4500)  Acc@1: 87.5000 (89.1000)  Acc@5: 100.0000 (98.3000)  time: 0.5888  data: 0.0005  max mem: 1331\n",
      "Test: [Task 5] Total time: 0:00:25 (0.6008 s / it)\n",
      "* Acc@1 89.100 Acc@5 98.300 loss 0.450\n",
      "Test: [Task 6]  [ 0/42]  eta: 0:00:38  Loss: 0.5243 (0.5243)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.9233  data: 0.3661  max mem: 1331\n",
      "Test: [Task 6]  [10/42]  eta: 0:00:20  Loss: 0.5243 (0.5355)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (98.4848)  time: 0.6267  data: 0.0340  max mem: 1331\n",
      "Test: [Task 6]  [20/42]  eta: 0:00:13  Loss: 0.4832 (0.5340)  Acc@1: 83.3333 (84.1270)  Acc@5: 100.0000 (98.8095)  time: 0.5971  data: 0.0008  max mem: 1331\n",
      "Test: [Task 6]  [30/42]  eta: 0:00:07  Loss: 0.4832 (0.5382)  Acc@1: 83.3333 (83.7366)  Acc@5: 100.0000 (98.6559)  time: 0.5978  data: 0.0006  max mem: 1331\n",
      "Test: [Task 6]  [40/42]  eta: 0:00:01  Loss: 0.5333 (0.5493)  Acc@1: 83.3333 (83.2317)  Acc@5: 100.0000 (98.4756)  time: 0.5983  data: 0.0003  max mem: 1331\n",
      "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.5150 (0.5443)  Acc@1: 83.3333 (83.4000)  Acc@5: 100.0000 (98.5000)  time: 0.5888  data: 0.0003  max mem: 1331\n",
      "Test: [Task 6] Total time: 0:00:25 (0.6033 s / it)\n",
      "* Acc@1 83.400 Acc@5 98.500 loss 0.544\n",
      "Test: [Task 7]  [ 0/42]  eta: 0:00:33  Loss: 0.3556 (0.3556)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.8050  data: 0.2448  max mem: 1331\n",
      "Test: [Task 7]  [10/42]  eta: 0:00:19  Loss: 0.4035 (0.4231)  Acc@1: 91.6667 (88.6364)  Acc@5: 100.0000 (98.1061)  time: 0.6170  data: 0.0226  max mem: 1331\n",
      "Test: [Task 7]  [20/42]  eta: 0:00:13  Loss: 0.4035 (0.4623)  Acc@1: 91.6667 (87.8968)  Acc@5: 100.0000 (98.2143)  time: 0.5987  data: 0.0005  max mem: 1331\n",
      "Test: [Task 7]  [30/42]  eta: 0:00:07  Loss: 0.4569 (0.5008)  Acc@1: 87.5000 (87.6344)  Acc@5: 100.0000 (97.7151)  time: 0.5990  data: 0.0005  max mem: 1331\n",
      "Test: [Task 7]  [40/42]  eta: 0:00:01  Loss: 0.4491 (0.4886)  Acc@1: 87.5000 (87.2968)  Acc@5: 100.0000 (97.8659)  time: 0.5992  data: 0.0004  max mem: 1331\n",
      "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.4491 (0.4886)  Acc@1: 87.5000 (87.2000)  Acc@5: 100.0000 (97.9000)  time: 0.5894  data: 0.0004  max mem: 1331\n",
      "Test: [Task 7] Total time: 0:00:25 (0.6014 s / it)\n",
      "* Acc@1 87.200 Acc@5 97.900 loss 0.489\n",
      "Test: [Task 8]  [ 0/42]  eta: 0:00:38  Loss: 0.3781 (0.3781)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.9174  data: 0.3593  max mem: 1331\n",
      "Test: [Task 8]  [10/42]  eta: 0:00:20  Loss: 0.3889 (0.5241)  Acc@1: 91.6667 (87.5000)  Acc@5: 100.0000 (97.7273)  time: 0.6276  data: 0.0331  max mem: 1331\n",
      "Test: [Task 8]  [20/42]  eta: 0:00:13  Loss: 0.3991 (0.4831)  Acc@1: 91.6667 (87.8968)  Acc@5: 100.0000 (98.2143)  time: 0.5992  data: 0.0005  max mem: 1331\n",
      "Test: [Task 8]  [30/42]  eta: 0:00:07  Loss: 0.4366 (0.4912)  Acc@1: 87.5000 (87.6344)  Acc@5: 100.0000 (97.9839)  time: 0.6001  data: 0.0004  max mem: 1331\n",
      "Test: [Task 8]  [40/42]  eta: 0:00:01  Loss: 0.5200 (0.4961)  Acc@1: 87.5000 (87.2968)  Acc@5: 95.8333 (98.0691)  time: 0.6006  data: 0.0003  max mem: 1331\n",
      "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 0.5200 (0.4897)  Acc@1: 87.5000 (87.4000)  Acc@5: 100.0000 (98.1000)  time: 0.5907  data: 0.0003  max mem: 1331\n",
      "Test: [Task 8] Total time: 0:00:25 (0.6050 s / it)\n",
      "* Acc@1 87.400 Acc@5 98.100 loss 0.490\n",
      "[Average accuracy till task8]\tAcc@1: 86.4125\tAcc@5: 97.8625\tLoss: 0.5195\tForgetting: 5.2000\tBackward: -5.2000\n",
      "Loading checkpoint from: ./output/checkpoint/task9_checkpoint.pth\n",
      "Test: [Task 1]  [ 0/42]  eta: 0:00:33  Loss: 0.6969 (0.6969)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.7980  data: 0.2368  max mem: 1331\n",
      "Test: [Task 1]  [10/42]  eta: 0:00:19  Loss: 0.5240 (0.5216)  Acc@1: 83.3333 (85.9849)  Acc@5: 100.0000 (99.2424)  time: 0.6171  data: 0.0222  max mem: 1331\n",
      "Test: [Task 1]  [20/42]  eta: 0:00:13  Loss: 0.5240 (0.5601)  Acc@1: 83.3333 (84.3254)  Acc@5: 100.0000 (98.8095)  time: 0.5996  data: 0.0007  max mem: 1331\n",
      "Test: [Task 1]  [30/42]  eta: 0:00:07  Loss: 0.4948 (0.5362)  Acc@1: 83.3333 (85.3495)  Acc@5: 100.0000 (99.0591)  time: 0.6002  data: 0.0007  max mem: 1331\n",
      "Test: [Task 1]  [40/42]  eta: 0:00:01  Loss: 0.4771 (0.5142)  Acc@1: 87.5000 (86.1789)  Acc@5: 100.0000 (98.9837)  time: 0.6000  data: 0.0005  max mem: 1331\n",
      "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.4738 (0.5091)  Acc@1: 87.5000 (86.2000)  Acc@5: 100.0000 (99.0000)  time: 0.5901  data: 0.0005  max mem: 1331\n",
      "Test: [Task 1] Total time: 0:00:25 (0.6020 s / it)\n",
      "* Acc@1 86.200 Acc@5 99.000 loss 0.509\n",
      "Test: [Task 2]  [ 0/42]  eta: 0:00:36  Loss: 0.6829 (0.6829)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.8580  data: 0.2977  max mem: 1331\n",
      "Test: [Task 2]  [10/42]  eta: 0:00:19  Loss: 0.5953 (0.6118)  Acc@1: 83.3333 (85.9849)  Acc@5: 95.8333 (96.5909)  time: 0.6236  data: 0.0279  max mem: 1331\n",
      "Test: [Task 2]  [20/42]  eta: 0:00:13  Loss: 0.6095 (0.6586)  Acc@1: 83.3333 (83.5317)  Acc@5: 95.8333 (96.2302)  time: 0.6003  data: 0.0007  max mem: 1331\n",
      "Test: [Task 2]  [30/42]  eta: 0:00:07  Loss: 0.6456 (0.6538)  Acc@1: 83.3333 (84.1398)  Acc@5: 95.8333 (96.3710)  time: 0.6006  data: 0.0006  max mem: 1331\n",
      "Test: [Task 2]  [40/42]  eta: 0:00:01  Loss: 0.5803 (0.6344)  Acc@1: 87.5000 (84.5528)  Acc@5: 95.8333 (96.6463)  time: 0.6009  data: 0.0004  max mem: 1331\n",
      "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.5651 (0.6290)  Acc@1: 87.5000 (84.6000)  Acc@5: 95.8333 (96.7000)  time: 0.5912  data: 0.0004  max mem: 1331\n",
      "Test: [Task 2] Total time: 0:00:25 (0.6044 s / it)\n",
      "* Acc@1 84.600 Acc@5 96.700 loss 0.629\n",
      "Test: [Task 3]  [ 0/42]  eta: 0:00:34  Loss: 0.4019 (0.4019)  Acc@1: 95.8333 (95.8333)  Acc@5: 95.8333 (95.8333)  time: 0.8265  data: 0.2626  max mem: 1331\n",
      "Test: [Task 3]  [10/42]  eta: 0:00:19  Loss: 0.5189 (0.5493)  Acc@1: 83.3333 (85.9849)  Acc@5: 100.0000 (98.1061)  time: 0.6204  data: 0.0244  max mem: 1331\n",
      "Test: [Task 3]  [20/42]  eta: 0:00:13  Loss: 0.4720 (0.5272)  Acc@1: 83.3333 (85.1191)  Acc@5: 100.0000 (98.2143)  time: 0.6004  data: 0.0005  max mem: 1331\n",
      "Test: [Task 3]  [30/42]  eta: 0:00:07  Loss: 0.4623 (0.5133)  Acc@1: 87.5000 (86.0215)  Acc@5: 100.0000 (98.5215)  time: 0.6008  data: 0.0006  max mem: 1331\n",
      "Test: [Task 3]  [40/42]  eta: 0:00:01  Loss: 0.5717 (0.5624)  Acc@1: 87.5000 (85.5691)  Acc@5: 95.8333 (97.6626)  time: 0.6006  data: 0.0005  max mem: 1331\n",
      "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5358 (0.5559)  Acc@1: 87.5000 (85.6000)  Acc@5: 95.8333 (97.7000)  time: 0.5912  data: 0.0005  max mem: 1331\n",
      "Test: [Task 3] Total time: 0:00:25 (0.6035 s / it)\n",
      "* Acc@1 85.600 Acc@5 97.700 loss 0.556\n",
      "Test: [Task 4]  [ 0/42]  eta: 0:00:34  Loss: 0.9543 (0.9543)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  time: 0.8308  data: 0.2634  max mem: 1331\n",
      "Test: [Task 4]  [10/42]  eta: 0:00:19  Loss: 0.5448 (0.6008)  Acc@1: 83.3333 (84.8485)  Acc@5: 95.8333 (96.5909)  time: 0.6213  data: 0.0245  max mem: 1331\n",
      "Test: [Task 4]  [20/42]  eta: 0:00:13  Loss: 0.5423 (0.5877)  Acc@1: 87.5000 (85.3175)  Acc@5: 95.8333 (96.8254)  time: 0.6003  data: 0.0005  max mem: 1331\n",
      "Test: [Task 4]  [30/42]  eta: 0:00:07  Loss: 0.3824 (0.5212)  Acc@1: 87.5000 (86.9624)  Acc@5: 100.0000 (97.3118)  time: 0.6009  data: 0.0006  max mem: 1331\n",
      "Test: [Task 4]  [40/42]  eta: 0:00:01  Loss: 0.4694 (0.5631)  Acc@1: 87.5000 (85.7724)  Acc@5: 95.8333 (96.6463)  time: 0.6021  data: 0.0005  max mem: 1331\n",
      "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.3824 (0.5568)  Acc@1: 87.5000 (85.9000)  Acc@5: 95.8333 (96.7000)  time: 0.5923  data: 0.0005  max mem: 1331\n",
      "Test: [Task 4] Total time: 0:00:25 (0.6043 s / it)\n",
      "* Acc@1 85.900 Acc@5 96.700 loss 0.557\n",
      "Test: [Task 5]  [ 0/42]  eta: 0:00:36  Loss: 0.2314 (0.2314)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  time: 0.8716  data: 0.3065  max mem: 1331\n",
      "Test: [Task 5]  [10/42]  eta: 0:00:20  Loss: 0.3898 (0.4204)  Acc@1: 95.8333 (90.9091)  Acc@5: 100.0000 (98.4848)  time: 0.6258  data: 0.0284  max mem: 1331\n",
      "Test: [Task 5]  [20/42]  eta: 0:00:13  Loss: 0.3898 (0.4585)  Acc@1: 87.5000 (89.4841)  Acc@5: 100.0000 (98.4127)  time: 0.6010  data: 0.0005  max mem: 1331\n",
      "Test: [Task 5]  [30/42]  eta: 0:00:07  Loss: 0.3882 (0.4556)  Acc@1: 87.5000 (88.4409)  Acc@5: 100.0000 (98.2527)  time: 0.6011  data: 0.0004  max mem: 1331\n",
      "Test: [Task 5]  [40/42]  eta: 0:00:01  Loss: 0.5070 (0.4757)  Acc@1: 87.5000 (88.2114)  Acc@5: 100.0000 (97.9675)  time: 0.6015  data: 0.0003  max mem: 1331\n",
      "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.5374 (0.4929)  Acc@1: 87.5000 (87.9000)  Acc@5: 95.8333 (97.8000)  time: 0.5915  data: 0.0003  max mem: 1331\n",
      "Test: [Task 5] Total time: 0:00:25 (0.6052 s / it)\n",
      "* Acc@1 87.900 Acc@5 97.800 loss 0.493\n",
      "Test: [Task 6]  [ 0/42]  eta: 0:00:41  Loss: 0.5202 (0.5202)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.9820  data: 0.4235  max mem: 1331\n",
      "Test: [Task 6]  [10/42]  eta: 0:00:20  Loss: 0.5563 (0.5660)  Acc@1: 83.3333 (82.5758)  Acc@5: 100.0000 (98.1061)  time: 0.6356  data: 0.0394  max mem: 1331\n",
      "Test: [Task 6]  [20/42]  eta: 0:00:13  Loss: 0.5563 (0.5623)  Acc@1: 83.3333 (82.7381)  Acc@5: 100.0000 (98.6111)  time: 0.6007  data: 0.0007  max mem: 1331\n",
      "Test: [Task 6]  [30/42]  eta: 0:00:07  Loss: 0.5619 (0.5620)  Acc@1: 83.3333 (82.5269)  Acc@5: 100.0000 (98.7903)  time: 0.5997  data: 0.0004  max mem: 1331\n",
      "Test: [Task 6]  [40/42]  eta: 0:00:01  Loss: 0.5619 (0.5742)  Acc@1: 83.3333 (81.9106)  Acc@5: 100.0000 (98.3740)  time: 0.5993  data: 0.0003  max mem: 1331\n",
      "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.5419 (0.5690)  Acc@1: 83.3333 (82.1000)  Acc@5: 100.0000 (98.4000)  time: 0.5897  data: 0.0003  max mem: 1331\n",
      "Test: [Task 6] Total time: 0:00:25 (0.6067 s / it)\n",
      "* Acc@1 82.100 Acc@5 98.400 loss 0.569\n",
      "Test: [Task 7]  [ 0/42]  eta: 0:00:35  Loss: 0.3856 (0.3856)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (95.8333)  time: 0.8445  data: 0.2861  max mem: 1331\n",
      "Test: [Task 7]  [10/42]  eta: 0:00:19  Loss: 0.4154 (0.4285)  Acc@1: 87.5000 (87.8788)  Acc@5: 100.0000 (97.3485)  time: 0.6220  data: 0.0269  max mem: 1331\n",
      "Test: [Task 7]  [20/42]  eta: 0:00:13  Loss: 0.4154 (0.4739)  Acc@1: 87.5000 (86.7064)  Acc@5: 100.0000 (97.6190)  time: 0.5994  data: 0.0007  max mem: 1331\n",
      "Test: [Task 7]  [30/42]  eta: 0:00:07  Loss: 0.5415 (0.5139)  Acc@1: 87.5000 (86.1559)  Acc@5: 100.0000 (97.1774)  time: 0.5991  data: 0.0005  max mem: 1331\n",
      "Test: [Task 7]  [40/42]  eta: 0:00:01  Loss: 0.4527 (0.5008)  Acc@1: 87.5000 (86.4837)  Acc@5: 100.0000 (97.4594)  time: 0.5992  data: 0.0004  max mem: 1331\n",
      "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.4527 (0.5075)  Acc@1: 87.5000 (86.4000)  Acc@5: 100.0000 (97.5000)  time: 0.5894  data: 0.0004  max mem: 1331\n",
      "Test: [Task 7] Total time: 0:00:25 (0.6027 s / it)\n",
      "* Acc@1 86.400 Acc@5 97.500 loss 0.508\n",
      "Test: [Task 8]  [ 0/42]  eta: 0:00:40  Loss: 0.3822 (0.3822)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.9527  data: 0.3980  max mem: 1331\n",
      "Test: [Task 8]  [10/42]  eta: 0:00:20  Loss: 0.3822 (0.5185)  Acc@1: 87.5000 (87.8788)  Acc@5: 100.0000 (97.7273)  time: 0.6303  data: 0.0368  max mem: 1331\n",
      "Test: [Task 8]  [20/42]  eta: 0:00:13  Loss: 0.4121 (0.4857)  Acc@1: 87.5000 (88.0952)  Acc@5: 100.0000 (98.0159)  time: 0.5980  data: 0.0006  max mem: 1331\n",
      "Test: [Task 8]  [30/42]  eta: 0:00:07  Loss: 0.4374 (0.4917)  Acc@1: 87.5000 (87.9032)  Acc@5: 95.8333 (97.7151)  time: 0.5981  data: 0.0007  max mem: 1331\n",
      "Test: [Task 8]  [40/42]  eta: 0:00:01  Loss: 0.5147 (0.4914)  Acc@1: 87.5000 (87.8049)  Acc@5: 95.8333 (97.6626)  time: 0.5983  data: 0.0006  max mem: 1331\n",
      "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 0.5147 (0.4863)  Acc@1: 87.5000 (87.9000)  Acc@5: 95.8333 (97.7000)  time: 0.5888  data: 0.0006  max mem: 1331\n",
      "Test: [Task 8] Total time: 0:00:25 (0.6043 s / it)\n",
      "* Acc@1 87.900 Acc@5 97.700 loss 0.486\n",
      "Test: [Task 9]  [ 0/42]  eta: 0:00:34  Loss: 0.1911 (0.1911)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  time: 0.8309  data: 0.2749  max mem: 1331\n",
      "Test: [Task 9]  [10/42]  eta: 0:00:19  Loss: 0.3333 (0.3459)  Acc@1: 87.5000 (90.1515)  Acc@5: 100.0000 (98.1061)  time: 0.6205  data: 0.0263  max mem: 1331\n",
      "Test: [Task 9]  [20/42]  eta: 0:00:13  Loss: 0.2666 (0.3169)  Acc@1: 91.6667 (90.6746)  Acc@5: 100.0000 (99.0079)  time: 0.5989  data: 0.0009  max mem: 1331\n",
      "Test: [Task 9]  [30/42]  eta: 0:00:07  Loss: 0.2666 (0.3288)  Acc@1: 91.6667 (90.5914)  Acc@5: 100.0000 (99.0591)  time: 0.5990  data: 0.0006  max mem: 1331\n",
      "Test: [Task 9]  [40/42]  eta: 0:00:01  Loss: 0.2656 (0.3086)  Acc@1: 91.6667 (91.2602)  Acc@5: 100.0000 (99.1870)  time: 0.5993  data: 0.0007  max mem: 1331\n",
      "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 0.2656 (0.3030)  Acc@1: 91.6667 (91.4000)  Acc@5: 100.0000 (99.2000)  time: 0.5898  data: 0.0006  max mem: 1331\n",
      "Test: [Task 9] Total time: 0:00:25 (0.6023 s / it)\n",
      "* Acc@1 91.400 Acc@5 99.200 loss 0.303\n",
      "[Average accuracy till task9]\tAcc@1: 86.4444\tAcc@5: 97.8556\tLoss: 0.5122\tForgetting: 5.2000\tBackward: -5.1375\n",
      "Loading checkpoint from: ./output/checkpoint/task10_checkpoint.pth\n",
      "Test: [Task 1]  [ 0/42]  eta: 0:00:33  Loss: 0.7249 (0.7249)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.7988  data: 0.2368  max mem: 1331\n",
      "Test: [Task 1]  [10/42]  eta: 0:00:19  Loss: 0.5669 (0.5520)  Acc@1: 83.3333 (84.0909)  Acc@5: 100.0000 (98.8636)  time: 0.6167  data: 0.0221  max mem: 1331\n",
      "Test: [Task 1]  [20/42]  eta: 0:00:13  Loss: 0.5219 (0.5821)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (98.8095)  time: 0.5989  data: 0.0005  max mem: 1331\n",
      "Test: [Task 1]  [30/42]  eta: 0:00:07  Loss: 0.5219 (0.5659)  Acc@1: 83.3333 (83.7366)  Acc@5: 100.0000 (98.7903)  time: 0.5999  data: 0.0005  max mem: 1331\n",
      "Test: [Task 1]  [40/42]  eta: 0:00:01  Loss: 0.5133 (0.5388)  Acc@1: 83.3333 (84.6545)  Acc@5: 100.0000 (98.6789)  time: 0.6011  data: 0.0005  max mem: 1331\n",
      "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5086 (0.5334)  Acc@1: 83.3333 (84.7000)  Acc@5: 100.0000 (98.7000)  time: 0.5913  data: 0.0005  max mem: 1331\n",
      "Test: [Task 1] Total time: 0:00:25 (0.6023 s / it)\n",
      "* Acc@1 84.700 Acc@5 98.700 loss 0.533\n",
      "Test: [Task 2]  [ 0/42]  eta: 0:00:37  Loss: 0.6809 (0.6809)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (95.8333)  time: 0.8978  data: 0.3330  max mem: 1331\n",
      "Test: [Task 2]  [10/42]  eta: 0:00:20  Loss: 0.6108 (0.6506)  Acc@1: 83.3333 (83.7121)  Acc@5: 95.8333 (96.9697)  time: 0.6267  data: 0.0310  max mem: 1331\n",
      "Test: [Task 2]  [20/42]  eta: 0:00:13  Loss: 0.6684 (0.6998)  Acc@1: 79.1667 (80.9524)  Acc@5: 95.8333 (96.4286)  time: 0.6010  data: 0.0006  max mem: 1331\n",
      "Test: [Task 2]  [30/42]  eta: 0:00:07  Loss: 0.6718 (0.6878)  Acc@1: 79.1667 (82.1237)  Acc@5: 95.8333 (96.6398)  time: 0.6021  data: 0.0005  max mem: 1331\n",
      "Test: [Task 2]  [40/42]  eta: 0:00:01  Loss: 0.5812 (0.6584)  Acc@1: 83.3333 (82.8252)  Acc@5: 95.8333 (96.8496)  time: 0.6015  data: 0.0004  max mem: 1331\n",
      "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.5640 (0.6530)  Acc@1: 83.3333 (82.8000)  Acc@5: 100.0000 (96.9000)  time: 0.5916  data: 0.0004  max mem: 1331\n",
      "Test: [Task 2] Total time: 0:00:25 (0.6059 s / it)\n",
      "* Acc@1 82.800 Acc@5 96.900 loss 0.653\n",
      "Test: [Task 3]  [ 0/42]  eta: 0:00:40  Loss: 0.3917 (0.3917)  Acc@1: 95.8333 (95.8333)  Acc@5: 95.8333 (95.8333)  time: 0.9632  data: 0.4028  max mem: 1331\n",
      "Test: [Task 3]  [10/42]  eta: 0:00:20  Loss: 0.5235 (0.5421)  Acc@1: 83.3333 (86.3636)  Acc@5: 100.0000 (97.7273)  time: 0.6309  data: 0.0373  max mem: 1331\n",
      "Test: [Task 3]  [20/42]  eta: 0:00:13  Loss: 0.4824 (0.5128)  Acc@1: 83.3333 (85.9127)  Acc@5: 100.0000 (98.0159)  time: 0.5978  data: 0.0006  max mem: 1331\n",
      "Test: [Task 3]  [30/42]  eta: 0:00:07  Loss: 0.4353 (0.5126)  Acc@1: 87.5000 (86.2903)  Acc@5: 100.0000 (97.9839)  time: 0.5981  data: 0.0005  max mem: 1331\n",
      "Test: [Task 3]  [40/42]  eta: 0:00:01  Loss: 0.5440 (0.5654)  Acc@1: 87.5000 (85.7724)  Acc@5: 95.8333 (97.2561)  time: 0.5982  data: 0.0004  max mem: 1331\n",
      "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5168 (0.5602)  Acc@1: 87.5000 (85.8000)  Acc@5: 95.8333 (97.3000)  time: 0.5887  data: 0.0004  max mem: 1331\n",
      "Test: [Task 3] Total time: 0:00:25 (0.6049 s / it)\n",
      "* Acc@1 85.800 Acc@5 97.300 loss 0.560\n",
      "Test: [Task 4]  [ 0/42]  eta: 0:00:48  Loss: 1.0269 (1.0269)  Acc@1: 70.8333 (70.8333)  Acc@5: 87.5000 (87.5000)  time: 1.1445  data: 0.5783  max mem: 1331\n",
      "Test: [Task 4]  [10/42]  eta: 0:00:20  Loss: 0.5838 (0.6361)  Acc@1: 87.5000 (84.4697)  Acc@5: 95.8333 (95.4545)  time: 0.6459  data: 0.0535  max mem: 1331\n",
      "Test: [Task 4]  [20/42]  eta: 0:00:13  Loss: 0.5682 (0.6133)  Acc@1: 87.5000 (85.1190)  Acc@5: 95.8333 (95.8333)  time: 0.5959  data: 0.0007  max mem: 1331\n",
      "Test: [Task 4]  [30/42]  eta: 0:00:07  Loss: 0.4393 (0.5578)  Acc@1: 87.5000 (86.1559)  Acc@5: 100.0000 (96.6398)  time: 0.5965  data: 0.0003  max mem: 1331\n",
      "Test: [Task 4]  [40/42]  eta: 0:00:01  Loss: 0.5779 (0.5986)  Acc@1: 83.3333 (85.3659)  Acc@5: 95.8333 (95.9350)  time: 0.5968  data: 0.0003  max mem: 1331\n",
      "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.5077 (0.5965)  Acc@1: 83.3333 (85.4000)  Acc@5: 95.8333 (96.0000)  time: 0.5869  data: 0.0003  max mem: 1331\n",
      "Test: [Task 4] Total time: 0:00:25 (0.6087 s / it)\n",
      "* Acc@1 85.400 Acc@5 96.000 loss 0.596\n",
      "Test: [Task 5]  [ 0/42]  eta: 0:00:48  Loss: 0.2857 (0.2857)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  time: 1.1510  data: 0.5888  max mem: 1331\n",
      "Test: [Task 5]  [10/42]  eta: 0:00:20  Loss: 0.4928 (0.4870)  Acc@1: 87.5000 (87.1212)  Acc@5: 100.0000 (98.1061)  time: 0.6444  data: 0.0541  max mem: 1331\n",
      "Test: [Task 5]  [20/42]  eta: 0:00:13  Loss: 0.5018 (0.5218)  Acc@1: 87.5000 (86.1111)  Acc@5: 100.0000 (98.2143)  time: 0.5948  data: 0.0005  max mem: 1331\n",
      "Test: [Task 5]  [30/42]  eta: 0:00:07  Loss: 0.5018 (0.5122)  Acc@1: 87.5000 (85.4839)  Acc@5: 100.0000 (97.9839)  time: 0.5962  data: 0.0004  max mem: 1331\n",
      "Test: [Task 5]  [40/42]  eta: 0:00:01  Loss: 0.5076 (0.5327)  Acc@1: 87.5000 (85.4675)  Acc@5: 100.0000 (97.6626)  time: 0.5978  data: 0.0004  max mem: 1331\n",
      "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.5789 (0.5497)  Acc@1: 83.3333 (85.2000)  Acc@5: 95.8333 (97.4000)  time: 0.5884  data: 0.0004  max mem: 1331\n",
      "Test: [Task 5] Total time: 0:00:25 (0.6084 s / it)\n",
      "* Acc@1 85.200 Acc@5 97.400 loss 0.550\n",
      "Test: [Task 6]  [ 0/42]  eta: 0:00:48  Loss: 0.6103 (0.6103)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (95.8333)  time: 1.1589  data: 0.6034  max mem: 1331\n",
      "Test: [Task 6]  [10/42]  eta: 0:00:20  Loss: 0.6103 (0.5899)  Acc@1: 83.3333 (82.5758)  Acc@5: 95.8333 (97.3485)  time: 0.6488  data: 0.0554  max mem: 1331\n",
      "Test: [Task 6]  [20/42]  eta: 0:00:13  Loss: 0.5677 (0.5854)  Acc@1: 83.3333 (82.7381)  Acc@5: 100.0000 (98.0159)  time: 0.5982  data: 0.0005  max mem: 1331\n",
      "Test: [Task 6]  [30/42]  eta: 0:00:07  Loss: 0.5241 (0.5896)  Acc@1: 83.3333 (82.5269)  Acc@5: 100.0000 (98.1183)  time: 0.5991  data: 0.0005  max mem: 1331\n",
      "Test: [Task 6]  [40/42]  eta: 0:00:01  Loss: 0.6017 (0.5991)  Acc@1: 83.3333 (82.4187)  Acc@5: 95.8333 (97.4594)  time: 0.5990  data: 0.0004  max mem: 1331\n",
      "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.5401 (0.5937)  Acc@1: 83.3333 (82.6000)  Acc@5: 100.0000 (97.5000)  time: 0.5891  data: 0.0003  max mem: 1331\n",
      "Test: [Task 6] Total time: 0:00:25 (0.6106 s / it)\n",
      "* Acc@1 82.600 Acc@5 97.500 loss 0.594\n",
      "Test: [Task 7]  [ 0/42]  eta: 0:00:44  Loss: 0.4027 (0.4027)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (95.8333)  time: 1.0705  data: 0.5074  max mem: 1331\n",
      "Test: [Task 7]  [10/42]  eta: 0:00:20  Loss: 0.4027 (0.4327)  Acc@1: 87.5000 (88.2576)  Acc@5: 100.0000 (97.3485)  time: 0.6397  data: 0.0465  max mem: 1331\n",
      "Test: [Task 7]  [20/42]  eta: 0:00:13  Loss: 0.4392 (0.4880)  Acc@1: 87.5000 (86.5079)  Acc@5: 100.0000 (97.6190)  time: 0.5965  data: 0.0004  max mem: 1331\n",
      "Test: [Task 7]  [30/42]  eta: 0:00:07  Loss: 0.4717 (0.5220)  Acc@1: 87.5000 (86.4247)  Acc@5: 100.0000 (96.7742)  time: 0.5972  data: 0.0004  max mem: 1331\n",
      "Test: [Task 7]  [40/42]  eta: 0:00:01  Loss: 0.4646 (0.5090)  Acc@1: 87.5000 (86.5854)  Acc@5: 95.8333 (97.0528)  time: 0.5984  data: 0.0003  max mem: 1331\n",
      "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.4646 (0.5170)  Acc@1: 87.5000 (86.5000)  Acc@5: 95.8333 (97.1000)  time: 0.5888  data: 0.0003  max mem: 1331\n",
      "Test: [Task 7] Total time: 0:00:25 (0.6079 s / it)\n",
      "* Acc@1 86.500 Acc@5 97.100 loss 0.517\n",
      "Test: [Task 8]  [ 0/42]  eta: 0:00:41  Loss: 0.4760 (0.4760)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.9915  data: 0.4354  max mem: 1331\n",
      "Test: [Task 8]  [10/42]  eta: 0:00:20  Loss: 0.4508 (0.5939)  Acc@1: 87.5000 (85.6061)  Acc@5: 100.0000 (97.7273)  time: 0.6347  data: 0.0400  max mem: 1331\n",
      "Test: [Task 8]  [20/42]  eta: 0:00:13  Loss: 0.4508 (0.5547)  Acc@1: 87.5000 (86.1111)  Acc@5: 95.8333 (97.8175)  time: 0.5992  data: 0.0006  max mem: 1331\n",
      "Test: [Task 8]  [30/42]  eta: 0:00:07  Loss: 0.5421 (0.5663)  Acc@1: 87.5000 (86.2903)  Acc@5: 95.8333 (97.4462)  time: 0.6002  data: 0.0006  max mem: 1331\n",
      "Test: [Task 8]  [40/42]  eta: 0:00:01  Loss: 0.5466 (0.5528)  Acc@1: 87.5000 (86.9919)  Acc@5: 95.8333 (97.5610)  time: 0.6010  data: 0.0003  max mem: 1331\n",
      "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 0.5421 (0.5473)  Acc@1: 87.5000 (87.0000)  Acc@5: 95.8333 (97.6000)  time: 0.5912  data: 0.0003  max mem: 1331\n",
      "Test: [Task 8] Total time: 0:00:25 (0.6074 s / it)\n",
      "* Acc@1 87.000 Acc@5 97.600 loss 0.547\n",
      "Test: [Task 9]  [ 0/42]  eta: 0:00:38  Loss: 0.2714 (0.2714)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  time: 0.9117  data: 0.3519  max mem: 1331\n",
      "Test: [Task 9]  [10/42]  eta: 0:00:20  Loss: 0.4291 (0.4870)  Acc@1: 87.5000 (85.2273)  Acc@5: 100.0000 (98.1061)  time: 0.6282  data: 0.0323  max mem: 1331\n",
      "Test: [Task 9]  [20/42]  eta: 0:00:13  Loss: 0.3779 (0.4229)  Acc@1: 87.5000 (86.9048)  Acc@5: 100.0000 (98.8095)  time: 0.5998  data: 0.0006  max mem: 1331\n",
      "Test: [Task 9]  [30/42]  eta: 0:00:07  Loss: 0.3688 (0.4282)  Acc@1: 87.5000 (86.8280)  Acc@5: 100.0000 (98.7903)  time: 0.5999  data: 0.0006  max mem: 1331\n",
      "Test: [Task 9]  [40/42]  eta: 0:00:01  Loss: 0.3453 (0.3979)  Acc@1: 87.5000 (87.8049)  Acc@5: 100.0000 (98.9837)  time: 0.5997  data: 0.0004  max mem: 1331\n",
      "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 0.3453 (0.3904)  Acc@1: 87.5000 (88.0000)  Acc@5: 100.0000 (99.0000)  time: 0.5899  data: 0.0004  max mem: 1331\n",
      "Test: [Task 9] Total time: 0:00:25 (0.6048 s / it)\n",
      "* Acc@1 88.000 Acc@5 99.000 loss 0.390\n",
      "Test: [Task 10]  [ 0/42]  eta: 0:00:33  Loss: 0.2934 (0.2934)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  time: 0.8016  data: 0.2386  max mem: 1331\n",
      "Test: [Task 10]  [10/42]  eta: 0:00:19  Loss: 0.4210 (0.4547)  Acc@1: 91.6667 (88.2576)  Acc@5: 100.0000 (98.8636)  time: 0.6155  data: 0.0221  max mem: 1331\n",
      "Test: [Task 10]  [20/42]  eta: 0:00:13  Loss: 0.4404 (0.4550)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (98.8095)  time: 0.5975  data: 0.0004  max mem: 1331\n",
      "Test: [Task 10]  [30/42]  eta: 0:00:07  Loss: 0.4280 (0.4275)  Acc@1: 87.5000 (88.4409)  Acc@5: 100.0000 (99.0591)  time: 0.5978  data: 0.0004  max mem: 1331\n",
      "Test: [Task 10]  [40/42]  eta: 0:00:01  Loss: 0.3730 (0.4314)  Acc@1: 87.5000 (87.9065)  Acc@5: 100.0000 (98.9837)  time: 0.5971  data: 0.0003  max mem: 1331\n",
      "Test: [Task 10]  [41/42]  eta: 0:00:00  Loss: 0.3730 (0.4248)  Acc@1: 87.5000 (88.0000)  Acc@5: 100.0000 (99.0000)  time: 0.5877  data: 0.0003  max mem: 1331\n",
      "Test: [Task 10] Total time: 0:00:25 (0.5999 s / it)\n",
      "* Acc@1 88.000 Acc@5 99.000 loss 0.425\n",
      "[Average accuracy till task10]\tAcc@1: 85.6000\tAcc@5: 97.6500\tLoss: 0.5366\tForgetting: 5.7333\tBackward: -5.6778\n",
      "[rank0]:[W122 19:24:16.330854294 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dataset Structure and Overlap Analysis\n",
    "\n",
    "Before proceeding with feature extraction and backward-compatible retrieval experiments, it is necessary to analyze the structure of the datasets involved and verify the absence of unintended overlaps.\n",
    "\n",
    "Three datasets are used at different stages of the pipeline:\n",
    "\n",
    "- **ImageNet**: used to pre-train the Vision Transformer (ViT-B/16) backbone.\n",
    "- **CIFAR-100**: used to train the DualPrompt model in a continual learning setting.\n",
    "- **CIFAR-10**: used exclusively for backward compatibility and retrieval experiments (gallery–query matching).\n",
    "\n",
    "### ImageNet vs CIFAR Datasets\n",
    "\n",
    "ImageNet and CIFAR datasets differ significantly in both image resolution and data curation process:\n",
    "\n",
    "- ImageNet contains high-resolution natural images collected from the web and manually curated.\n",
    "- CIFAR-10 and CIFAR-100 consist of 32×32 images derived from a different collection pipeline.\n",
    "\n",
    "There is no instance-level overlap between ImageNet and CIFAR datasets. While some semantic categories may share similar names (e.g., *dog*, *car*), the actual images are distinct. Therefore, ImageNet pre-training does not introduce data leakage into CIFAR-based experiments.\n",
    "\n",
    "### CIFAR-100 vs CIFAR-10\n",
    "\n",
    "CIFAR-10 and CIFAR-100 are constructed from the same original image pool but are split into **disjoint label sets**:\n",
    "\n",
    "- CIFAR-10 contains 10 coarse-grained classes.\n",
    "- CIFAR-100 contains 100 fine-grained classes.\n",
    "\n",
    "Each image appears in **only one** of the two datasets, and no image is shared between CIFAR-10 and CIFAR-100. Consequently:\n",
    "\n",
    "- Training DualPrompt on CIFAR-100 does not expose the model to any images from CIFAR-10.\n",
    "- CIFAR-10 can be safely used as an independent benchmark for evaluating backward compatibility.\n",
    "\n",
    "### Implications for Backward-Compatible Evaluation\n",
    "\n",
    "This dataset separation ensures that:\n",
    "\n",
    "- Feature representations extracted from the DualPrompt model are evaluated on **unseen data**.\n",
    "- Gallery–query matching on CIFAR-10 measures representation consistency rather than memorization.\n",
    "- Backward compatibility is tested under a realistic deployment scenario, where a new model must interoperate with previously indexed features from a different data distribution.\n",
    "\n",
    "This setup aligns with the assumptions of backward-compatible representation learning, where old embeddings and new embeddings coexist without requiring reprocessing of the original gallery.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# ----------------------------\n",
    "# Load CIFAR-10 and CIFAR-100\n",
    "# ----------------------------\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(root=data_path, train=True, download=True, transform=transform)\n",
    "cifar10_test = datasets.CIFAR10(root=data_path, train=False, download=True, transform=transform)\n",
    "\n",
    "cifar100_train = datasets.CIFAR100(root=data_path, train=True, download=False, transform=transform)\n",
    "cifar100_test = datasets.CIFAR100(root=data_path, train=False, download=False, transform=transform)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ----------------------------\n",
    "# Dataset statistics\n",
    "# ----------------------------\n",
    "\n",
    "print(\"CIFAR-10: \")\n",
    "print(\" Train samples:\", len(cifar10_train))\n",
    "print(\" Test samples:\", len(cifar10_test))\n",
    "print(\" Classes:\", cifar10_train.classes)\n",
    "\n",
    "print(\"\\nCIFAR-100: \")\n",
    "print(\" Train samples:\", len(cifar100_train))\n",
    "print(\" Test samples:\", len(cifar100_test))\n",
    "print(\" Classes:\", cifar100_train.classes)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ----------------------------\n",
    "# Helper: compute image hash\n",
    "# ----------------------------\n",
    "\n",
    "import hashlib\n",
    "import numpy as np\n",
    "\n",
    "def image_hash(img_tensor):\n",
    "    \"\"\"\n",
    "    Compute a hash for an image tensor.\n",
    "    Tensor shape: [C, H, W]\n",
    "    \"\"\"\n",
    "    img_bytes = (img_tensor.numpy() * 255).astype(np.uint8).tobytes()\n",
    "    return hashlib.md5(img_bytes).hexdigest()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_samples = 5000  # subset size to keep computation reasonable\n",
    "\n",
    "cifar10_hashes = set(\n",
    "    image_hash(cifar10_train[i][0]) for i in range(num_samples)\n",
    ")\n",
    "\n",
    "overlap_count = 0\n",
    "for i in range(num_samples):\n",
    "    h = image_hash(cifar100_train[i][0])\n",
    "    if h in cifar10_hashes:\n",
    "        overlap_count += 1\n",
    "\n",
    "print(f\"Checked {num_samples} images from each dataset.\")\n",
    "print(f\"Number of overlapping images found: {overlap_count}\")\n",
    "\n",
    "if overlap_count == 0:\n",
    "    print(\"No image-level overlap detected between CIFAR-10 and CIFAR-100.\")\n",
    "else:\n",
    "    print(\"Potential overlap detected (unexpected).\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feature Extractor Construction from DualPrompt Checkpoints\n",
    "\n",
    "To evaluate representation stability and backward compatibility, the classification head is removed and only the feature extractor is retained. The analysis focuses on the output of the last layer before the classification head, which represents the learned embedding used for retrieval.\n",
    "\n",
    "### Checkpoint Selection\n",
    "\n",
    "DualPrompt training produces a sequence of checkpoints corresponding to different tasks. Each checkpoint represents a model state after learning a new subset of classes. In this experiment:\n",
    "\n",
    "- One feature extractor is built from each checkpoint.\n",
    "- A total of 10 models are considered, corresponding to the 10 incremental tasks.\n",
    "- All models share the same Vision Transformer backbone architecture.\n",
    "\n",
    "This setup allows tracking how the learned representation evolves across tasks and enables direct comparison between embeddings extracted at different stages of continual learning.\n",
    "\n",
    "### Purpose for Backward Compatibility Evaluation\n",
    "\n",
    "Using feature-only models enables:\n",
    "\n",
    "- Direct comparison between embeddings from different checkpoints.\n",
    "- Simulation of a real-world deployment scenario where older gallery features coexist with newer query features.\n",
    "- Evaluation of whether queries generated by a newer model can successfully retrieve gallery items indexed with older embeddings.\n",
    "\n",
    "The extracted embeddings serve as the basis for retrieval experiments on CIFAR-10, using backward compatibility criteria inspired by the BCT framework.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Download DualPrompt checkpoints from Google Drive\n",
    "import gdown\n",
    "\n",
    "output_dir = './output/checkpoint'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "folder_id = 'https://drive.google.com/drive/folders/1PdhU4Ko7iRqoPjZzsAG_eZ4S8TRJSrM0?usp=sharing'\n",
    "gdown.download_folder(url=folder_id, output=output_dir, quiet=True)\n",
    "print(\"Checkpoints downloaded to:\", output_dir)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from timm.models import create_model\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "NUM_TASKS = 10\n",
    "CHECKPOINT_DIR = \"./output/checkpoint\"\n",
    "\n",
    "MODEL_NAME = \"vit_base_patch16_224\"\n",
    "NUM_CLASSES = 100   # CIFAR-100\n",
    "PROMPT_CONFIG = dict(\n",
    "    prompt_length=5,\n",
    "    embedding_key=\"cls\",\n",
    "    prompt_init=\"uniform\",\n",
    "    prompt_pool=True,\n",
    "    prompt_key=True,\n",
    "    pool_size=10,\n",
    "    top_k=1,\n",
    "    batchwise_prompt=True,\n",
    "    head_type=\"token\",\n",
    "    use_prompt_mask=True,\n",
    "    use_g_prompt=True,\n",
    "    g_prompt_length=5,\n",
    "    g_prompt_layer_idx=[0, 1],\n",
    "    use_prefix_tune_for_g_prompt=True,\n",
    "    use_e_prompt=True,\n",
    "    e_prompt_layer_idx=[2, 3, 4],\n",
    "    use_prefix_tune_for_e_prompt=True,\n",
    "    same_key_value=False,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Load DualPrompt Models\n",
    "# -----------------------------\n",
    "def load_dualprompt_models():\n",
    "    models = []\n",
    "\n",
    "    for task_id in range(NUM_TASKS):\n",
    "        print(f\"Loading model for task {task_id + 1}\")\n",
    "\n",
    "        # Create model (same as main.py)\n",
    "        model = create_model(\n",
    "            MODEL_NAME,\n",
    "            pretrained=False,\n",
    "            num_classes=NUM_CLASSES,\n",
    "            drop_rate=0.0,\n",
    "            drop_path_rate=0.0,\n",
    "            **PROMPT_CONFIG\n",
    "        )\n",
    "\n",
    "        checkpoint_path = os.path.join(\n",
    "            CHECKPOINT_DIR,\n",
    "            f\"task{task_id + 1}_checkpoint.pth\"\n",
    "        )\n",
    "\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "\n",
    "        checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
    "        model.load_state_dict(checkpoint[\"model\"], strict=True)\n",
    "\n",
    "        # Remove classification head → feature extractor\n",
    "        model.head = torch.nn.Identity()\n",
    "\n",
    "        model.to(DEVICE)\n",
    "        model.eval()\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "    print(\"All DualPrompt models loaded successfully.\")\n",
    "    return models\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load models\n",
    "dualprompt_models = load_dualprompt_models()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# print model summary\n",
    "from torchsummary import summary\n",
    "summary(dualprompt_models[0], (3, 224, 224))"
   ]
  }
 ]
}
